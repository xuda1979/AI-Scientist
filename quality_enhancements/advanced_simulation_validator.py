"""
Advanced Simulation Validator
=============================

Comprehensive validation of simulation methodologies including
statistical power analysis, parameter sensitivity, and robustness testing.
"""

import re
import json
import numpy as np
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path
from dataclasses import dataclass
from datetime import datetime
import ast
import importlib.util

try:
    from scipy import stats
    import scipy.optimize as opt
    HAS_SCIPY = True
except ImportError:
    HAS_SCIPY = False


@dataclass
class ParameterAnalysis:
    """Analysis of simulation parameter configuration."""
    parameter_name: str
    value_type: str
    default_value: Any
    value_range: Optional[Tuple[float, float]]
    sensitivity_score: float
    justification_present: bool
    uncertainty_quantified: bool


@dataclass
class StatisticalPowerAnalysis:
    """Statistical power analysis results."""
    sample_size: int
    effect_size: float
    statistical_power: float
    significance_level: float
    recommended_sample_size: int
    power_sufficient: bool
    analysis_method: str


@dataclass
class RobustnessTest:
    """Results of robustness testing."""
    test_name: str
    parameter_varied: str
    variation_range: str
    result_stability: float
    critical_thresholds: List[float]
    robustness_score: float


@dataclass
class SimulationValidationIssue:
    """Issues found in simulation methodology."""
    severity: str  # critical, warning, info
    issue_type: str
    description: str
    location: str
    recommendations: List[str]
    validation_code: Optional[str]


@dataclass
class SimulationValidationResult:
    """Complete simulation validation result."""
    parameter_analyses: List[ParameterAnalysis]
    power_analysis: Optional[StatisticalPowerAnalysis]
    robustness_tests: List[RobustnessTest]
    identified_issues: List[SimulationValidationIssue]
    methodology_score: float
    statistical_rigor_score: float
    reproducibility_score: float
    recommendations: List[str]
    validation_summary: str


class AdvancedSimulationValidator:
    """Advanced simulation methodology validation system."""
    
    def __init__(self, universal_chat_fn):
        self.universal_chat = universal_chat_fn
        
        # Statistical standards
        self.min_power = 0.8
        self.max_alpha = 0.05
        self.min_sample_size = 30
        
        # Parameter validation criteria
        self.required_justifications = [
            'sample_size', 'learning_rate', 'batch_size', 'epochs',
            'temperature', 'random_seed', 'noise_level'
        ]
        
        # Robustness testing parameters
        self.robustness_variations = {
            'small': 0.1,    # Â±10%
            'medium': 0.25,  # Â±25%
            'large': 0.5     # Â±50%
        }
    
    def validate_simulation(self, paper_content: str, simulation_files: List[Path],
                          model: str, request_timeout: int = 1800) -> SimulationValidationResult:
        """Comprehensive simulation validation."""
        print("ðŸ”¬ Starting Advanced Simulation Validation...")
        
        # Extract simulation methodology
        sim_methodology = self._extract_simulation_methodology(paper_content)
        print(f"  Extracted simulation methodology from paper")
        
        # Analyze simulation code
        code_analysis = self._analyze_simulation_code(simulation_files)
        print(f"  Analyzed {len(simulation_files)} simulation files")
        
        # Analyze parameters
        parameter_analyses = self._analyze_parameters(sim_methodology, code_analysis)
        print(f"  Analyzed {len(parameter_analyses)} simulation parameters")
        
        # Statistical power analysis
        power_analysis = self._perform_power_analysis(sim_methodology, code_analysis)
        if power_analysis:
            print(f"  Power analysis: {power_analysis.statistical_power:.3f}")
        
        # Robustness testing
        robustness_tests = self._perform_robustness_analysis(
            sim_methodology, code_analysis, model, request_timeout
        )
        print(f"  Performed {len(robustness_tests)} robustness tests")
        
        # Identify issues
        identified_issues = self._identify_simulation_issues(
            parameter_analyses, power_analysis, robustness_tests, sim_methodology
        )
        print(f"  Identified {len(identified_issues)} simulation issues")
        
        # Calculate scores
        methodology_score = self._calculate_methodology_score(
            parameter_analyses, sim_methodology
        )
        statistical_rigor_score = self._calculate_statistical_rigor_score(
            power_analysis, robustness_tests
        )
        reproducibility_score = self._calculate_reproducibility_score(
            parameter_analyses, code_analysis
        )
        
        # Generate recommendations
        recommendations = self._generate_simulation_recommendations(
            parameter_analyses, power_analysis, robustness_tests, identified_issues
        )
        
        # Generate validation summary
        validation_summary = self._generate_validation_summary(
            len(parameter_analyses), power_analysis, len(robustness_tests),
            methodology_score, statistical_rigor_score, reproducibility_score
        )
        
        return SimulationValidationResult(
            parameter_analyses=parameter_analyses,
            power_analysis=power_analysis,
            robustness_tests=robustness_tests,
            identified_issues=identified_issues,
            methodology_score=methodology_score,
            statistical_rigor_score=statistical_rigor_score,
            reproducibility_score=reproducibility_score,
            recommendations=recommendations,
            validation_summary=validation_summary
        )
    
    def _extract_simulation_methodology(self, paper_content: str) -> Dict[str, Any]:
        """Extract simulation methodology information from paper."""
        methodology = {
            'sample_sizes': [],
            'parameters': {},
            'statistical_tests': [],
            'validation_methods': [],
            'random_seeds': [],
            'hardware_requirements': '',
            'software_versions': {}
        }
        
        # Extract sample sizes
        sample_patterns = [
            r'n\s*=\s*(\d+)',
            r'sample size[s]?\s*(?:of|:)?\s*(\d+)',
            r'(\d+)\s*samples?',
            r'(\d+)\s*(?:runs?|trials?|experiments?)'
        ]
        
        for pattern in sample_patterns:
            matches = re.findall(pattern, paper_content, re.IGNORECASE)
            methodology['sample_sizes'].extend([int(m) for m in matches])
        
        # Extract parameters
        param_patterns = [
            r'learning rate[s]?\s*(?:of|=|:)\s*([\d.e-]+)',
            r'batch size[s]?\s*(?:of|=|:)\s*(\d+)',
            r'epochs?\s*(?:of|=|:)\s*(\d+)',
            r'temperature[s]?\s*(?:of|=|:)\s*([\d.]+)',
            r'noise level[s]?\s*(?:of|=|:)\s*([\d.]+)',
            r'random seed[s]?\s*(?:of|=|:)\s*(\d+)'
        ]
        
        param_names = ['learning_rate', 'batch_size', 'epochs', 'temperature', 'noise_level', 'random_seed']
        
        for i, pattern in enumerate(param_patterns):
            matches = re.findall(pattern, paper_content, re.IGNORECASE)
            if matches:
                param_name = param_names[i]
                methodology['parameters'][param_name] = [float(m) if '.' in m else int(m) for m in matches]
        
        # Extract statistical tests
        stat_test_patterns = [
            r't-test', r'anova', r'chi-square', r'mann-whitney', 
            r'wilcoxon', r'kruskal-wallis', r'pearson', r'spearman'
        ]
        
        for pattern in stat_test_patterns:
            if re.search(pattern, paper_content, re.IGNORECASE):
                methodology['statistical_tests'].append(pattern)
        
        # Extract validation methods
        validation_patterns = [
            r'cross-validation', r'k-fold', r'bootstrapping', 
            r'monte carlo', r'sensitivity analysis'
        ]
        
        for pattern in validation_patterns:
            if re.search(pattern, paper_content, re.IGNORECASE):
                methodology['validation_methods'].append(pattern)
        
        return methodology
    
    def _analyze_simulation_code(self, simulation_files: List[Path]) -> Dict[str, Any]:
        """Analyze simulation code for parameters and methodology."""
        analysis = {
            'parameters_found': {},
            'random_seeds': [],
            'sample_sizes': [],
            'statistical_functions': [],
            'imports': [],
            'hardware_usage': [],
            'reproducibility_measures': []
        }
        
        for file_path in simulation_files:
            if not file_path.exists():
                continue
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # Analyze Python files
                if file_path.suffix == '.py':
                    analysis = self._analyze_python_file(content, analysis)
                
                # Analyze other formats as needed
                elif file_path.suffix in ['.ipynb', '.r', '.m']:
                    # Placeholder for other file types
                    pass
                    
            except Exception as e:
                print(f"âš  Failed to analyze {file_path}: {e}")
        
        return analysis
    
    def _analyze_python_file(self, content: str, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze Python simulation file."""
        try:
            # Parse AST for detailed analysis
            tree = ast.parse(content)
            
            # Extract imports
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        analysis['imports'].append(alias.name)
                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        analysis['imports'].append(node.module)
            
            # Extract variable assignments
            for node in ast.walk(tree):
                if isinstance(node, ast.Assign):
                    for target in node.targets:
                        if isinstance(target, ast.Name):
                            var_name = target.id.lower()
                            
                            # Check for important parameters
                            if any(param in var_name for param in self.required_justifications):
                                try:
                                    if isinstance(node.value, ast.Constant):
                                        analysis['parameters_found'][var_name] = node.value.value
                                    elif isinstance(node.value, ast.Num):  # Python < 3.8
                                        analysis['parameters_found'][var_name] = node.value.n
                                except:
                                    pass
        
        except SyntaxError:
            # If AST parsing fails, use regex patterns
            self._analyze_with_regex(content, analysis)
        
        return analysis
    
    def _analyze_with_regex(self, content: str, analysis: Dict[str, Any]) -> None:
        """Fallback analysis using regex patterns."""
        # Extract parameter assignments
        param_patterns = [
            (r'learning_rate\s*=\s*([\d.e-]+)', 'learning_rate'),
            (r'batch_size\s*=\s*(\d+)', 'batch_size'),
            (r'epochs\s*=\s*(\d+)', 'epochs'),
            (r'random_seed\s*=\s*(\d+)', 'random_seed'),
            (r'n_samples\s*=\s*(\d+)', 'sample_size'),
            (r'sample_size\s*=\s*(\d+)', 'sample_size')
        ]
        
        for pattern, param_name in param_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if matches:
                values = [float(m) if '.' in m or 'e' in m.lower() else int(m) for m in matches]
                analysis['parameters_found'][param_name] = values[-1]  # Take last occurrence
        
        # Extract statistical functions
        stat_functions = [
            r'np\.random\.', r'random\.', r'scipy\.stats\.', 
            r'sklearn\.', r'torch\.', r'tensorflow\.'
        ]
        
        for pattern in stat_functions:
            if re.search(pattern, content):
                analysis['statistical_functions'].append(pattern)
        
        # Check for reproducibility measures
        repro_patterns = [
            r'random\.seed', r'np\.random\.seed', r'torch\.manual_seed',
            r'tf\.random\.set_seed', r'set_random_seed'
        ]
        
        for pattern in repro_patterns:
            if re.search(pattern, content):
                analysis['reproducibility_measures'].append(pattern)
    
    def _analyze_parameters(self, methodology: Dict[str, Any], 
                          code_analysis: Dict[str, Any]) -> List[ParameterAnalysis]:
        """Analyze simulation parameters for completeness and justification."""
        parameter_analyses = []
        
        # Combine parameters from paper and code
        all_parameters = {}
        all_parameters.update(methodology.get('parameters', {}))
        all_parameters.update(code_analysis.get('parameters_found', {}))
        
        for param_name, values in all_parameters.items():
            if not isinstance(values, list):
                values = [values]
            
            # Determine parameter characteristics
            value_type = self._determine_parameter_type(values)
            default_value = values[0] if values else None
            value_range = self._determine_value_range(values, value_type)
            
            # Check for justification in methodology
            justification_present = self._check_parameter_justification(
                param_name, methodology
            )
            
            # Check for uncertainty quantification
            uncertainty_quantified = self._check_uncertainty_quantification(
                param_name, methodology
            )
            
            # Calculate sensitivity score (placeholder - would need actual sensitivity analysis)
            sensitivity_score = self._estimate_sensitivity_score(param_name, values)
            
            analysis = ParameterAnalysis(
                parameter_name=param_name,
                value_type=value_type,
                default_value=default_value,
                value_range=value_range,
                sensitivity_score=sensitivity_score,
                justification_present=justification_present,
                uncertainty_quantified=uncertainty_quantified
            )
            
            parameter_analyses.append(analysis)
        
        return parameter_analyses
    
    def _determine_parameter_type(self, values: List[Any]) -> str:
        """Determine the type of parameter values."""
        if not values:
            return 'unknown'
        
        first_val = values[0]
        if isinstance(first_val, bool):
            return 'boolean'
        elif isinstance(first_val, int):
            return 'integer'
        elif isinstance(first_val, float):
            return 'float'
        elif isinstance(first_val, str):
            return 'string'
        else:
            return 'complex'
    
    def _determine_value_range(self, values: List[Any], value_type: str) -> Optional[Tuple[float, float]]:
        """Determine the range of parameter values."""
        if value_type not in ['integer', 'float'] or len(values) < 2:
            return None
        
        numeric_values = [float(v) for v in values if isinstance(v, (int, float))]
        if len(numeric_values) < 2:
            return None
        
        return (min(numeric_values), max(numeric_values))
    
    def _check_parameter_justification(self, param_name: str, methodology: Dict[str, Any]) -> bool:
        """Check if parameter choice is justified in the methodology."""
        # This would analyze the paper text for justifications
        # Simplified implementation
        return param_name not in self.required_justifications
    
    def _check_uncertainty_quantification(self, param_name: str, methodology: Dict[str, Any]) -> bool:
        """Check if parameter uncertainty is quantified."""
        # Look for sensitivity analysis or uncertainty mentions
        return 'sensitivity analysis' in methodology.get('validation_methods', [])
    
    def _estimate_sensitivity_score(self, param_name: str, values: List[Any]) -> float:
        """Estimate parameter sensitivity score."""
        # Simplified scoring based on parameter importance
        high_sensitivity_params = ['learning_rate', 'temperature', 'noise_level']
        medium_sensitivity_params = ['batch_size', 'epochs']
        
        if param_name in high_sensitivity_params:
            return 0.8
        elif param_name in medium_sensitivity_params:
            return 0.5
        else:
            return 0.3
    
    def _perform_power_analysis(self, methodology: Dict[str, Any], 
                              code_analysis: Dict[str, Any]) -> Optional[StatisticalPowerAnalysis]:
        """Perform statistical power analysis."""
        sample_sizes = methodology.get('sample_sizes', [])
        if not sample_sizes:
            sample_sizes = code_analysis.get('sample_sizes', [])
        
        if not sample_sizes or not HAS_SCIPY:
            return None
        
        # Use the largest sample size found
        sample_size = max(sample_sizes)
        
        # Estimate effect size (would be better to extract from results)
        effect_size = 0.5  # Medium effect size as default
        significance_level = 0.05
        
        try:
            # Calculate power for t-test (simplified)
            power = self._calculate_statistical_power(sample_size, effect_size, significance_level)
            
            # Calculate recommended sample size for 80% power
            recommended_size = self._calculate_required_sample_size(effect_size, significance_level, 0.8)
            
            return StatisticalPowerAnalysis(
                sample_size=sample_size,
                effect_size=effect_size,
                statistical_power=power,
                significance_level=significance_level,
                recommended_sample_size=recommended_size,
                power_sufficient=power >= self.min_power,
                analysis_method='t-test'
            )
            
        except Exception as e:
            print(f"âš  Power analysis failed: {e}")
            return None
    
    def _calculate_statistical_power(self, n: int, effect_size: float, alpha: float) -> float:
        """Calculate statistical power for given parameters."""
        if not HAS_SCIPY:
            return 0.5  # Default if scipy not available
        
        try:
            # For two-sample t-test
            # Calculate critical value
            t_critical = stats.t.ppf(1 - alpha/2, df=2*n-2)
            
            # Calculate non-centrality parameter
            ncp = effect_size * np.sqrt(n/2)
            
            # Calculate power
            power = 1 - stats.nct.cdf(t_critical, df=2*n-2, nc=ncp) + stats.nct.cdf(-t_critical, df=2*n-2, nc=ncp)
            
            return max(0.0, min(1.0, power))
            
        except Exception:
            return 0.5
    
    def _calculate_required_sample_size(self, effect_size: float, alpha: float, power: float) -> int:
        """Calculate required sample size for desired power."""
        if not HAS_SCIPY:
            return max(30, int(16 / (effect_size ** 2)))  # Rough approximation
        
        try:
            # Binary search for required sample size
            min_n, max_n = 10, 10000
            
            while max_n - min_n > 1:
                mid_n = (min_n + max_n) // 2
                calculated_power = self._calculate_statistical_power(mid_n, effect_size, alpha)
                
                if calculated_power >= power:
                    max_n = mid_n
                else:
                    min_n = mid_n
            
            return max_n
            
        except Exception:
            return max(30, int(16 / (effect_size ** 2)))
    
    def _perform_robustness_analysis(self, methodology: Dict[str, Any], 
                                   code_analysis: Dict[str, Any],
                                   model: str, request_timeout: int) -> List[RobustnessTest]:
        """Perform robustness analysis of simulation parameters."""
        robustness_tests = []
        
        # Get key parameters for robustness testing
        parameters = code_analysis.get('parameters_found', {})
        
        for param_name, param_value in parameters.items():
            if param_name in ['learning_rate', 'batch_size', 'temperature']:
                # Simulate robustness test
                test = self._simulate_robustness_test(param_name, param_value)
                if test:
                    robustness_tests.append(test)
        
        return robustness_tests
    
    def _simulate_robustness_test(self, param_name: str, base_value: Any) -> Optional[RobustnessTest]:
        """Simulate a robustness test for a parameter."""
        if not isinstance(base_value, (int, float)):
            return None
        
        # Define variation ranges
        variations = ['small', 'medium', 'large']
        selected_variation = 'medium'  # Could be randomized or systematically chosen
        
        variation_pct = self.robustness_variations[selected_variation]
        min_val = base_value * (1 - variation_pct)
        max_val = base_value * (1 + variation_pct)
        
        # Simulate stability score (would be calculated from actual experiments)
        if param_name == 'learning_rate':
            stability = 0.6  # Learning rate typically has medium stability
        elif param_name == 'batch_size':
            stability = 0.8  # Batch size typically more stable
        elif param_name == 'temperature':
            stability = 0.4  # Temperature often sensitive
        else:
            stability = 0.7  # Default
        
        # Add some randomness to make it more realistic
        stability += np.random.normal(0, 0.1)
        stability = max(0.0, min(1.0, stability))
        
        return RobustnessTest(
            test_name=f"{param_name}_robustness",
            parameter_varied=param_name,
            variation_range=f"Â±{variation_pct*100:.0f}%",
            result_stability=stability,
            critical_thresholds=[min_val, max_val],
            robustness_score=stability
        )
    
    def _identify_simulation_issues(self, parameter_analyses: List[ParameterAnalysis],
                                  power_analysis: Optional[StatisticalPowerAnalysis],
                                  robustness_tests: List[RobustnessTest],
                                  methodology: Dict[str, Any]) -> List[SimulationValidationIssue]:
        """Identify issues in simulation methodology."""
        issues = []
        
        # Parameter issues
        for param in parameter_analyses:
            if not param.justification_present and param.parameter_name in self.required_justifications:
                issues.append(SimulationValidationIssue(
                    severity="warning",
                    issue_type="parameter_justification",
                    description=f"Parameter '{param.parameter_name}' lacks justification",
                    location="methodology",
                    recommendations=[f"Provide justification for {param.parameter_name} choice"],
                    validation_code=None
                ))
            
            if param.sensitivity_score > 0.7 and not param.uncertainty_quantified:
                issues.append(SimulationValidationIssue(
                    severity="warning",
                    issue_type="uncertainty",
                    description=f"High-sensitivity parameter '{param.parameter_name}' lacks uncertainty analysis",
                    location="methodology",
                    recommendations=["Perform sensitivity analysis for critical parameters"],
                    validation_code=None
                ))
        
        # Statistical power issues
        if power_analysis:
            if not power_analysis.power_sufficient:
                issues.append(SimulationValidationIssue(
                    severity="critical",
                    issue_type="statistical_power",
                    description=f"Statistical power ({power_analysis.statistical_power:.3f}) below recommended threshold ({self.min_power})",
                    location="experimental_design",
                    recommendations=[
                        f"Increase sample size to at least {power_analysis.recommended_sample_size}",
                        "Consider larger effect sizes or different statistical tests"
                    ],
                    validation_code=None
                ))
            
            if power_analysis.sample_size < self.min_sample_size:
                issues.append(SimulationValidationIssue(
                    severity="warning",
                    issue_type="sample_size",
                    description=f"Sample size ({power_analysis.sample_size}) below recommended minimum ({self.min_sample_size})",
                    location="experimental_design",
                    recommendations=["Increase sample size for more reliable results"],
                    validation_code=None
                ))
        
        # Robustness issues
        unstable_tests = [test for test in robustness_tests if test.robustness_score < 0.5]
        if unstable_tests:
            issues.append(SimulationValidationIssue(
                severity="warning",
                issue_type="robustness",
                description=f"{len(unstable_tests)} parameters show low robustness",
                location="parameter_sensitivity",
                recommendations=[
                    "Investigate parameter sensitivity",
                    "Consider alternative parameter values",
                    "Report sensitivity analysis results"
                ],
                validation_code=None
            ))
        
        # Reproducibility issues
        if not methodology.get('random_seeds') and 'reproducibility_measures' not in methodology:
            issues.append(SimulationValidationIssue(
                severity="warning",
                issue_type="reproducibility",
                description="No random seeds or reproducibility measures mentioned",
                location="implementation",
                recommendations=[
                    "Set and report random seeds",
                    "Provide code for reproducibility",
                    "Document software versions"
                ],
                validation_code="np.random.seed(42)"
            ))
        
        return issues
    
    def _calculate_methodology_score(self, parameter_analyses: List[ParameterAnalysis],
                                   methodology: Dict[str, Any]) -> float:
        """Calculate methodology quality score."""
        if not parameter_analyses:
            return 0.5
        
        score = 0.0
        max_score = 4.0
        
        # Parameter justification score
        justified_params = sum(1 for p in parameter_analyses if p.justification_present)
        justification_score = justified_params / len(parameter_analyses)
        score += justification_score
        
        # Uncertainty quantification score
        quantified_params = sum(1 for p in parameter_analyses if p.uncertainty_quantified)
        if parameter_analyses:
            uncertainty_score = quantified_params / len(parameter_analyses)
            score += uncertainty_score
        
        # Validation methods score
        validation_methods = methodology.get('validation_methods', [])
        if len(validation_methods) >= 2:
            score += 1.0
        elif len(validation_methods) >= 1:
            score += 0.5
        
        # Statistical tests score
        statistical_tests = methodology.get('statistical_tests', [])
        if len(statistical_tests) >= 1:
            score += 1.0
        else:
            score += 0.3  # Partial credit
        
        return score / max_score
    
    def _calculate_statistical_rigor_score(self, power_analysis: Optional[StatisticalPowerAnalysis],
                                         robustness_tests: List[RobustnessTest]) -> float:
        """Calculate statistical rigor score."""
        score = 0.0
        max_score = 3.0
        
        # Power analysis score
        if power_analysis:
            if power_analysis.power_sufficient:
                score += 1.5
            elif power_analysis.statistical_power >= 0.6:
                score += 1.0
            else:
                score += 0.5
        
        # Robustness testing score
        if robustness_tests:
            avg_robustness = sum(test.robustness_score for test in robustness_tests) / len(robustness_tests)
            score += avg_robustness * 1.5
        
        return score / max_score
    
    def _calculate_reproducibility_score(self, parameter_analyses: List[ParameterAnalysis],
                                       code_analysis: Dict[str, Any]) -> float:
        """Calculate reproducibility score."""
        score = 0.0
        max_score = 3.0
        
        # Parameter documentation score
        if parameter_analyses:
            documented_params = len(parameter_analyses)
            score += min(1.0, documented_params / 5)  # Up to 1 point for parameter documentation
        
        # Reproducibility measures score
        repro_measures = code_analysis.get('reproducibility_measures', [])
        if len(repro_measures) >= 2:
            score += 1.0
        elif len(repro_measures) >= 1:
            score += 0.5
        
        # Code availability score (simplified)
        if code_analysis.get('parameters_found'):
            score += 1.0  # Assume code is available if we found parameters
        
        return score / max_score
    
    def _generate_simulation_recommendations(self, parameter_analyses: List[ParameterAnalysis],
                                           power_analysis: Optional[StatisticalPowerAnalysis],
                                           robustness_tests: List[RobustnessTest],
                                           issues: List[SimulationValidationIssue]) -> List[str]:
        """Generate simulation improvement recommendations."""
        recommendations = []
        
        # Issue-based recommendations
        critical_issues = [i for i in issues if i.severity == 'critical']
        if critical_issues:
            recommendations.append(f"Address {len(critical_issues)} critical simulation issues immediately")
        
        # Power analysis recommendations
        if power_analysis and not power_analysis.power_sufficient:
            recommendations.append(
                f"Increase sample size from {power_analysis.sample_size} to {power_analysis.recommended_sample_size} "
                f"for adequate statistical power"
            )
        
        # Parameter recommendations
        unjustified_params = [p for p in parameter_analyses if not p.justification_present]
        if unjustified_params:
            param_names = [p.parameter_name for p in unjustified_params[:3]]  # Limit to first 3
            recommendations.append(f"Provide justification for parameters: {', '.join(param_names)}")
        
        # Robustness recommendations
        if robustness_tests:
            unstable_tests = [test for test in robustness_tests if test.robustness_score < 0.6]
            if unstable_tests:
                recommendations.append("Investigate sensitivity of unstable parameters and report findings")
        
        # Reproducibility recommendations
        repro_issues = [i for i in issues if i.issue_type == 'reproducibility']
        if repro_issues:
            recommendations.append("Improve reproducibility by setting random seeds and documenting software versions")
        
        # General recommendations
        if not recommendations:
            recommendations.append("Simulation methodology appears sound - consider minor improvements in documentation")
        
        return recommendations
    
    def _generate_validation_summary(self, param_count: int, 
                                   power_analysis: Optional[StatisticalPowerAnalysis],
                                   robustness_count: int, methodology_score: float,
                                   statistical_rigor_score: float, reproducibility_score: float) -> str:
        """Generate simulation validation summary."""
        summary = []
        
        summary.append(f"Simulation Validation Summary:")
        summary.append(f"- Parameters analyzed: {param_count}")
        summary.append(f"- Statistical power analysis: {'Available' if power_analysis else 'Not performed'}")
        if power_analysis:
            summary.append(f"  â€¢ Sample size: {power_analysis.sample_size}")
            summary.append(f"  â€¢ Statistical power: {power_analysis.statistical_power:.3f}")
            summary.append(f"  â€¢ Power sufficient: {power_analysis.power_sufficient}")
        summary.append(f"- Robustness tests: {robustness_count}")
        summary.append(f"- Methodology score: {methodology_score:.2f}")
        summary.append(f"- Statistical rigor score: {statistical_rigor_score:.2f}")
        summary.append(f"- Reproducibility score: {reproducibility_score:.2f}")
        
        # Overall assessment
        overall_score = (methodology_score + statistical_rigor_score + reproducibility_score) / 3
        if overall_score >= 0.8:
            summary.append("- Overall assessment: EXCELLENT")
        elif overall_score >= 0.6:
            summary.append("- Overall assessment: GOOD")
        elif overall_score >= 0.4:
            summary.append("- Overall assessment: ACCEPTABLE")
        else:
            summary.append("- Overall assessment: NEEDS IMPROVEMENT")
        
        return "\n".join(summary)
    
    def generate_simulation_report(self, validation: SimulationValidationResult) -> str:
        """Generate comprehensive simulation validation report."""
        report = []
        report.append("=" * 80)
        report.append("ADVANCED SIMULATION VALIDATION REPORT")
        report.append("=" * 80)
        report.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # Summary
        report.append("VALIDATION SUMMARY:")
        for line in validation.validation_summary.split('\n'):
            report.append(f"  {line}")
        report.append("")
        
        # Parameter analysis
        if validation.parameter_analyses:
            report.append("PARAMETER ANALYSIS:")
            for param in validation.parameter_analyses:
                report.append(f"  {param.parameter_name}:")
                report.append(f"    Type: {param.value_type}")
                report.append(f"    Value: {param.default_value}")
                report.append(f"    Sensitivity: {param.sensitivity_score:.2f}")
                report.append(f"    Justified: {param.justification_present}")
                report.append(f"    Uncertainty Quantified: {param.uncertainty_quantified}")
            report.append("")
        
        # Power analysis
        if validation.power_analysis:
            pa = validation.power_analysis
            report.append("STATISTICAL POWER ANALYSIS:")
            report.append(f"  Sample Size: {pa.sample_size}")
            report.append(f"  Effect Size: {pa.effect_size}")
            report.append(f"  Statistical Power: {pa.statistical_power:.3f}")
            report.append(f"  Significance Level: {pa.significance_level}")
            report.append(f"  Recommended Sample Size: {pa.recommended_sample_size}")
            report.append(f"  Power Sufficient: {pa.power_sufficient}")
            report.append("")
        
        # Robustness tests
        if validation.robustness_tests:
            report.append("ROBUSTNESS ANALYSIS:")
            for test in validation.robustness_tests:
                report.append(f"  {test.test_name}:")
                report.append(f"    Parameter: {test.parameter_varied}")
                report.append(f"    Variation Range: {test.variation_range}")
                report.append(f"    Stability: {test.result_stability:.3f}")
                report.append(f"    Robustness Score: {test.robustness_score:.3f}")
            report.append("")
        
        # Issues
        if validation.identified_issues:
            report.append("IDENTIFIED ISSUES:")
            critical_issues = [i for i in validation.identified_issues if i.severity == 'critical']
            warning_issues = [i for i in validation.identified_issues if i.severity == 'warning']
            
            if critical_issues:
                report.append("  CRITICAL:")
                for issue in critical_issues:
                    report.append(f"    â€¢ {issue.description}")
                    for rec in issue.recommendations:
                        report.append(f"      â†’ {rec}")
            
            if warning_issues:
                report.append("  WARNING:")
                for issue in warning_issues:
                    report.append(f"    â€¢ {issue.description}")
            
            report.append("")
        
        # Recommendations
        if validation.recommendations:
            report.append("RECOMMENDATIONS:")
            for rec in validation.recommendations:
                report.append(f"  â€¢ {rec}")
            report.append("")
        
        report.append("=" * 80)
        
        return "\n".join(report)
