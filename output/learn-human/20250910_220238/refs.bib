@article{Baddeley2012WorkingMemory,
  author = {Baddeley, Alan},
  title = {Working memory: theories, models, and controversies},
  journal = {Annual Review of Psychology},
  year = {2012},
  volume = {63},
  pages = {1--29},
  doi = {10.1146/annurev-psych-120710-100422}
}
@article{Botvinick2001ConflictMonitoring,
  author = {Botvinick, Matthew M. and Braver, Todd S. and Barch, Deanna M. and Carter, Cameron S. and Cohen, Jonathan D.},
  title = {Conflict monitoring and cognitive control},
  journal = {Psychological Review},
  year = {2001},
  volume = {108},
  number = {3},
  pages = {624--652},
  doi = {10.1037/0033-295X.108.3.624}
}
@article{Fries2015Rhythms,
  author = {Fries, Pascal},
  title = {Rhythms for Cognition: Communication through Coherence},
  journal = {Neuron},
  year = {2015},
  volume = {88},
  number = {1},
  pages = {220--235},
  doi = {10.1016/j.neuron.2015.09.034}
}
@article{Mattar2018PrioritizedReplay,
  author = {Mattar, Marcelo G. and Daw, Nathaniel D.},
  title = {Prioritized memory access explains planning and hippocampal replay},
  journal = {Nature Neuroscience},
  year = {2018},
  volume = {21},
  pages = {1609--1617},
  doi = {10.1038/s41593-018-0232-z}
}
@article{Gershman2018Hippocampus,
  author = {Gershman, Samuel J.},
  title = {What does the hippocampus do?},
  journal = {Trends in Cognitive Sciences},
  year = {2018},
  volume = {22},
  number = {7},
  pages = {512--524},
  doi = {10.1016/j.tics.2018.07.004}
}
@article{Wei2022CoT,
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  journal = {arXiv},
  year = {2022},
  eprint = {2201.11903},
  doi = {10.48550/arXiv.2201.11903}
}
@article{Kojima2022ZeroShot,
  author = {Kojima, Takeshi and Srikumar, Vivek and Sch{\"u}tze, Hinrich and Li, Yutaka and Sugawara, Saku and Sinha, Karthik and Shimizu, Nobuyuki},
  title = {Large Language Models are Zero-Shot Reasoners},
  journal = {arXiv},
  year = {2022},
  eprint = {2205.11916},
  doi = {10.48550/arXiv.2205.11916}
}
@inproceedings{Wang2023SelfConsistency,
  author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  title = {Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  booktitle = {International Conference on Learning Representations},
  year = {2023},
  doi = {10.48550/arXiv.2203.11171}
}
@article{Yao2023ReAct,
  author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Cao, Nan and Yu, Karthik Narasimhan and others},
  title = {ReAct: Synergizing Reasoning and Acting in Language Models},
  journal = {arXiv},
  year = {2023},
  eprint = {2210.03629},
  doi = {10.48550/arXiv.2210.03629}
}
@article{Yao2023ToT,
  author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Liang, Percy and Narasimhan, Karthik},
  title = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
  journal = {arXiv},
  year = {2023},
  eprint = {2305.10601},
  doi = {10.48550/arXiv.2305.10601}
}
@article{Shinn2023Reflexion,
  author = {Shinn, Noah and Labash, Behnam and Gopinath, Ashwin},
  title = {Reflexion: Language Agents with Verbal Reinforcement Learning},
  journal = {arXiv},
  year = {2023},
  eprint = {2303.11366},
  doi = {10.48550/arXiv.2303.11366}
}
@article{Madaan2023SelfRefine,
  author = {Madaan, Aman and Yazdanbakhsh, Amir and Abhishek, Sahil and others},
  title = {Self-Refine: Iterative Refinement with Self-Feedback},
  journal = {arXiv},
  year = {2023},
  eprint = {2303.17651},
  doi = {10.48550/arXiv.2303.17651}
}
@article{Zelikman2022STAR,
  author = {Zelikman, Eric and Wu, Yuhuai and Li, Jesse Muqing and Goodman, Noah D.},
  title = {STaR: Bootstrapping Reasoning With Reasoning},
  journal = {Transactions on Machine Learning Research},
  year = {2022},
  note = {ISSN 2835-8856},
  doi = {10.48550/arXiv.2203.14465}
}
@article{Lightman2023LetsVerify,
  author = {Lightman, Hunter and Welleck, Sean and Ouyang, Long and Olsson, Catherine and others},
  title = {Let's Verify Step by Step},
  journal = {arXiv},
  year = {2023},
  eprint = {2305.20050},
  doi = {10.48550/arXiv.2305.20050}
}
@article{OpenAI2023GPT4,
  author = {{OpenAI}},
  title = {GPT-4 Technical Report},
  journal = {arXiv},
  year = {2023},
  eprint = {2303.08774},
  doi = {10.48550/arXiv.2303.08774}
}
@article{Bubeck2023Sparks,
  author = {Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and others},
  title = {Sparks of Artificial General Intelligence: Early experiments with GPT-4},
  journal = {arXiv},
  year = {2023},
  eprint = {2303.12712},
  doi = {10.48550/arXiv.2303.12712}
}
@article{Kaplan2020ScalingLaws,
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and others},
  title = {Scaling Laws for Neural Language Models},
  journal = {arXiv},
  year = {2020},
  eprint = {2001.08361},
  doi = {10.48550/arXiv.2001.08361}
}
@article{Hoffmann2022Chinchilla,
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and others},
  title = {Training Compute-Optimal Large Language Models},
  journal = {arXiv},
  year = {2022},
  eprint = {2203.15556},
  doi = {10.48550/arXiv.2203.15556}
}
@article{Lewis2020RAG,
  author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and others},
  title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP},
  journal = {arXiv},
  year = {2020},
  eprint = {2005.11401},
  doi = {10.48550/arXiv.2005.11401}
}
@inproceedings{Brown2020GPT3,
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and others},
  title = {Language Models are Few-Shot Learners},
  booktitle = {Advances in Neural Information Processing Systems},
  year = {2020},
  doi = {10.48550/arXiv.2005.14165}
}
