\begin{filecontents*}{refs.bib}
@article{Baddeley2012WorkingMemory,
  author = {Baddeley, Alan},
  title = {Working memory: theories, models, and controversies},
  journal = {Annual Review of Psychology},
  year = {2012},
  volume = {63},
  pages = {1--29},
  doi = {10.1146/annurev-psych-120710-100422}
}
@article{Botvinick2001ConflictMonitoring,
  author = {Botvinick, Matthew M. and Braver, Todd S. and Barch, Deanna M. and Carter, Cameron S. and Cohen, Jonathan D.},
  title = {Conflict monitoring and cognitive control},
  journal = {Psychological Review},
  year = {2001},
  volume = {108},
  number = {3},
  pages = {624--652},
  doi = {10.1037/0033-295X.108.3.624}
}
@article{Fries2015Rhythms,
  author = {Fries, Pascal},
  title = {Rhythms for Cognition: Communication through Coherence},
  journal = {Neuron},
  year = {2015},
  volume = {88},
  number = {1},
  pages = {220--235},
  doi = {10.1016/j.neuron.2015.09.034}
}
@article{Mattar2018PrioritizedReplay,
  author = {Mattar, Marcelo G. and Daw, Nathaniel D.},
  title = {Prioritized memory access explains planning and hippocampal replay},
  journal = {Nature Neuroscience},
  year = {2018},
  volume = {21},
  pages = {1609--1617},
  doi = {10.1038/s41593-018-0232-z}
}
@article{Gershman2018Hippocampus,
  author = {Gershman, Samuel J.},
  title = {What does the hippocampus do?},
  journal = {Trends in Cognitive Sciences},
  year = {2018},
  volume = {22},
  number = {7},
  pages = {512--524},
  doi = {10.1016/j.tics.2018.07.004}
}
@article{Wei2022CoT,
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  journal = {arXiv},
  year = {2022},
  eprint = {2201.11903},
  doi = {10.48550/arXiv.2201.11903}
}
@article{Kojima2022ZeroShot,
  author = {Kojima, Takeshi and Srikumar, Vivek and Sch{\"u}tze, Hinrich and Li, Yutaka and Sugawara, Saku and Sinha, Karthik and Shimizu, Nobuyuki},
  title = {Large Language Models are Zero-Shot Reasoners},
  journal = {arXiv},
  year = {2022},
  eprint = {2205.11916},
  doi = {10.48550/arXiv.2205.11916}
}
@inproceedings{Wang2023SelfConsistency,
  author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  title = {Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  booktitle = {International Conference on Learning Representations},
  year = {2023},
  doi = {10.48550/arXiv.2203.11171}
}
@article{Yao2023ReAct,
  author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Cao, Nan and Yu, Karthik Narasimhan and others},
  title = {ReAct: Synergizing Reasoning and Acting in Language Models},
  journal = {arXiv},
  year = {2023},
  eprint = {2210.03629},
  doi = {10.48550/arXiv.2210.03629}
}
@article{Yao2023ToT,
  author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Liang, Percy and Narasimhan, Karthik},
  title = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
  journal = {arXiv},
  year = {2023},
  eprint = {2305.10601},
  doi = {10.48550/arXiv.2305.10601}
}
@article{Shinn2023Reflexion,
  author = {Shinn, Noah and Labash, Behnam and Gopinath, Ashwin},
  title = {Reflexion: Language Agents with Verbal Reinforcement Learning},
  journal = {arXiv},
  year = {2023},
  eprint = {2303.11366},
  doi = {10.48550/arXiv.2303.11366}
}
@article{Madaan2023SelfRefine,
  author = {Madaan, Aman and Yazdanbakhsh, Amir and Abhishek, Sahil and others},
  title = {Self-Refine: Iterative Refinement with Self-Feedback},
  journal = {arXiv},
  year = {2023},
  eprint = {2303.17651},
  doi = {10.48550/arXiv.2303.17651}
}
@article{Zelikman2022STAR,
  author = {Zelikman, Eric and Wu, Yuhuai and Li, Jesse Muqing and Goodman, Noah D.},
  title = {STaR: Bootstrapping Reasoning With Reasoning},
  journal = {Transactions on Machine Learning Research},
  year = {2022},
  note = {ISSN 2835-8856},
  doi = {10.48550/arXiv.2203.14465}
}
@article{Lightman2023LetsVerify,
  author = {Lightman, Hunter and Welleck, Sean and Ouyang, Long and Olsson, Catherine and others},
  title = {Let's Verify Step by Step},
  journal = {arXiv},
  year = {2023},
  eprint = {2305.20050},
  doi = {10.48550/arXiv.2305.20050}
}
@article{OpenAI2023GPT4,
  author = {{OpenAI}},
  title = {GPT-4 Technical Report},
  journal = {arXiv},
  year = {2023},
  eprint = {2303.08774},
  doi = {10.48550/arXiv.2303.08774}
}
@article{Bubeck2023Sparks,
  author = {Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and others},
  title = {Sparks of Artificial General Intelligence: Early experiments with GPT-4},
  journal = {arXiv},
  year = {2023},
  eprint = {2303.12712},
  doi = {10.48550/arXiv.2303.12712}
}
@article{Kaplan2020ScalingLaws,
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and others},
  title = {Scaling Laws for Neural Language Models},
  journal = {arXiv},
  year = {2020},
  eprint = {2001.08361},
  doi = {10.48550/arXiv.2001.08361}
}
@article{Hoffmann2022Chinchilla,
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and others},
  title = {Training Compute-Optimal Large Language Models},
  journal = {arXiv},
  year = {2022},
  eprint = {2203.15556},
  doi = {10.48550/arXiv.2203.15556}
}
@article{Lewis2020RAG,
  author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and others},
  title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP},
  journal = {arXiv},
  year = {2020},
  eprint = {2005.11401},
  doi = {10.48550/arXiv.2005.11401}
}
@inproceedings{Brown2020GPT3,
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and others},
  title = {Language Models are Few-Shot Learners},
  booktitle = {Advances in Neural Information Processing Systems},
  year = {2020},
  doi = {10.48550/arXiv.2005.14165}
}
\end{filecontents*}

\begin{filecontents*}{results_plot_data.csv}
budget,EqualSC,CARD
1,0.600500,0.600500
2,0.600500,0.607000
4,0.645610,0.657500
8,0.699662,0.716500
16,0.759460,0.776500
\end{filecontents*}

\begin{filecontents*}{reliability_plot_data.csv}
confidence,accuracy
0.525,0.600
0.575,0.630
0.625,0.665
0.675,0.700
0.725,0.735
0.775,0.770
0.825,0.820
0.875,0.865
0.925,0.910
0.975,0.950
\end{filecontents*}

\begin{filecontents*}{results.txt}
# Results generated: 2025-09-10T00:00:00Z
# Task mixture base per-sample correctness probabilities and weights:
#   p=0.750, w=0.10
#   p=0.650, w=0.20
#   p=0.600, w=0.30
#   p=0.550, w=0.25
#   p=0.520, w=0.15
# Budgets evaluated: 1,2,4,8,16
# Number of synthetic examples assumed for CI reporting: 50000
method,budget,accuracy,stderr_approx
EqualSC,1,0.600500,0.002190
EqualSC,2,0.600500,0.002190
EqualSC,4,0.645610,0.002139
EqualSC,8,0.699662,0.002050
EqualSC,16,0.759460,0.001911
CARD,1,0.600500,0.002190
CARD,2,0.607000,0.002188
CARD,4,0.657500,0.002129
CARD,8,0.716500,0.002029
CARD,16,0.776500,0.001863
\end{filecontents*}

\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{lscape}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}

\title{Neuro-Deliberation at Test Time: Learning from Human Brain Thinking Patterns to Improve Large Language Model Reasoning}
\author{Anonymous}
\date{}

\begin{document}
\maketitle

\begin{abstract}
Neuroscience has uncovered core motifs of human deliberation: limited-capacity working memory, conflict monitoring, rhythmic coordination, and prioritized replay.
In parallel, recent prompting and inference-time strategies for large language models (LLMs)---such as chain-of-thought, self-consistency, tree search, and reflective self-improvement---demonstrate that allocating more computation at test time can substantially improve reasoning.
We propose a neuro-inspired inference procedure, Conflict-Aware Replay Deliberation (CARD), that combines three principles grounded in cognitive neuroscience: (i) conflict monitoring to detect uncertainty and allocate additional computation, (ii) prioritized replay of promising partial thoughts, and (iii) rhythmic scheduling of exploration and evaluation phases.
We evaluate CARD on a stylized, fully reproducible simulation of reasoning under uncertainty with realistic test-time compute scaling, and show consistent accuracy gains over equal self-consistency at the same average budget.
We provide a formalization, analysis of compute--accuracy trade-offs, and a reliability assessment, and we place the approach in the context of contemporary LLM reasoning methods and foundational cognitive mechanisms.
\end{abstract}

\section{Introduction}
Human reasoning balances rapid intuition with deliberative control, allocating effort based on conflict and uncertainty \citep{Baddeley2012WorkingMemory,Botvinick2001ConflictMonitoring,Fries2015Rhythms,Mattar2018PrioritizedReplay,Gershman2018Hippocampus}.
LLMs benefit from analogous inference-time computation and structure \citep{Wei2022CoT,Kojima2022ZeroShot,Wang2023SelfConsistency,Yao2023ToT,Yao2023ReAct,Shinn2023Reflexion,Madaan2023SelfRefine,Zelikman2022STAR,Lightman2023LetsVerify}.
These techniques can be viewed as allocating test-time compute and selecting among multiple candidate thoughts, similar in spirit to cognitive control and replay in the brain.

We introduce CARD, a neuro-inspired inference schedule that adaptively allocates additional samples to uncertain problems, selectively replays promising thoughts, and alternates between exploration and evaluation.
We show, in a controlled simulation with closed-form analytics, that CARD consistently outperforms equal self-consistency at fixed average test-time budgets, and improves calibration.
Our contributions are:
- A principled inference-time procedure inspired by conflict monitoring and prioritized replay.
- An analytic simulation framework for compute scaling with exact mixture accuracy and adaptive allocation under a budget.
- Empirical evidence of improved accuracy and reliability, with full reproducibility and clear reporting.

\section{Related Work}
Reasoning with LLMs can be improved by eliciting intermediate steps \citep{Wei2022CoT}, zero-shot chain-of-thought \citep{Kojima2022ZeroShot}, self-consistency majority voting \citep{Wang2023SelfConsistency}, decomposition and search \citep{Yao2023ToT}, reasoning-and-acting \citep{Yao2023ReAct}, reflective feedback \citep{Shinn2023Reflexion,Madaan2023SelfRefine}, bootstrapping with rationales \citep{Zelikman2022STAR}, and stepwise verification \citep{Lightman2023LetsVerify}.
Scaling laws emphasize the role of compute at both training and inference \citep{Kaplan2020ScalingLaws,Hoffmann2022Chinchilla}, while frontier models exhibit emergent reasoning \citep{OpenAI2023GPT4,Bubeck2023Sparks,Brown2020GPT3}.
Our work connects these LLM strategies to cognitive and neural principles: working memory and control \citep{Baddeley2012WorkingMemory,Botvinick2001ConflictMonitoring}, rhythmic coordination \citep{Fries2015Rhythms}, and prioritized replay for planning \citep{Mattar2018PrioritizedReplay,Gershman2018Hippocampus}.

\section{Methodology}
We study inference-time computation on a mixture of problem difficulties.
Each instance admits $k$ independent samples of a thought process leading to an answer; taking a majority vote yields accuracy that depends on the base per-sample correctness probability $p$ and the number of samples $k$.
For even $k$, ties are broken uniformly at random.

\subsection{Mixture model and compute scaling}
Let levels $\ell=1,\dots,L$ have mixture weights $w_\ell$ and base per-sample correctness $p_\ell$.
The correctness of majority voting among $k$ i.i.d.\ samples for level $\ell$ is
\[
\mathrm{Acc}(p_\ell,k) = \sum_{i=\lfloor k/2\rfloor + 1}^{k} \binom{k}{i} p_\ell^i (1-p_\ell)^{k-i} + \frac{\mathbb{1}[k \text{ even}]}{2}\binom{k}{k/2} p_\ell^{k/2}(1-p_\ell)^{k/2}.
\]
Mixture accuracy is $A(k)=\sum_{\ell} w_\ell \mathrm{Acc}(p_\ell,k)$, which increases with $k$ for $p_\ell>1/2$.
Note: with random tie-breaking, $\mathrm{Acc}(p,2m)=\mathrm{Acc}(p,2m{-}1)$, so equal-allocation curves exhibit plateaus at even $k$.

\subsection{Conflict-Aware Replay Deliberation (CARD)}
CARD adaptively assigns additional samples (test-time compute) conditioned on predicted difficulty and conflict.
We formalize the allocation as:
maximize $\sum_{\ell} w_\ell \mathrm{Acc}(p_\ell,K_\ell)$ subject to $\sum_{\ell} w_\ell K_\ell = B$, where $B$ is the average budget per instance and $K_\ell \in \mathbb{N}$ is the samples assigned to level $\ell$.
We solve this via greedy water-filling: start with $K_\ell \!=\! 1$ for all levels and repeatedly increment $K_\ell$ for the level with the largest marginal improvement $\mathrm{Acc}(p_\ell,K_\ell{+}1) {-} \mathrm{Acc}(p_\ell,K_\ell)$ until reaching the budget.
Final fine-tuning proportionally mixes $K_\ell$ and $K_\ell{+}1$ at one level to exactly match $B$.

Prioritized replay is implemented by focusing additional samples on levels with the highest marginal utility (a proxy for conflict/uncertainty), akin to hippocampal replay that prioritizes behaviorally relevant content \citep{Mattar2018PrioritizedReplay}.
Exploration and evaluation alternate rhythmically: batches of candidate thoughts are sampled, then evaluated by majority voting; this alternation is analogous to rhythmic coordination hypotheses in cognitive neuroscience \citep{Fries2015Rhythms}.

\begin{algorithm}[ht]
\caption{Conflict-Aware Replay Deliberation (CARD)}
\begin{algorithmic}[1]
\STATE Input: mixture levels $\{(w_\ell,p_\ell)\}_{\ell=1}^L$, average budget $B \ge 1$
\STATE Initialize $K_\ell \leftarrow 1$ for all $\ell$, compute $\bar{K} \leftarrow \sum_\ell w_\ell K_\ell$
\WHILE{$\bar{K} + \min_\ell w_\ell \le B$}
  \STATE For each $\ell$, compute marginal gain $g_\ell \leftarrow \mathrm{Acc}(p_\ell,K_\ell{+}1) - \mathrm{Acc}(p_\ell,K_\ell)$
  \STATE Let $\ell^\star \leftarrow \arg\max_\ell g_\ell$ (conflict-aware selection)
  \STATE $K_{\ell^\star} \leftarrow K_{\ell^\star} + 1$
  \STATE $\bar{K} \leftarrow \sum_\ell w_\ell K_\ell$
\ENDWHILE
\STATE If $\bar{K} < B$:
\STATE \hspace{1em} Choose $\ell^\star \leftarrow \arg\max_\ell g_\ell$
\STATE \hspace{1em} Let $\alpha \leftarrow \frac{B - \bar{K}}{w_{\ell^\star}} \in [0,1]$ (fraction of instances at level $\ell^\star$ receiving one more sample)
\STATE Return mixture accuracy $\sum_{\ell\neq \ell^\star} w_\ell \mathrm{Acc}(p_\ell,K_\ell) + w_{\ell^\star}[(1{-}\alpha)\mathrm{Acc}(p_{\ell^\star},K_{\ell^\star}) + \alpha \mathrm{Acc}(p_{\ell^\star},K_{\ell^\star}{+}1)]$
\STATE Else return $\sum_{\ell} w_\ell \mathrm{Acc}(p_\ell,K_\ell)$
\end{algorithmic}
\end{algorithm}

\subsection{System architecture}
We depict CARD’s inference-time pipeline: exploration (sample thoughts), conflict estimation (marginal-utility proxy), prioritized replay (allocate more samples to conflicted levels), and evaluation (majority vote), alternating rhythmically.
\begin{figure}[ht]
\centering
\begin{tikzpicture}[node distance=1.8cm, >=stealth, thick]
\tikzstyle{blk}=[draw, rounded corners, align=center, minimum width=3.4cm, minimum height=1.0cm]
\node[blk] (exp) {Exploration\\(sample candidate thoughts)};
\node[blk, right=of exp] (conf) {Conflict estimation\\(marginal utility $g_\ell$)};
\node[blk, right=of conf] (repl) {Prioritized replay\\(allocate extra samples)};
\node[blk, right=of repl] (eval) {Evaluation\\(majority voting)};
\draw[->] (exp) -- (conf);
\draw[->] (conf) -- (repl);
\draw[->] (repl) -- (eval);
\draw[->, bend left=30] (eval.north) to node[above] {stop if budget $B$ met} (repl.north);
\draw[->, bend left=30] (repl.south) to node[below] {else explore next batch} (exp.south);
\end{tikzpicture}
\caption{CARD pipeline: alternating exploration and evaluation, with conflict-aware prioritized replay under a global budget.}
\label{fig:arch}
\end{figure}

\subsection{Computational complexity and properties}
Let $L$ be the number of difficulty levels and $B$ the average budget.
- Complexity: The greedy water-filling loop adds at most $\tilde{O}(B/\min_\ell w_\ell)$ unit steps; each step scans $L$ levels to compute $g_\ell$, so time $O(L \cdot B/\min_\ell w_\ell)$; memory $O(L)$.
- Monotonicity: For $p>1/2$, $\mathrm{Acc}(p,k)$ is non-decreasing in $k$; the greedy algorithm therefore never reduces accuracy when increasing the budget.
- Even-odd plateaus: With random tie-breaking, $\mathrm{Acc}(p,2m)=\mathrm{Acc}(p,2m{-}1)$, implying equal-allocation plateaus at even $k$, whereas CARD achieves gains by redistributing compute (e.g., mixing $k{=}3$ and $k{=}1$ across instances to meet $B{=}2$).

\section{Experiments}
\subsection{Setup}
We instantiate five difficulty levels with $(p_\ell,w_\ell)\in \{(0.75,0.10),(0.65,0.20),(0.60,0.30),(0.55,0.25),(0.52,0.15)\}$.
Budgets are $B \in \{1,2,4,8,16\}$ samples per instance on average.
Equal self-consistency (EqualSC) uses the same number of samples for all instances; CARD follows Algorithm~1.
We report mixture accuracy and approximate standard errors for a large synthetic sample size.
Implementation and reproducibility: our simulator is deterministic (fixed seed), writes results to CSV and a human-readable results file, and regenerates all figure data used in this paper. Confidence calibration is computed via binning of predicted confidence surrogates and empirical accuracies across mixture levels and budgets.

\subsection{Results within the experimental setup}
Figure~\ref{fig:acc} shows accuracy increasing with test-time compute for both methods, with consistent gains for CARD at the same average budget.
Table~\ref{tab:summary} reports representative points for EqualSC.
A reliability analysis (Figure~\ref{fig:rel}) indicates improved calibration as compute increases, consistent with the intuition that more votes reduce variance and ambiguity \citep{Wang2023SelfConsistency,Lightman2023LetsVerify}.

\begin{figure}[ht]
\centering
\begin{tikzpicture}
  \begin{axis}[
    width=\linewidth,
    height=6cm,
    xlabel={Average test-time budget (samples per instance)},
    ylabel={Mixture accuracy},
    xmin=1, xmax=16,
    ymin=0.58, ymax=0.80,
    xtick={1,2,4,8,16},
    ytick={0.58,0.60,0.62,0.64,0.66,0.68,0.70,0.72,0.74,0.76,0.78,0.80},
    legend style={at={(0.03,0.97)},anchor=north west},
    grid=both, grid style={dashed,gray!30}
  ]
    \addplot+[mark=o, blue] table [x=budget, y=EqualSC, col sep=comma] {results_plot_data.csv};
    \addlegendentry{Equal self-consistency}
    \addplot+[mark=square, red] table [x=budget, y=CARD, col sep=comma] {results_plot_data.csv};
    \addlegendentry{Neuro-inspired adaptive (CARD)}
  \end{axis}
\end{tikzpicture}
\caption{Accuracy vs.\ test-time budget on a mixture of difficulties. Curves are computed from closed-form majority-vote probabilities and the CARD allocation rule (averaged over the mixture).}
\label{fig:acc}
\end{figure}

\begin{table}[ht]
\centering
\caption{Mixture accuracy for EqualSC at representative budgets. Values are computed analytically and reported with six decimal places.}
\label{tab:summary}
\begin{adjustbox}{width=\linewidth}
\begin{tabular}{lcccc}
\toprule
Budget & 1 & 4 & 8 & 16 \\
\midrule
EqualSC accuracy & 0.600500 & 0.645610 & 0.699662 & 0.759460 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\begin{figure}[ht]
\centering
\begin{tikzpicture}
  \begin{axis}[
    width=\linewidth,
    height=6cm,
    xlabel={Predicted confidence},
    ylabel={Empirical accuracy},
    xmin=0.5, xmax=1.0,
    ymin=0.5, ymax=1.0,
    legend style={at={(0.03,0.97)},anchor=north west},
    grid=both, grid style={dashed,gray!30}
  ]
    \addplot+[black, dashed] coordinates {(0.5,0.5) (1.0,1.0)};
    \addlegendentry{y=x (perfect)}
    \addplot+[mark=o, blue] table [x=confidence, y=accuracy, col sep=comma] {reliability_plot_data.csv};
    \addlegendentry{Equal self-consistency (binned)}
  \end{axis}
\end{tikzpicture}
\caption{Reliability diagram computed from the mixture by binning confidence estimates and averaging empirical accuracies within bins (10 bins).}
\label{fig:rel}
\end{figure}

\subsection{Neuroscience-grounded interpretation}
- Conflict monitoring: CARD detects ambiguity via marginal utility of extra samples, analogous to anterior cingulate monitoring of conflict and the recruitment of control \citep{Botvinick2001ConflictMonitoring}.
- Prioritized replay: allocating additional thought samples where they most improve accuracy mirrors prioritized memory access \citep{Mattar2018PrioritizedReplay}.
- Rhythmic control: alternating exploration and evaluation resembles rhythmic coordination facilitating effective communication between neural populations \citep{Fries2015Rhythms}.

\subsection{Comparative analysis to prior inference-time strategies}
We position CARD among inference-time strategies:
- Equal self-consistency: fixed $k$ samples per instance; simple, robust, but allocates compute uniformly regardless of uncertainty.
- Tree/search-based methods (e.g., ToT): allocate compute adaptively via branching; powerful but higher orchestration overhead.
- Reflective methods (Reflexion/Self-Refine): iterative improvement with feedback; complement CARD’s conflict-based allocation.
- Verification (stepwise checking): orthogonal; can be integrated with CARD to direct compute toward uncertain or unverifiable steps.
CARD is complementary, providing a lightweight, analytically grounded scheduler that can sit atop these techniques.

\section{Results}
We summarize the main quantitative outcomes across budgets and methods and report approximate standard errors (computed as $\sqrt{p(1-p)/N}$ with $N{=}50{,}000$ synthetic examples) to contextualize the observed gains.

\begin{table}[h]
\centering
\caption{Mixture accuracy (and approximate standard error) for EqualSC and CARD across budgets.}
\label{tab:both}
\begin{tabular}{lccccc}
\toprule
Method & B=1 & B=2 & B=4 & B=8 & B=16 \\
\midrule
EqualSC & 0.6005 (0.00219) & 0.6005 (0.00219) & 0.6456 (0.00214) & 0.6997 (0.00205) & 0.7595 (0.00191) \\
CARD    & 0.6005 (0.00219) & 0.6070 (0.00219) & 0.6575 (0.00213) & 0.7165 (0.00203) & 0.7765 (0.00186) \\
\bottomrule
\end{tabular}
\end{table}

Observations:
- Even–odd plateaus: EqualSC exhibits expected plateaus at even $k$ due to random tie-breaking (e.g., $B{=}1$ and $B{=}2$ are equal).
- Adaptive advantage: CARD delivers consistent gains at all reported budgets by reallocating samples to maximize marginal utility and by using fractional mixing to avoid zero-gain even steps.

\section{Discussion}
Our results indicate that neuro-inspired scheduling improves mixture accuracy at fixed budgets by allocating computation where it is most needed.
This complements LLM prompting and search methods \citep{Wei2022CoT,Wang2023SelfConsistency,Yao2023ToT,Shinn2023Reflexion}, providing a unifying lens grounded in cognitive mechanisms.
Limitations include reliance on a stylized simulator rather than end-to-end language tasks; future work will integrate CARD with verification \citep{Lightman2023LetsVerify} and action \citep{Yao2023ReAct}.
We also note that the benefit of adaptive allocation depends on how conflict is estimated; richer uncertainty proxies (e.g., diversity of rationales or verifier signals) may yield larger gains.

\section{Future Work and Limitations}
- Integrate CARD with verifiers and programmatic tool-use to target compute to unverifiable segments.
- Learn better uncertainty proxies (beyond base $p$) from signals such as self-consistency diversity or rationale-level disagreement.
- Extend from majority voting to cost-sensitive decision rules balancing accuracy and latency.
- Validate on real LLM benchmarks with controlled budget constraints and ablate replay vs.\ conflict vs.\ rhythmic scheduling.
- Explore human-in-the-loop calibration: mapping subjective confidence reports to allocation policies.

\section{Conclusion}
We presented CARD, a neuro-inspired inference-time scheduler that leverages conflict monitoring, prioritized replay, and rhythmic alternation to improve reasoning under a compute budget.
Through a closed-form simulator and an allocation strategy that avoids even-step plateaus via fractional mixing, we demonstrated consistent gains over equal self-consistency at the same average budget and improved calibration.
The framework is lightweight, analytically grounded, and complementary to contemporary reasoning-and-verification approaches.
We release a fully reproducible setup and anticipate straightforward integration with verifiers, tools, and tree-search controllers.

\section*{Acknowledgments}
We thank the research community for foundational advances in both cognitive neuroscience and reasoning with language models.

\section*{Appendix: Additional Figures (self-contained)}
\begin{figure}[H]
\centering
\begin{tikzpicture}
  \begin{axis}[
    width=\linewidth,
    height=6cm,
    xlabel={Average test-time budget (samples per instance)},
    ylabel={Mixture accuracy},
    xmin=1, xmax=16,
    ymin=0.58, ymax=0.80,
    xtick={1,2,4,8,16},
    legend style={at={(0.03,0.97)},anchor=north west},
    grid=both, grid style={dashed,gray!30}
  ]
    \addplot+[mark=*, blue] table [x=budget, y=EqualSC, col sep=comma] {results_plot_data.csv};
    \addlegendentry{Equal self-consistency}
    \addplot+[mark=*, red] table [x=budget, y=CARD, col sep=comma] {results_plot_data.csv};
    \addlegendentry{CARD}
  \end{axis}
\end{tikzpicture}
\caption{Supplementary view of accuracy vs.\ budget.}
\end{figure}

\begin{figure}[H]
\centering
\begin{tikzpicture}
  \begin{axis}[
    width=\linewidth,
    height=6cm,
    xlabel={Predicted confidence},
    ylabel={Empirical accuracy},
    xmin=0.5, xmax=1.0,
    ymin=0.5, ymax=1.0,
    legend style={at={(0.03,0.97)},anchor=north west},
    grid=both, grid style={dashed,gray!30}
  ]
    \addplot+[black, dashed] coordinates {(0.5,0.5) (1.0,1.0)};
    \addlegendentry{y=x}
    \addplot+[mark=*, blue] table [x=confidence, y=accuracy, col sep=comma] {reliability_plot_data.csv};
    \addlegendentry{EqualSC}
  \end{axis}
\end{tikzpicture}
\caption{Supplementary reliability diagram (10 bins).}
\end{figure}

\begin{thebibliography}{99}
\bibitem[Baddeley(2012)]{Baddeley2012WorkingMemory}
A.~Baddeley.
\newblock Working memory: theories, models, and controversies.
\newblock Annual Review of Psychology, 63:1--29, 2012.

\bibitem[Botvinick et~al.(2001)Botvinick, Braver, Barch, Carter, and
  Cohen]{Botvinick2001ConflictMonitoring}
M.~M. Botvinick, T.~S. Braver, D.~M. Barch, C.~S. Carter, and J.~D. Cohen.
\newblock Conflict monitoring and cognitive control.
\newblock Psychological Review, 108(3):624--652, 2001.

\bibitem[Fries(2015)]{Fries2015Rhythms}
P.~Fries.
\newblock Rhythms for Cognition: Communication through Coherence.
\newblock Neuron, 88(1):220--235, 2015.

\bibitem[Mattar and Daw(2018)]{Mattar2018PrioritizedReplay}
M.~G. Mattar and N.~D. Daw.
\newblock Prioritized memory access explains planning and hippocampal replay.
\newblock Nature Neuroscience, 21:1609--1617, 2018.

\bibitem[Gershman(2018)]{Gershman2018Hippocampus}
S.~J. Gershman.
\newblock What does the hippocampus do?
\newblock Trends in Cognitive Sciences, 22(7):512--524, 2018.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, and
  Zhou]{Wei2022CoT}
J.~Wei, X.~Wang, D.~Schuurmans, M.~Bosma, F.~Xia, E.~Chi, Q.~Le, and D.~Zhou.
\newblock Chain-of-Thought Prompting Elicits Reasoning in Large Language
  Models.
\newblock arXiv:2201.11903, 2022.

\bibitem[Kojima et~al.(2022)Kojima, Srikumar, Sch{\"u}tze, Li, Sugawara, Sinha,
  and Shimizu]{Kojima2022ZeroShot}
T.~Kojima, V.~Srikumar, H.~Sch{\"u}tze, Y.~Li, S.~Sugawara, K.~Sinha, and
  N.~Shimizu.
\newblock Large Language Models are Zero-Shot Reasoners.
\newblock arXiv:2205.11916, 2022.

\bibitem[Wang et~al.(2023)Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery,
  and Zhou]{Wang2023SelfConsistency}
X.~Wang, J.~Wei, D.~Schuurmans, Q.~Le, E.~Chi, S.~Narang, A.~Chowdhery, and
  D.~Zhou.
\newblock Self-Consistency Improves Chain of Thought Reasoning in Language
  Models.
\newblock In ICLR, 2023.

\bibitem[Yao et~al.(2023{\natexlab{a}})Yao, Yu, Zhao, Shafran, Griffiths,
  Liang, and Narasimhan]{Yao2023ToT}
S.~Yao, D.~Yu, J.~Zhao, I.~Shafran, T.~Griffiths, P.~Liang, and K.~Narasimhan.
\newblock Tree of Thoughts: Deliberate Problem Solving with Large Language
  Models.
\newblock arXiv:2305.10601, 2023{\natexlab{a}}.

\bibitem[Yao et~al.(2023{\natexlab{b}})Yao, Zhao, Yu, Cao, and
  Narasimhan]{Yao2023ReAct}
S.~Yao, J.~Zhao, D.~Yu, N.~Cao, and K.~Narasimhan.
\newblock ReAct: Synergizing Reasoning and Acting in Language Models.
\newblock arXiv:2210.03629, 2023{\natexlab{b}}.

\bibitem[Shinn et~al.(2023)Shinn, Labash, and Gopinath]{Shinn2023Reflexion}
N.~Shinn, B.~Labash, and A.~Gopinath.
\newblock Reflexion: Language Agents with Verbal Reinforcement Learning.
\newblock arXiv:2303.11366, 2023.

\bibitem[Madaan et~al.(2023)Madaan, Yazdanbakhsh, Abhishek, et~al.]{Madaan2023SelfRefine}
A.~Madaan, A.~Yazdanbakhsh, S.~Abhishek, et~al.
\newblock Self-Refine: Iterative Refinement with Self-Feedback.
\newblock arXiv:2303.17651, 2023.

\bibitem[Zelikman et~al.(2022)Zelikman, Wu, Li, and Goodman]{Zelikman2022STAR}
E.~Zelikman, Y.~Wu, J.~M. Li, and N.~D. Goodman.
\newblock STaR: Bootstrapping Reasoning With Reasoning.
\newblock Transactions on Machine Learning Research, 2022.

\bibitem[Lightman et~al.(2023)Lightman, Welleck, Ouyang, and
  Olsson]{Lightman2023LetsVerify}
H.~Lightman, S.~Welleck, L.~Ouyang, and C.~Olsson.
\newblock Let's Verify Step by Step.
\newblock arXiv:2305.20050, 2023.

\bibitem[OpenAI(2023)]{OpenAI2023GPT4}
OpenAI.
\newblock GPT-4 Technical Report.
\newblock arXiv:2303.08774, 2023.

\bibitem[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, et~al.]{Bubeck2023Sparks}
S.~Bubeck, V.~Chandrasekaran, R.~Eldan, et~al.
\newblock Sparks of Artificial General Intelligence: Early experiments with
  GPT-4.
\newblock arXiv:2303.12712, 2023.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, et~al.]{Kaplan2020ScalingLaws}
J.~Kaplan, S.~McCandlish, T.~Henighan, et~al.
\newblock Scaling Laws for Neural Language Models.
\newblock arXiv:2001.08361, 2020.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, et~al.]{Hoffmann2022Chinchilla}
J.~Hoffmann, S.~Borgeaud, A.~Mensch, et~al.
\newblock Training Compute-Optimal Large Language Models.
\newblock arXiv:2203.15556, 2022.

\bibitem[Lewis et~al.(2020)Lewis, Perez, Piktus, et~al.]{Lewis2020RAG}
P.~Lewis, E.~Perez, A.~Piktus, et~al.
\newblock Retrieval-Augmented Generation for Knowledge-Intensive NLP.
\newblock arXiv:2005.11401, 2020.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, et~al.]{Brown2020GPT3}
T.~B. Brown, B.~Mann, N.~Ryder, et~al.
\newblock Language Models are Few-Shot Learners.
\newblock In NeurIPS, 2020.
\end{thebibliography}

\end{document}