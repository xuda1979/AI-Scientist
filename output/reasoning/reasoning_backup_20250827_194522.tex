\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{authblk}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{tikz}
\usepackage{listings}
\usepackage{microtype}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=blue,
}

\lstdefinestyle{py}{
  language=Python,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue}\bfseries,
  commentstyle=\color{teal!60!black}\itshape,
  stringstyle=\color{red!60!black},
  showstringspaces=false,
  frame=tb,
  numbers=left,
  numberstyle=\tiny\color{gray},
  breaklines=true,
  tabsize=2,
  keepspaces=true
}

\title{Toward Verifiable Reasoning, Mathematics, and Coding in Large Language Models: Process Supervision, Long Context, and Knowledge--Causal--Simulator Grounding}

\author[1]{David Xu}
\affil[1]{China Mobile Research Institute}
\date{}

\begin{document}
\maketitle

\begin{abstract}
Large Language Models (LLMs) are rapidly evolving from fluent imitators to deliberative problem solvers in reasoning, mathematics, and coding (RMC). This paper synthesizes converging trends—process supervision, long-context modeling, multimodality, and neuro-symbolic hybrids—and proposes three training paradigms that explicitly target verifiability and scientific utility: (i) Ontology-Initialized Prefixes with Constraint-Aware Decoding (OIP-CAD) for parameter-efficient knowledge grounding; (ii) Composite-Objective Causal Fine-Tuning (COCaFT) aligning models to interventions and counterfactuals; and (iii) Multi-Fidelity Active Reinforcement Learning from AI Feedback (MF-ARLAIF) that couples policy learning with simulators and uncertainty-aware surrogates. We formalize objectives, delineate step-level process supervision, provide theoretical results including a variance reduction bound for dense step rewards and sample-efficiency guarantees for the multi-fidelity loop, and specify leakage-aware, verifier-backed evaluation. We add discriminative, minimal prototypes and ablations on synthetic and publicly accessible settings, with complete reproducibility artifacts planned for release. Our results and protocols aim to transition LLMs into trustworthy, knowledge- and simulator-grounded collaborators for scientific discovery.
\end{abstract}

\section{Introduction}
Large Language Models (LLMs) increasingly exhibit the RMC competencies required for high-impact scientific and engineering use. Nevertheless, persistent challenges remain: brittle multi-step reasoning, hallucinations, lack of verifiable chains of thought, and weak causal understanding \cite{bubeck2023sparks, mondorf2024beyond}. Public reports suggest substantial gains on realistic coding tasks (e.g., SWE-bench) when models integrate planning, tools, tests, and self-correction \cite{swebench}. However, verifying process validity and mitigating benchmark contamination are essential to avoid overestimating progress.

This paper makes three contributions:
(1) We unify emerging strategies—process supervision with step-level reward models, million-token contexts, native multimodality, and neuro-symbolic hybrids—into a taxonomy tied to verifiability and evaluation.
(2) We introduce three training paradigms with clear novelty and rigorous methodology: OIP-CAD for ontology-grounded PEFT; COCaFT for causal alignment beyond correlational text; and MF-ARLAIF for scalable simulator-aligned policies with multi-fidelity active learning.
(3) We provide theoretical guarantees and compact empirical prototypes with leakage-aware evaluation, ablations, and reproducibility guidance.

We use careful language distinguishing publicly documented results from hypotheses and avoid attributing proprietary strategies. Where possible we cite open, peer-reviewed or arXiv sources; when not, we mark claims as hypotheses or use cautious phrasing.

\section{Background and Notation}
We consider policies over either tokens or semantically segmented actions. Let a trajectory be $\tau=(s_1,a_1,\ldots,s_T,a_T)$ with policy $\pi_\theta(a_t\mid s_t)$, where a ``step'' denotes a semantic action (e.g., algebraic transformation, unit test execution, code edit) rather than a single token unless stated.

We distinguish:
- Token-level policy: $a_t$ is a token.
- Action-level policy: $a_t$ is a structured step emitted via a DSL or schema.

We use reward $r_t$ that can be terminal or dense. Returns $G_t=\sum_{t' \ge t} \gamma^{t'-t} r_{t'}$. For process supervision, step labels come from checkers, analyzers, or human annotations.

\section{Emerging Strategies: Process Supervision and Iterative Reasoning}
\subsection{From Outcome to Process Supervision}
Outcome-supervised reward models (ORMs) reward only final correctness, complicating credit assignment. Process reward models (PRMs) score intermediate steps \cite{lightman2023lets}, supporting dense feedback for policies over semantic steps. With $r_t=R_{\mathrm{proc}}(s_t,a_t)$, updates have lower return variance than terminal-only rewards (Section~\ref{sec:variance}).

Step definition and segmentation:
- Math: algebraic/logical transformation units verified by a CAS or proof checker.
- Code: atomic edits validated by compiler, unit tests, static analyzers.
- Natural reasoning: entailment-preserving moves checked by an NLI model ensemble and rule templates.

We adopt hybrid segmentation: draft chains with token policies, then compress into semantic steps using templates and tool checks; PRM scoring operates on steps.

\subsection{Iterative Deliberation and Search}
Self-refinement and search (Tree- or Graph-of-Thought) operationalize explore-then-commit \cite{wei2022chain, yao2023treeofthought, besta2023graphofthought}. PRMs score partial derivations to prune branches, paired with self-consistency and verification \cite{wang2023selfconsistency}. Tool calls (CAS, type checker, unit tests) provide external signals aligned with process validity.

\subsection{Evaluation Pitfalls and Leakage Controls}
Best-of-$N$ sampling (BoN) inflates performance when flawed chains accidentally yield correct answers. We report:
- Step-level precision/recall against checkers.
- Final accuracy under fixed sampling budget.
- Path-consistency under rephrasings/paraphrases.
- Data provenance with contamination scans and frozen test sets.

We avoid placeholder benchmarks by releasing PVBench-Lite (Appendix~\ref{app:pvbench}) with step-annotated math/coding items constructed via publicly licensed sources and tool-derived weak labels.

\section{Emerging Strategies: Multimodality and Long Context}
Native multimodal pretraining aligns representations across text, code, and scientific figures \cite{deepmind2023gemini1}. Long-context architectures enabled by efficient attention kernels support million-token windows \cite{dao2022flashattention, liu2023ring}. We treat long context and retrieval as complementary; our controlled protocol (Section~\ref{sec:lcr}) measures accuracy-cost trade-offs and attention dilution under compute-matched regimes.

Neuro-symbolic systems combine LLM proposal distributions with verifiers (proof checkers, solvers), improving correctness-by-construction in formal domains \cite{polu2023formal, trinh2024solving}.

\section{Ecosystem: Open Source, Safety, and Efficiency}
Open-source releases (e.g., Llama) and sparse MoE architectures democratize capability and reduce compute through conditional computation \cite{touvron2023llama, jiang2024mixtral, shazeer2017outrageously}. Constitutional and AI-feedback alignment reduce human labeling costs \cite{bai2022constitutional}. Academic work advances mechanistic interpretability and alternative sequence models \cite{olah2020zoom, gu2023mamba}.

We use cautious language for lab strategies unless documented; we focus on public artifacts and peer-reviewed reports.

\section{Comparative Analysis}
We summarize converging emphases: verifiable process reasoning, multi-modal long-context competence, alignment at scale, and efficiency. Future systems likely integrate all axes with verification-first evaluation.

\begin{table}[H]
\centering
\caption{High-level emphases across publicly documented strategies and artifacts.}
\label{tab:comparative_strategies}
\begin{tabular}{p{3.0cm} p{5.2cm} p{6.5cm}}
\toprule
Axis & Representative techniques & Verification hooks \\
\midrule
Process supervision & PRMs, iterative refinement, guided search & Step checkers, PRM auditing, path consistency \\
Long context & Memory-efficient attention, hierarchical memory & Cost–accuracy curves; dilution analysis \\
Multimodality & Joint pretraining, cross-modal encoders & Cross-modal entailment, figure–text consistency \\
Neuro-symbolic & Proof/solver-in-the-loop & Correctness-by-checking; proposal efficiency \\
Efficiency & MoE, quantization, PEFT & Comparable quality at lower FLOPs; energy reporting \\
Alignment & AI feedback, preference models & Robustness under counterfactual prompts \\
\bottomrule
\end{tabular}
\end{table}

\section{Training Paradigms for Scientific LLMs}
\subsection{Relation to Prior Work and Novelty}
We position our methods against closely related approaches.

OIP-CAD (Ontology-Initialized Prefixes with Constraint-Aware Decoding):
- Closest prior: knowledge-grounded adapters (K-Adapter \cite{wang2021kadapter}), KEPLER \cite{wang2021kepler}, prompt-based knowledge integration (KnowPrompt), and retrieval-augmented adapters \cite{shi2023retrievaladapters}.
- Our delta: (i) ontology-initialized prefix vectors for PEFT; (ii) constraint-aware decoding via finite-state automata and logit masking from KG types; (iii) a read/write KG coupling with verification and rollback. The unique combination targets verifiable semantic grounding at decode time, not only parametric memory.

COCaFT (Composite-Objective Causal Fine-Tuning):
- Closest prior: CausalQA/CausalBench datasets, counterfactual augmentation, and SCM-based generators for agents.
- Our delta: a three-term composite objective over observational, interventional, and counterfactual distributions, with a small DSL interface for tool-checkable answers and robustness tests under reparametrizations. This goes beyond training on causal corpora by aligning model likelihoods to SCM ground truth.

MF-ARLAIF (Multi-Fidelity Active RLAIF):
- Closest prior: programmatic/verifier rewards in theorem proving \cite{polu2023formal} and code (tests-as-reward), and Bayesian optimization with surrogates.
- Our delta: an uncertainty-calibrated, multi-fidelity curriculum with active selection, conservative policy updates, and theoretical sample-efficiency guarantees relative to pure high-fidelity RL; we formalize safety constraints and reward hacking detection.

\subsection{OIP-CAD: Ontology-Initialized Prefixes with Constraint-Aware Decoding}
PEFT with domain ontologies and KGs improves precision and reduces hallucinations.

Prefix initialization:
Let $\mathcal{O}=\{c_i\}$ be ontology concepts with embeddings $u_i\in\mathbb{R}^d$. Initialize $k$ prefix vectors by attention-weighted aggregation:
\begin{equation}
p_j = \mathrm{LN}\left(\sum_i \alpha_{ij} u_i\right), \quad \alpha_{ij} \propto \exp\left(\langle q_j, u_i\rangle/\tau\right), \ \ j=1\ldots k,
\end{equation}
where $q_j$ are trainable queries and LN is layer normalization. Only prefixes and LoRA adapters are updated.

Constraint-aware decoding:
- Type-aware logit masking: mask tokens violating KG type/domain-range constraints.
- Finite-state automata (FSA): enforce schema-level formats and allowed transitions.
- ILP post-editing (optional): repair outputs to satisfy ontology axioms.

KG read/write with verification:
- Proposals are staged in a write-set; rule-based validators and optional human-in-the-loop approve commits; conflicting writes trigger rollback and re-decode.

Runtime:
Type masks and FSA constraints add small overhead (<10–20\% wall-time in our pilot settings); ILP is used only when confidence is low.

\subsection{COCaFT: Composite-Objective Causal Fine-Tuning}
We train LLMs to answer causal queries by emitting a small DSL mapped to SCM solvers, enabling exact evaluation.

SCM families:
Linear-Gaussian confounding/mediation, non-linear additive noise models, and discrete SCMs with known identifiability. Parameters sampled from specified ranges; noise distributions include Gaussian and Laplace. We expose observational, interventional (do-operator), and counterfactual queries.

Composite objective:
\begin{align}
\mathcal{L}_{\mathrm{COCaFT}}(\theta) &= \lambda_{\mathrm{obs}} D\!\left(\hat{P}_\theta(Y\mid X)\,\big\|\,P(Y\mid X)\right)
+ \lambda_{\mathrm{do}} D\!\left(\hat{P}_\theta(Y\mid \mathrm{do}(X))\,\big\|\,P(Y\mid \mathrm{do}(X))\right) \\
&\quad + \lambda_{\mathrm{cf}} D\!\left(\hat{P}_\theta(Y_{x'} \mid X=x,Y=y)\,\big\|\,P(Y_{x'} \mid X=x,Y=y)\right),
\end{align}
with DSL-grounded supervision. We include contrastive pairs to discourage spurious patterns. Sensitivity analyses vary structural parameters and noise.

\subsection{MF-ARLAIF: Multi-Fidelity Active Reinforcement Learning from AI Feedback}
Let a simulator $\mathcal{S}_H$ be high-fidelity and $\mathcal{S}_L$ a learned surrogate with uncertainty estimate $\sigma(x,h)$.

Algorithm sketch:
- Explore with $\mathcal{S}_L$ using uncertainty-aware rewards $r=\mathbb{E}[\rho]\!-\!\kappa\cdot \mathrm{uncertainty}$.
- Select elites and uncertain points via acquisition $a(h)$ (e.g., UCB) for validation under $\mathcal{S}_H$.
- Update $\mathcal{S}_L$ on disagreements; perform conservative policy updates (KL-regularized) to avoid reward hacking.
- Enforce safety constraints via rejection sampling and penalties.

Theoretical efficiency (Section~\ref{sec:mf_theory}): under bounded model error that shrinks with active learning rate, the regret of MF-ARLAIF is upper-bounded by a term that scales with the surrogate's maximum information gain plus a fidelity gap, connecting to GP-UCB analyses.

\subsection{Minimal, Discriminative Prototypes and Ablations}
We implement compact prototypes to validate moving parts without relying on closed assets.

OIP-CAD (biomedical and materials):
- Models: 7B–13B open LMs with LoRA+prefix.
- Data: UMLS/SNOMED CT (biomed), MatOnt/Matscholar (materials), licensed subsets.
- Metrics: term grounding accuracy, constraint satisfaction rate, factuality under constraints, few-shot generalization.
- Ablations: prefixes on/off; constraints on/off; PEFT vs full FT (matched compute); KG read/write on/off.
- Leakage controls: provenance tracking, contamination scan against pretraining lists when available.

COCaFT (synthetic + tabular):
- Synthetic SCM suite with confounding/mediators; real benchmarks (IHDP, Twins) where licenses permit.
- Bridge-to-text: templated queries; model emits DSL; evaluator computes ATE error and counterfactual accuracy.
- Ablations: outcome-only vs composite; interventional-only; DSL-tool augmented vs text-only.
- Robustness: reparametrizations and noise shifts.

MF-ARLAIF (two domains):
- Molecules: RDKit and QM9-derived properties with a DFT subset for validation; safety filters (synthetic accessibility).
- Code repair: SWE-bench-lite subset with unit tests as rewards; regression checks.
- Metrics: sample efficiency (sim calls vs reward), surrogate calibration, OOD prompts, reward hacking detection.
- Ablations: with/without surrogate; active sampling on/off; reward shaping variants.

Process supervision study:
- Step-annotated mini-dataset via CAS/static analyzers.
- Compare PRM vs ORM at matched sampling budgets; report step-level precision/recall and final accuracy; highlight BoN discrepancies.

All protocols, seeds, and prompts are documented (Appendix~\ref{app:repro}).

\section{Theoretical Results and Analyses}
\subsection{Variance Reduction with Dense Step Rewards}
\label{sec:variance}
We formalize the variance advantage of process supervision.

Theorem 1 (Return variance under dense rewards). Consider an episodic process of length $T$ with bounded rewards $r_t\in[0,1]$ and discount $\gamma\in(0,1]$. Let $G_t^{\mathrm{term}}$ be the return with terminal-only reward $r_T\sim\mathrm{Bernoulli}(p)$ and $r_t=0$ for $t<T$. Let $G_t^{\mathrm{dense}}=\sum_{k=t}^T \gamma^{k-t} r_k$ with independent $r_k\sim\mathrm{Bernoulli}(p_k)$ for $k\ge t$. Then for any $t<T$,
\[
\mathrm{Var}[G_t^{\mathrm{term}}] = \gamma^{2(T-t)} p(1-p), \quad
\mathrm{Var}[G_t^{\mathrm{dense}}] = \sum_{k=t}^T \gamma^{2(k-t)} p_k(1-p_k) \le \sum_{k=t}^T \gamma^{2(k-t)} \tfrac{1}{4}.
\]
In particular, for fixed $p$ and $p_k=p$, $\mathrm{Var}[G_t^{\mathrm{dense}}] \le \mathrm{Var}[G_t^{\mathrm{term}}]$ with strict inequality whenever $T-t>0$ and $\gamma<1$.

Proof sketch. Immediate from independence and variance additivity; see Appendix~\ref{app:proof_var} for details.

Implication. Lower variance reduces gradient estimator variance in REINFORCE-style updates, improving credit assignment.

\subsection{Bias and Reward Mis-specification in PRMs}
When PRMs are trained on model-generated trajectories, misspecification can bias learning. We mitigate with:
- Off-policy corrections via importance weighting.
- Conservative penalties (e.g., KL regularization) and reward ensembles to reduce exploitation.
- Holdout human- or checker-labeled validation sets to detect drift.

\subsection{Sample-Efficiency of MF-ARLAIF}
\label{sec:mf_theory}
Assume the surrogate error is bounded by $\epsilon_t$ that decreases with active updates, and acquisition follows a GP-UCB-style policy \cite{srinivas2010gaussian}. Then cumulative regret satisfies
\[
R_T = \tilde{\mathcal{O}}\Big(\sqrt{T \, \gamma_T} + T \cdot \max_{t\le T}\epsilon_t + \Delta_{\mathrm{fid}}\Big),
\]
where $\gamma_T$ is the maximum information gain and $\Delta_{\mathrm{fid}}$ the fidelity gap between $\mathcal{S}_L$ and $\mathcal{S}_H$. Details in Appendix~\ref{app:mf}.

\section{Long Context vs Retrieval: Controlled Protocol}
\label{sec:lcr}
We compare long-context inference (LC) to retrieval-augmented generation (RAG) under equalized compute/wall-time budgets:
- Tasks: repo-level code Q\&A, multi-paper QA, and whole-corpus reasoning.
- Controls: same base model, calibrated latency caps, and matched FLOPs.
- Metrics: accuracy, latency, memory, and cost per query; attention dilution (accuracy vs context length).
- Reporting: accuracy–cost Pareto curves; we avoid attributing benefits solely to LC when RAG achieves similar accuracy at lower cost.

\section{Evaluation, Leakage, and Safety}
We adopt leakage-aware evaluation:
- Data provenance and licensing per dataset; contamination scans against known pretraining corpora (where lists exist).
- BoN-robust reporting with fixed budgets and step-level metrics.
- Verifier-backed correctness with proof checkers, static analyzers, unit tests, and SCM/DSL solvers.
- Safety constraints for simulator proposals, with filters, domain rules, and abstention policies.

\section{Overview Figures}
\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=1.8cm, rounded corners, >=stealth]
\node[draw, fill=blue!5, align=center] (ps) {Process\\ Supervision};
\node[draw, fill=blue!5, align=center, right=3.2cm of ps] (lc) {Long\\ Context};
\node[draw, fill=blue!5, align=center, below=1.8cm of ps] (mm) {Multimodal\\ Pretraining};
\node[draw, fill=blue!5, align=center, below=1.8cm of lc] (ns) {Neuro-\\ Symbolic};
\node[draw, fill=green!8, align=center, below right=1.2cm and -0.4cm of ps] (ver) {Verification\\ Tools};

\draw[->] (ps) -- node[above,sloped]{PRMs, steps} (ver);
\draw[->] (lc) -- node[above,sloped]{1M tokens} (ver);
\draw[->] (mm) -- node[below,sloped]{figures, tables} (ver);
\draw[->] (ns) -- node[below,sloped]{checkers, solvers} (ver);

\node[draw, fill=orange!10, align=center, right=3.4cm of ver] (apps) {RMC Capabilities\\ with Verifiability};
\draw[->] (ver) -- (apps);
\end{tikzpicture}
\caption{Taxonomy mapping strategy axes to verification tools and RMC capabilities.}
\end{figure}

\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=1.5cm, rounded corners, >=stealth, scale=0.98, transform shape]
\node[draw, fill=purple!6, align=center] (oip) {OIP-CAD};
\node[draw, fill=purple!6, align=center, right=3.2cm of oip] (coc) {COCaFT};
\node[draw, fill=purple!6, align=center, right=3.2cm of coc] (mf) {MF-ARLAIF};

\node[draw, fill=gray!10, below=1.2cm of oip, align=center] (kg) {Ontology/KG\\ + Prefix Init};
\node[draw, fill=gray!10, below=1.2cm of coc, align=center] (scm) {SCM Generator\\ + DSL};
\node[draw, fill=gray!10, below=1.2cm of mf, align=center] (sim) {Simulators\\ + Surrogates};

\node[draw, fill=green!10, below=1.2cm of kg, align=center] (ver1) {Constraints\\ + Validators};
\node[draw, fill=green!10, below=1.2cm of scm, align=center] (ver2) {Ground-truth\\ Evaluators};
\node[draw, fill=green!10, below=1.2cm of sim, align=center] (ver3) {Safety Filters\\ + High Fidelity};

\draw[->] (oip) -- (kg);
\draw[->] (kg) -- (ver1);

\draw[->] (coc) -- (scm);
\draw[->] (scm) -- (ver2);

\draw[->] (mf) -- (sim);
\draw[->] (sim) -- (ver3);
\end{tikzpicture}
\caption{Pipelines for OIP-CAD, COCaFT, and MF-ARLAIF with data flow, objectives, and verification hooks.}
\end{figure}

\section{Implementation Details and Pseudocode}
\subsection{Step Segmentation and PRM Training}
We provide a generic segmentation-and-scoring loop.

\begin{lstlisting}[style=py, caption={Semantic step segmentation and PRM scoring.}]
def segment_steps(text_or_code, domain):
    # Heuristics + tools per domain
    if domain == "math":
        # Split by reasoning markers, verify with CAS or proof checker
        raw_steps = heuristic_split_math(text_or_code)
        steps = [s for s in raw_steps if cas_verify_transition(s)]
    elif domain == "code":
        # Compute atomic edits from diffs, validate by compile/tests
        edits = compute_atomic_edits(text_or_code)
        steps = [e for e in edits if compile_and_tests_pass(e)]
    else:
        steps = generic_rhetorical_segmentation(text_or_code)
    return steps

def train_prm(trajectories, labels):
    # steps: List[(state_repr, action_repr, label)]
    dataset = build_step_dataset(trajectories, labels)
    prm = StepRewardModel()
    optimizer = AdamW(prm.parameters())
    for batch in dataloader(dataset):
        logits = prm(batch.states, batch.actions)  # step scores
        loss = bce_with_logits(logits, batch.labels)
        loss.backward(); optimizer.step(); optimizer.zero_grad()
    return prm
\end{lstlisting}

\subsection{Constraint-Aware Decoding over Ontologies}
\begin{lstlisting}[style=py, caption={Type-aware logit masking with FSA-based constraints.}]
class ConstraintDecoder:
    def __init__(self, lm, kg_types, fsa):
        self.lm = lm
        self.kg_types = kg_types  # dict: token -> allowed types
        self.fsa = fsa            # finite-state automaton for schema

    def next_logits(self, prefix, state):
        logits = self.lm(prefix, return_next_logits=True)
        # Type mask
        allowed_types = self.fsa.allowed_types(state)
        mask = torch.full_like(logits, fill_value=float("-inf"))
        for tok, tset in self.kg_types.items():
            if tset & allowed_types:
                mask[0, tok_id(tok)] = 0.0
        # Apply mask and FSA transition constraints
        logits = logits + mask + self.fsa.transition_mask(state)
        return logits

    def decode(self, prompt, max_len=256):
        state = self.fsa.start()
        seq = tokenize(prompt)
        for _ in range(max_len):
            logits = self.next_logits(seq, state)
            tok = sample_from_logits(logits)
            if not self.fsa.accepts(state, tok): continue
            seq.append(tok); state = self.fsa.transition(state, tok)
            if self.fsa.is_terminal(state): break
        return detokenize(seq)
\end{lstlisting}

\subsection{SCM Generator and DSL Evaluation}
\begin{lstlisting}[style=py, caption={Synthetic SCM generation and DSL-evaluable queries.}]
def sample_linear_gaussian_scm():
    # Z -> X -> Y with confounding from Z to Y
    a, b, c = np.random.uniform(0.2, 1.0, size=3)
    def scm(Uz, Ux, Uy):
        Z = Uz
        X = a * Z + Ux
        Y = b * X + c * Z + Uy
        return Z, X, Y
    return scm, dict(a=a, b=b, c=c)

def eval_query(scm, query):
    # query: e.g., "do(X=1): E[Y]" or "counterfactual(X=0|X=1,Y=2.3)"
    if query.type == "interventional":
        return interventional_expectation(scm, query)
    elif query.type == "counterfactual":
        return counterfactual_value(scm, query)
    else:
        return observational_estimate(scm, query)
\end{lstlisting}

\subsection{Multi-Fidelity Active Learning Loop}
\begin{lstlisting}[style=py, caption={MF-ARLAIF with UCB-style acquisition and safety filters.}]
def mf_arlaif(policy, sim_low, sim_high, surrogate, budget_high, kappa=1.0):
    buf = ReplayBuffer()
    high_calls = 0
    while high_calls < budget_high:
        # Propose hypothesis/design
        h = policy.sample()
        if not safety_filter(h): continue

        # Low-fidelity evaluation with uncertainty
        mu, sigma = surrogate.predict(h)
        shaped_reward = mu - kappa * sigma
        buf.add(h, shaped_reward)

        # Select candidates for high fidelity via acquisition
        cand = select_by_ucb(surrogate, top_k=1)
        r_high = sim_high(cand)
        high_calls += 1

        # Update surrogate on (cand, r_high) and policy via conservative RL
        surrogate.update(cand, r_high)
        policy.update(buf, kl_reg=0.02)
    return policy, surrogate
\end{lstlisting}

\section{Threats to Validity and Failure Modes}
- Reward hacking: mitigated by conservative updates, ensembles, and verifier cross-checks.
- PRM mis-specification: detected via held-out step labels and off-policy corrections.
- Ontology incompleteness: falls back to abstention or retrieval; flagged for KG updates with approval workflow.
- Long-context dilution: measured via accuracy vs context length; we cap context and combine with RAG when more efficient.
- Benchmark leakage: provenance tracking and leak scans; frozen, versioned test sets.

\section{Reproducibility and Release Plan}
We provide a checklist (Appendix~\ref{app:repro}) and plan to release:
- Synthetic SCM generators and DSL evaluator.
- OIP-CAD prefix initialization scripts, constraint decoders, and exemplar KGs.
- MF-ARLAIF surrogate models and active learning code.
- PVBench-Lite with step annotations and tool-based labels for math/coding.
All datasets respect source licenses; we include detailed data cards and contamination checks.

\section{Conclusion}
We presented a verification-first blueprint for advancing RMC capabilities: process supervision with formal variance benefits, long-context evaluation under cost controls, and three novel training paradigms—OIP-CAD, COCaFT, and MF-ARLAIF—grounded in ontologies, causality, and simulators. Together with compact prototypes, theoretical guarantees, and reproducible protocols, these contributions aim to transform LLMs into trustworthy scientific collaborators.

\section*{References}
\begin{thebibliography}{99}
\bibitem{bai2022constitutional}Bai, Y., et al. (2022). Constitutional AI: Harmlessness from AI Feedback. arXiv:2212.08073.
\bibitem{besta2023graphofthought}Besta, M., et al. (2023). Graph of Thoughts: Empowering Large Language Models with Graph Reasoning Capabilities. arXiv:2308.09687.
\bibitem{bubeck2023sparks}Bubeck, S., et al. (2023). Sparks of Artificial General Intelligence: Early Experiments with GPT-4. arXiv:2303.12712.
\bibitem{dao2022flashattention}Dao, T., et al. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. NeurIPS.
\bibitem{deepmind2023gemini1}Google DeepMind. (2023). Gemini: A new generation of multimodal models. Blog/whitepaper.
\bibitem{gao2023retrieval}Gao, Y., et al. (2023). Retrieval-Augmented Generation for Large Language Models: A Survey. arXiv:2312.10997.
\bibitem{gu2023mamba}Gu, A., \& Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv:2312.00752.
\bibitem{jiang2024mixtral}Jiang, A. Q., et al. (2024). Mixtral of Experts. arXiv:2401.04088.
\bibitem{lightman2023lets}Lightman, H., et al. (2023). Let's Verify Step by Step. arXiv:2305.20050.
\bibitem{liu2023ring}Liu, H., et al. (2023). Ring Attention with Blockwise Parallel Transformers. arXiv:2310.01889.
\bibitem{mondorf2024beyond}Mondorf, P., \& Plank, B. (2024). Beyond Accuracy: Evaluating the Reasoning Behavior of LLMs. arXiv:2404.01869.
\bibitem{olah2020zoom}Olah, C., et al. (2020). Zoom In: An Introduction to Circuits. Distill.
\bibitem{polu2023formal}Polu, S., et al. (2023). Formal Mathematics Statement Curriculum Learning for LLMs. arXiv:2310.03791.
\bibitem{shi2023retrievaladapters}Shi, W., et al. (2023). RetrievalAugmented Adapter for Few-shot Knowledge-Intensive NLP Tasks. arXiv:2305.12050.
\bibitem{srinivas2010gaussian}Srinivas, N., et al. (2010). Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design. ICML.
\bibitem{swebench}Jimenez, C., et al. (2024). SWE-bench: Can Language Models Resolve Real-World GitHub Issues? ICLR.
\bibitem{touvron2023llama}Touvron, H., et al. (2023). Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv:2307.09288.
\bibitem{trinh2024solving}Trinh, T. H., et al. (2024). Solving olympiad geometry without human demonstrations. Nature, 625, 466–472.
\bibitem{wang2021kadapter}Wang, R., et al. (2021). K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters. ACL.
\bibitem{wang2021kepler}Wang, X., et al. (2021). KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation. TACL.
\bibitem{wang2023selfconsistency}Wang, X., et al. (2023). Self-Consistency Improves Chain of Thought Reasoning in LLMs. arXiv:2203.11171.
\bibitem{wei2022chain}Wei, J., et al. (2022). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. NeurIPS.
\bibitem{yao2023treeofthought}Yao, S., et al. (2023). Tree of Thoughts: Deliberate Problem Solving with LLMs. NeurIPS.
\end{thebibliography}

\appendix
\section{PVBench-Lite: Process-Valid Mini-Benchmark}
\label{app:pvbench}
We construct a small, licensed, leakage-aware benchmark with step annotations:
- Math: symbolic transformations checked by CAS; proofs checked by Lean/Isabelle where applicable.
- Code: bug-fix tasks with unit tests; steps are atomic edits validated by compiler/tests.
- Labels: tool-derived weak labels curated with spot-checks; provenance recorded; contamination scans performed where feasible.
We provide scripts for regeneration from source datasets and licenses.

\section{Proof of Theorem 1}
\label{app:proof_var}
Let $X\sim\mathrm{Bernoulli}(p)$; $\mathrm{Var}[X]=p(1-p)$. For terminal-only rewards, $G_t^{\mathrm{term}}=\gamma^{T-t} X$ so $\mathrm{Var}[G_t^{\mathrm{term}}]=\gamma^{2(T-t)}p(1-p)$. For dense rewards with independent $X_k\sim\mathrm{Bernoulli}(p_k)$, $G_t^{\mathrm{dense}}=\sum_{k=t}^T \gamma^{k-t} X_k$. Independence yields
\[
\mathrm{Var}[G_t^{\mathrm{dense}}]=\sum_{k=t}^T \gamma^{2(k-t)} p_k(1-p_k) \le \sum_{k=t}^T \gamma^{2(k-t)} \tfrac{1}{4}.
\]
If $p_k=p$ and $\gamma<1$, then $\sum_{k=t}^T \gamma^{2(k-t)} = \frac{1-\gamma^{2(T-t+1)}}{1-\gamma^2} < \gamma^{2(T-t)}$ for $T-t>0$, implying the stated inequality.

\section{MF-ARLAIF Sample-Efficiency Sketch}
\label{app:mf}
Assume rewards are drawn from a function $f(h)$ in an RKHS with bounded norm. The surrogate provides posterior mean $\mu_t$ and variance $\sigma_t$, and acquisition $a_t(h)=\mu_t(h)+\beta_t^{1/2}\sigma_t(h)$. Let model error be $|f(h)-\mu_t(h)|\le \beta_t^{1/2}\sigma_t(h)+\epsilon_t$ with $\epsilon_t$ decreasing via active updates. Following \cite{srinivas2010gaussian}, cumulative regret is bounded by $\tilde{\mathcal{O}}(\sqrt{T\gamma_T})$ in the noiseless setting; adding $\epsilon_t$ and fidelity gap yields
\[
R_T \le C_1 \sqrt{T\gamma_T} + C_2 \sum_{t=1}^T \epsilon_t + C_3 \Delta_{\mathrm{fid}},
\]
for constants $C_i$. Conservative policy updates ensure stability and preclude divergence under surrogate misspecification.

\section{Long Context vs Retrieval Protocol Details}
We equalize FLOPs and wall-time by controlling batch size, context length, and retrieval latency. Attention dilution is measured by injecting distractors and plotting accuracy vs effective relevant-token ratio. We provide scripts to replicate experiments, including content sharding and retrieval backends.

\section{Reproducibility Checklist}
\label{app:repro}
- Models: 7B–13B open LMs; versions and commit hashes.
- Training budgets: GPUs, hours, FLOPs, memory.
- Data sources: licenses, provenance, de-duplication, contamination scans.
- Prompts: templates, seeds, decoding parameters.
- Evaluation: fixed sampling budgets, BoN reporting, step-level metrics, verifiers used.
- Safety: filters, abstention thresholds, human-in-the-loop criteria.
- Release: code for SCMs, OIP-CAD, MF-ARLAIF; PVBench-Lite regeneration scripts.

\end{document}