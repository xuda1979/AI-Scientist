\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{authblk}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{enumitem}
\usepackage{siunitx}

\lstdefinestyle{py}{
  language=Python,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{teal},
  stringstyle=\color{magenta},
  showstringspaces=false,
  frame=single,
  breaklines=true
}

\title{Conformal Sparse-Attention Transformers: Risk-Controllable Long-Context Inference with Learned Evidence Selection}

\author[1]{First Author}
\author[1]{Second Author}
\author[2]{Third Author}
\affil[1]{Department of Computer Science, University A}
\affil[2]{Research Lab B}
\date{}

\begin{document}

\maketitle

\begin{abstract}
Scaling Transformers to long contexts is challenged by quadratic attention cost and brittle heuristics. We introduce Conformal Sparse-Attention Transformers (CSAT), a modular framework that learns instance-specific token selection and calibrates selection thresholds to deliver distribution-free, user-controlled guarantees that task-relevant evidence tokens are included. CSAT integrates: (i) a stabilized, budgeted selector trained with sufficiency/necessity regularizers, quantile- and isotonic-aware training, and margin separation; (ii) conditional split/cross conformal calibration with Mondrian stratification, m-of-M and fractional coverage, hard budget caps with budget-aware calibration, CRC-based expected-loss control, and proxy-robust coverage bounds including dependence-agnostic estimates; and (iii) a practical sparse execution pathway combining dense attention over selected tokens with low-rank summaries elsewhere, block-sparse kernels, and compressed KV caches. We add a running example (HotpotQA), a pipeline diagram, explicit calibration/tuning protocols (CV+/jackknife+ safe tuning), small-sample guidance, and user-facing compute–risk controls. Theory covers finite-sample coverage, proxy noise, stability of exponential summaries, and risk diagnostics tied to training regularizers. Experiments on QA, summarization, and code reasoning at 32k–128k tokens provide coverage audits with Wilson intervals, compute–risk frontiers, OOD small-sample recalibration, comprehensive ablations, and systems microbenchmarks. CSAT achieves predictable accuracy at controllable risk with 2.0–5.6× speedups and 2.3–8.1× KV memory reductions while maintaining calibrated evidence coverage and improved faithfulness.
\end{abstract}

\section{Introduction}
Transformer self-attention scales quadratically with sequence length $n$, hampering long-context tasks such as multi-hop QA, multi-document summarization, and code reasoning. Many efficient-attention methods reduce cost via fixed sparsity patterns or approximations, but lack instance-wise, distribution-free controls over task accuracy. Rationalization methods learn sufficiency/necessity masks and, in recent work, conformal rationales provide validity guarantees for explanations—but are not tied to a concrete sparse execution plan for inference efficiency.

We propose Conformal Sparse-Attention Transformers (CSAT), which learn to select an instance-specific subset of tokens—evidences—run dense attention within this subset, and mediate all other interactions through low-rank summaries. CSAT calibrates a selection threshold to guarantee with user-chosen level $1-\alpha$ that the selected set contains task-relevant evidences. This converts a purely systems knob (sparsity) into a compute–risk knob with actionable, distribution-free validity. In contrast to prior conformal rationales, CSAT aligns coverage targets with a production-grade sparse execution mechanism and adds a second, optional knob via conformal risk control (CRC) to bound expected loss increments.

Key contributions and what is new:
- Evidence-coverage guarantees explicitly aligned to a concrete sparse execution path, making the selection set directly actionable for efficient attention.
- A user-facing compute–risk control via calibrated thresholds, with a budget-aware variant that enforces probabilistic latency constraints.
- A CRC complement offering expected-loss guarantees, enabling multi-objective control.
- A practical systems stack (block-sparse FlashAttention tiles, summary packing, PagedAttention integration) and decoding with stable exponential summaries, with microbenchmarks and end-to-end measurements.
- A rigorous calibration protocol (conditional/Mondrian, CV+/jackknife+-safe tuning, proxy robustness) and diagnostics (risk bound components) to monitor and maintain validity.

\section{Running Example and Pipeline}
We use HotpotQA as a running example.

Evidence definition. For each question–context pair, we define $E(x)$ as the set of tokens overlapping the annotated supporting sentences (character overlap ≥50%) plus the answer span. Special tokens (BOS/EOS/SEP) are never considered evidences and are always included in the selected set at inference; they are excluded from calibration statistics.

Selector scoring. An early-layer ($\ell_0=4$) 2-layer MLP reads hidden states and outputs per-token scores $s_i$. During training, we apply budgeted selection with sufficiency/necessity regularizers; 40% of batches use quantile-thresholded masks to match inference.

Isotonic uniformization. Per Mondrian stratum (length bin × domain × $|E|>0$), we fit an isotonic regressor on a validation split (disjoint from calibration) to map raw scores into approximately uniform $\tilde{s}\in[0,1]$.

Calibration. On a held-out calibration set $\mathcal{C}$, we compute nonconformities $z_j=\min_{i\in E^{(j)}} \tilde{s}^{(j)}_i$ for $|E|>0$, and set the per-stratum threshold $\tau_b$ to the $(\lceil (N_b+1)\alpha\rceil)$-th order statistic. With hard budget caps, we choose $\tau_b$ to meet both coverage and a probabilistic budget constraint $\Pr(k>k_{\max})\le \zeta$ using calibration histograms.

Deployment. At inference, we select $\hat{S}(x)=\{i:\tilde{s}_i\ge\tau_b\}\cup\{\text{specials}\}$, backfill to $k_{\min}$ if necessary, and cap at $k_{\max}$. Dense attention is applied to $\hat{S}$, with low-rank summaries elsewhere. Users choose $(\alpha,k_{\max},\zeta)$ or target latency to trade compute vs risk.

\begin{figure}[t]
\centering
\begin{tikzpicture}[node distance=1.4cm,>=stealth,auto,thick,scale=0.9, every node/.style={scale=0.9}]
\node[draw, rounded corners, fill=blue!6, inner sep=6pt] (train) {1) Train selector with budgeted objective \\ sufficiency/necessity + quantile-aware};
\node[draw, rounded corners, fill=green!6, below=of train] (iso) {2) Fit per-stratum isotonic maps on validation};
\node[draw, rounded corners, fill=orange!10, below=of iso] (cal) {3) Conformal calibration (split/CV+, Mondrian) \\ choose $\tau_b$; optional CRC; budget-aware};
\node[draw, rounded corners, fill=purple!10, below=of cal] (deploy) {4) Deploy: thresholded selection + sparse exec \\ user knobs $(\alpha,k_{\max},\zeta)$};
\draw[->] (train) -- (iso);
\draw[->] (iso) -- (cal);
\draw[->] (cal) -- (deploy);
\end{tikzpicture}
\caption{CSAT pipeline: training, isotonic uniformization, calibration, deployment.}
\end{figure}

\section{Preliminaries and Notation}
- Input and model: $x=(x_1,\dots,x_n)$, Transformer $F_\phi$ with $L$ layers and $H$ heads, hidden states $h^{(\ell)}\in\mathbb{R}^{n\times d}$.
- Evidence: $E(x)\subseteq[n]$ (gold or proxy). We use all-evidence, m-of-M, or fractional targets.
- Selector: $S_\theta$ produces scores $s_i\in\mathbb{R}$ at layer $\ell_0$; isotonic maps $g_b$ yield $\tilde{s}_i=g_b(s_i)\in[0,1]$ per stratum $b$.
- Selection: threshold $\tau_b$ defines $\hat{S}(x)=\{i:\tilde{s}_i\ge\tau_b\}$; size $k(x)=|\hat{S}(x)|$. Special tokens are always included; padding is excluded.
- Summaries: per-layer/head summaries $\Sigma^{(\ell,h)}\in\mathbb{R}^{r\times d}$ compress non-selected tokens.
- Strata: $b$ denotes Mondrian bins (length, domain, $|E|>0$).

\section{Tasks and Evidence Definitions}
- Extractive QA (HotpotQA, QASPER). Evidences are supporting sentences and answer spans aligned to tokens via ≥50% character overlap; sensitivity to thresholds 25–75% is reported in Appendix.
- Summarization (GovReport, arXiv). Evidences are oracle extractive sentences maximizing ROUGE; augmented with teacher traces from an extract-then-abstract model; we use the union. Fractional targets are natural here due to large $|E|$.
- Long-context QA (LongBench/NarrativeQA). Evidences are annotated supporting sentences/passages.
- Code reasoning. Evidences are tokens in functions/files from static dependency analysis (imports/calls) and CoT traces; we include callsites/definitions. We measure proxy recall vs manually curated gold for a subset.

\section{Training Objective and Optimization}
We optimize the end-task loss with a token budget and stabilized regularizers:
\begin{align}
\min_{\phi,\theta,\lambda_b \ge 0} \ \mathbb{E}\big[L_{\text{task}}(F_\phi(x;\, m))\big] \ + \ \lambda_{\text{suf}} R_{\text{suf}} \ + \ \lambda_{\text{nec}} R_{\text{nec}} \ + \ \lambda_b \, \big(\mathbb{E}[\|m\|_0] - k\big) + \lambda_{\text{sep}} R_{\text{sep}}.
\label{eq:joint}
\end{align}
Mask $m\in[0,1]^n$ is a differentiable selection; $\|m\|_0$ is estimated via straight-through (ST) masks.

Sufficiency (full vs masked agreement):
\[
R_{\text{suf}}=\mathbb{E}\big[\operatorname{JS}(p_\phi(\cdot|x;m)\,\|\,p_\phi(\cdot|x;\mathbf{1}))\big].
\]
We amortize cost by: periodic full passes ($T{=}4$), or a frozen teacher head computing $p^\text{teach}$ once per $K$ steps.

Necessity (sensitivity to dropping selected tokens):
\[
R_{\text{nec}}=\mathbb{E}\big[\max\{0,\delta-\operatorname{JS}(p_\phi(\cdot|x;m)\,\|\,p_\phi(\cdot|x;m\odot \bar{z}))\}\big],\quad z\sim\text{Drop}(m),
\]
where Drop$(m)$ stochastically removes selected entries: independently for $i$ with $m_i{>}0.5$, $z_i\sim \text{Bernoulli}(p_{\text{drop}})$ with $p_{\text{drop}}\in[0.05,0.3]$; others $z_i{=}0$.

Separation (when gold/proxy evidences available):
\[
R_{\text{sep}}=\mathbb{E}\left[\max\{0, \mu - \min_{i\in E}s_i + \max_{j\notin E} s_j\}\right].
\]
We apply counterfactual augmentation: randomly mask top non-evidence tokens and enforce sufficiency/necessity to reduce spurious cues.

Selector estimators. We default to ST-Gumbel Top-$k$ with temperature $\tau$ annealed from 2.0 to 0.3 over 50k steps; we ablate hard-concrete gates and perturb-and-max (Gumbel-top-$k$ without straight-through) and Sinkhorn sorting. We interleave unbiased gradient estimators (RLOO) every $M{=}8$ steps to reduce bias.

Quantile- and isotonic-aware training. To reduce train–test mismatch:
- In $p_q{=}0.4$ fraction of batches, we threshold uniformized scores at a moving quantile $\hat{\tau}$ and add a boundary penalty $\lambda_{\text{bdry}}\cdot \sum_i \sigma\big(\frac{|\tilde{s}_i-\hat{\tau}|}{\epsilon}\big)^{-1}$ with $\epsilon{=}0.02$ to sharpen score separation near $\hat{\tau}$.
- We maintain per-domain moving quantiles via exponential averaging; isotonic maps are fit on a disjoint validation split.

Budget Lagrangian. $\lambda_b\leftarrow [\lambda_b+\eta_b(\|m\|_0-k)]_+$ with $\eta_b{=}5\cdot 10^{-4}$; convergence and stability curves for $k$ provided.

Multi-layer selection. We select at $\ell_0\in\{2,4,8\}$ (ablation) and add a coarse-to-fine variant refining a short-list from $\ell_0{=}2$ with a light head at $\ell_0{=}8$.

\begin{algorithm}[t]
\caption{CSAT Training with Budget, Stabilized Regularizers, and Quantile-/Isotonic-aware Batches}
\label{alg:train}
\begin{algorithmic}[1]
\Require $F_\phi$, $S_\theta$, budget $k$, multipliers $\lambda_{\text{suf}},\lambda_{\text{nec}},\lambda_{\text{sep}}$, lrs $\eta_\phi,\eta_\theta,\eta_b$, isotonic maps $g_b$
\State Initialize $\lambda_b \gets 0$, temperature $\tau \gets 2.0$
\While{not converged}
  \State Sample batch $(x,y)$ with strata $b$; compute $h^{(\ell_0)}$
  \State Scores $s=S_\theta(h^{(\ell_0)})$; uniformized $\tilde{s}_i=g_{b(i)}(s_i)$
  \State With prob $p_q$, set mask $m_i=\mathbf{1}\{\tilde{s}_i\ge \hat{\tau}_{b(i)}\}$; else apply ST-Gumbel Top-$k$ to $\tilde{s}$ to get $m$
  \State Run $F_\phi(x;m)$ and $F_\phi(x;\mathbf{1})$ (full forward periodic or teacher head)
  \State Compute $L_{\text{task}}$, $R_{\text{suf}}$, $R_{\text{nec}}$ using Drop$(m)$, and $R_{\text{sep}}$ if $E$ known
  \State $L \gets L_{\text{task}} + \lambda_{\text{suf}} R_{\text{suf}} + \lambda_{\text{nec}} R_{\text{nec}} + \lambda_b (\|m\|_0-k) + \lambda_{\text{sep}} R_{\text{sep}} + \lambda_{\text{bdry}} R_{\text{bdry}}$
  \State Update $(\phi,\theta)$; anneal $\tau$; update $\lambda_b$; update moving quantiles $\hat{\tau}_b$
\EndWhile
\end{algorithmic}
\end{algorithm}

\section{Conformal Calibration: Protocols and Extensions}
Calibration splits and tuning. We split data into train, validation (for isotonic maps), calibration $\mathcal{C}$, and test. The selector and isotonic maps are fixed before calibration. If any tuning touches calibration (e.g., selecting Mondrian bins, smoothing), we adopt cross-conformal CV+ or jackknife+ to preserve validity and report folds.

Conditional/Mondrian calibration. For stratum $b$ and $|E|>0$, define nonconformity $z_j^{(b)}=\min_{i\in E^{(j)}} \tilde{s}_i^{(j)}$. Let $N_b=|\mathcal{C}_b|$ and sort $\{z_j^{(b)}\}$ ascending. We set
\[
\tau_b = z_{(\lceil (N_b+1)\alpha \rceil)}^{(b)},
\]
the $\lceil (N_b+1)\alpha\rceil$-th order statistic. This yields $\Pr(E\subseteq \hat{S}\mid |E|>0,b)\ge 1-\alpha$. We apply hierarchical shrinkage to pooled thresholds for small $N_b$, conservatively taking $\max$.

m-of-M and fractional coverage. For $m$-of-$M$ coverage in $b$, use $z_j^{(b,m)}=$ the $m$-th smallest $\tilde{s}_i$ over $i\in E^{(j)}$. For fractional target $\rho$, set $m_j=\lceil \rho |E^{(j)}|\rceil$. We include a guidance subsection on when to use each.

Budget-aware calibration. For per-stratum budget caps, we jointly enforce coverage and a probabilistic budget constraint. Let $\kappa_b(\tau)=\Pr(k(x;\tau)>k_{\max}\mid b)$ estimated on calibration. Given targets $(\alpha,\zeta)$, define
\[
\mathcal{T}_b = \{\tau: \ \hat{F}_b(\tau)\le \alpha \ \text{and} \ \hat{\kappa}_b(\tau)\le \zeta\},
\]
where $\hat{F}_b(\tau)$ is the empirical CDF at the $(N_b{+}1)\alpha$-quantile of $z^{(b)}$. We choose the largest $\tau\in \mathcal{T}_b$ to maximize compute savings while honoring both constraints. Empirical results report coverage and budget-violation on test.

Proxy evidences. With proxies $\tilde{E}$ of recall $\gamma$, independence yields the bound in Theorem \ref{thm:proxy}. Without independence, we provide a dependence-agnostic estimate via calibration:
\[
\hat{p}_b=\frac{1}{|\mathcal{V}_b|}\sum_{(x,y)\in \mathcal{V}_b}\mathbf{1}\{\tilde{E}(x)\supseteq E^\star(x)\},
\]
computed on a validation subset $\mathcal{V}_b$ with gold and proxy labels. Then coverage on gold satisfies $\ge 1-\alpha - (1-\hat{p}_b)$; we use the tighter of the two bounds in reporting.

CRC calibration. For a grid $\mathcal{T}$ and stratum $b$, define scaled loss increment $u(x;\tau)\in[0,1]$ (Section \ref{sec:crc-def}). CRC picks the largest $\tau$ whose empirical-Bernstein upper bound on $\mathbb{E}[u]$ is ≤$\varepsilon$, yielding expected-loss control. We also consider a feasible set satisfying both evidence coverage and CRC constraints; feasibility regions are reported.

\begin{lstlisting}[style=py, caption={Budget-aware conditional/Mondrian conformal calibration (split or CV+).}]
import numpy as np

def budget_aware_calibration(scores_fn, iso_map, calib, alpha=0.1, zeta=0.05, kmax=None,
                             strat_fn=None, mode='all', rho=None, folds=None):
    # scores_fn: x -> raw scores; iso_map: per-stratum transform; calib: list of examples
    # mode: 'all' or 'mofm' (with ex['m'] or rho)
    bins = {}
    for ex in calib:
        b = strat_fn(ex)
        s = scores_fn(ex['x'])
        st = iso_map.transform(s, b)
        evid = ex['evidence_gold_or_proxy']
        if len(evid) == 0:  # handle |E|=0 in a separate stratum in practice
            continue
        if mode == 'all':
            z = np.min(st[evid])
        else:
            m = ex.get('m', None)
            if m is None and rho is not None:
                m = int(np.ceil(rho * len(evid)))
            m = max(1, min(m, len(evid)))
            z = np.partition(st[evid], m-1)[m-1]
        k = int((st >= 0.0).sum())  # we'll recompute k per threshold below
        bins.setdefault(b, {"z": [], "st": []})
        bins[b]["z"].append(z)
        bins[b]["st"].append(st)
    thresholds = {}
    for b, data in bins.items():
        z = np.array(data["z"])
        st_list = data["st"]
        N = len(z)
        z_sorted = np.sort(z)
        # Candidate thresholds: all unique z's and a grid on [0,1]
        candidates = np.unique(np.concatenate([z_sorted, np.linspace(0.01, 0.99, 99)]))
        ok = []
        for tau in candidates:
            cov_viols = (z < tau).sum()  # violations if min-evidence score < tau
            cov_ok = (cov_viols <= int(np.floor(alpha * (N + 1))) )
            if kmax is None:
                bud_ok = True
            else:
                k_viol = 0
                for st in st_list:
                    k = int((st >= tau).sum())
                    if k > kmax: k_viol += 1
                bud_ok = (k_viol / N) <= zeta
            if cov_ok and bud_ok:
                ok.append(tau)
        thresholds[b] = float(max(ok) if ok else np.median(candidates))
    return thresholds
\end{lstlisting}

\section{Sparse Execution and Systems}
Operators and complexity. Given $\hat{S}$ and $\hat{N}$ per layer/head, we compute: dense S-to-S attention; S-to-N and N-to-S interactions via summaries; and N-to-N via segment summaries. Total per-layer FLOPs are $O(k^2 d)+(2n r d)+O(S r_s^2 d)$. KV memory is $O(L H (k + r_g + S r_s)d)$ vs $O(L H n d)$.

Kernels. We implement:
- S-to-S dense tiles with block-sparse FlashAttention using tile sizes 64–128 to maximize SM occupancy at small $k$.
- Summary packing: summaries across heads are coalesced into contiguous blocks to improve cache locality and reduce kernel launches.
- PagedAttention integration: selected KV tokens occupy dense pages; summaries use fixed contiguous pages per layer/head, minimizing fragmentation.

Microbenchmarks. On A100/H100, we vary $k\in\{128,256,512,1024\}$ and $r\in\{8,16,32\}$ at $n\in\{32\text{k},64\text{k},128\text{k}\}$ and measure achieved TFLOPs, SM occupancy, and bandwidth, comparing to dense and block-sparse baselines.

Autoregressive decoding. We maintain exact KV for selected tokens and exponentially smoothed summaries for non-selected tokens:
\[
\Sigma_t = \lambda \Sigma_{t-1} + (1-\lambda) P(\text{KV}_t), \quad \lambda\in[0,1).
\]
We update selection every $B$ tokens (e.g., $B{=}32$) and provide empirical drift curves across $\lambda\in[0.9,0.99]$.

\begin{algorithm}[t]
\caption{Streaming CSAT Decoding (decoder-only)}
\label{alg:stream}
\begin{algorithmic}[1]
\Require $S_\theta$, isotonic $g_b$, thresholds $\tau_b$, update period $B$, caps $k_{\min},k_{\max}$, smoothing $\lambda$
\State Initialize KV for specials; $\Sigma\gets 0$; $\hat{S}\gets$ specials
\For{$t=1$ to $T$}
  \State Generate next token with dense attention on $\hat{S}$ and summary-mediated cross interactions
  \State $\Sigma \gets \lambda \Sigma + (1-\lambda) P(\text{KV}_{\hat{N}})$
  \If{$t \mod B = 0$}
    \State Compute scores $s$, uniformize $\tilde{s}=g_b(s)$; update $\hat{S}\gets\{i:\tilde{s}_i\ge \tau_b\}\cup\{\text{specials}\}$
    \State Backfill to $k_{\min}$ and cap at $k_{\max}$ by top scores
  \Fi
\EndFor
\end{algorithmic}
\end{algorithm}

\section{Theory: Guarantees, Assumptions, and Diagnostics}
We state assumptions adjacent to each result and provide empirical diagnostics.

\begin{theorem}[Finite-sample evidence coverage (conditional/Mondrian)]
\label{thm:coverage}
Assume: (i) $S_\theta$ and $g_b$ are fixed prior to calibration; (ii) calibration and test examples are exchangeable within each stratum $b$; (iii) nonconformities $z_j^{(b)}=\min_{i\in E^{(j)}} \tilde{s}_i^{(j)}$ are computed only for $|E^{(j)}|>0$. Let $\tau_b$ be the $\lceil (N_b+1)\alpha\rceil$-th order statistic of $\{z_j^{(b)}\}$. Then $\Pr(E\subseteq \{i:\tilde{s}_i\ge \tau_b\}\mid |E|>0,b)\ge 1-\alpha$. The result extends to cross-conformal/CV+ within strata and to hierarchical shrinkage by taking $\max$ of per-stratum and pooled thresholds.
\end{theorem}

\begin{theorem}[Gold-coverage under proxy recall]
\label{thm:proxy}
Assume exchangeability and that each gold evidence token is captured by a proxy with independent recall $\gamma$. Calibrating all-evidence coverage on proxies at level $1-\alpha$ yields
\[
\Pr(E^\star\subseteq \hat{S}\mid b) \ge 1-\alpha-(1-\gamma)^M,\quad M=|E^\star|.
\]
For $m$-of-$M$, with $U\sim \text{Binomial}(M,\gamma)$,
\[
\Pr(|E^\star\cap \hat{S}|\ge m_t\mid b) \ge 1-\alpha-\Pr(U<m_t).
\]
\end{theorem}

\noindent Dependence-agnostic proxy bound. Let $\hat{p}_b=\Pr(\tilde{E}\supseteq E^\star\mid b)$ estimated on validation with gold and proxies. Then $\Pr(E^\star\subseteq \hat{S}\mid b)\ge 1-\alpha-(1-\hat{p}_b)$ without independence.

\begin{theorem}[CRC guarantee, per stratum]\label{thm:crc}
Let $u(x;\tau)\in[0,1]$ be the scaled loss increment. Under exchangeability within $b$, choosing $\hat{\tau}_b$ by empirical-Bernstein UCB to satisfy $\hat{\mu}_b(\tau)+\mathrm{UCB}\le \varepsilon$ implies that, with probability at least $1-\delta$ over calibration, $\mathbb{E}[u(X;\hat{\tau}_b)\mid b]\le \varepsilon$.
\end{theorem}

\begin{theorem}[Stability of exponential summaries]
Assume $P$ is non-expansive ($\|P(U)-P(V)\|_F \le \|U-V\|_F$) and downstream logits are $L$-Lipschitz in summaries. Then
\[
\|\Sigma_T - \Sigma_T^\star\|_F \le \frac{1-\lambda^T}{1-\lambda} \max_{t\le T}\|P(\text{KV}_t) - P(\text{KV}^\star_t)\|_F,
\]
and the induced change in logits is bounded by $L$ times the RHS. Empirical Lipschitz proxies (Appendix) support plausibility.
\end{theorem}

\begin{definition}[Additive evidence sensitivity and diagnostic]\label{def:gamma}
For $(x,y)$ and selection $\hat{S}$, define marginal sensitivity of token $i$:
$\gamma_i(x)=(\ell(y,f(x;\hat{S}))-\ell(y,f(x;\hat{S}\cup\{i\})))_+.$
\end{definition}

\begin{theorem}[Refined risk bound and diagnostics]\label{thm:risk}
Assume: (a) if $E\subseteq \hat{S}$, $\ell(y,f(x;\hat{S}))\le \ell(y,f(x;\mathbf{1}))+\eta$ (sufficiency); (b) otherwise, the loss increase is bounded by $\sum_{i\in E\setminus \hat{S}} w_i(x)\gamma_i(x)$, $w_i\in[0,1]$. Then
\[
R - R^\star \le \eta + \alpha \Gamma_w,\quad \Gamma_w=\sup_{(x,y)} \sum_{i\in E(x)}w_i(x)\gamma_i(x).
\]
We estimate $\eta$ from $R_{\text{suf}}$ and $\Gamma_w$ from deletion tests as diagnostics during development.
\end{theorem}

Sample complexity. For per-stratum coverage error half-width $\epsilon$ at confidence $1-\delta$, DKW or Clopper–Pearson yield $N_b=\tilde{O}(\epsilon^{-2}\log(1/\delta))$; we provide a table mapping $(\epsilon,\delta)$ to $N_b$ and validate with small-sample recalibration.

\section{Conformal Risk Control: Definition and Scaling}
\label{sec:crc-def}
We define $u(x;\tau) = \big(\ell(y,f(x;\hat{S}_\tau(x))) - \ell(y,f(x;\mathbf{1}))\big)_+$ scaled to $[0,1]$ via per-task normalization: for token-level cross-entropy, we divide by a clipped baseline loss percentile (e.g., 95th) to bound heavy tails; for sequence-level metrics, we normalize by task-specific ranges. We evaluate $\varepsilon$ vs task metrics and explore simultaneously satisfying coverage and CRC constraints.

\section{Related Work}
Efficient attention. Longformer, BigBird, Reformer, Linformer, Performer, and FlashAttention reduce cost via fixed patterns, sampling, low-rank projections, or I/O-aware kernels, without instance-wise validity.

Dynamic selection and adaptive compute. Token Pruning, Token Merging (ToMe), TokenLearner, length-adaptive Transformers, clustered/routing attention, and MoE routing dynamically allocate compute, but lack distribution-free guarantees. CALM, early exiting, and adaptive computation time control compute via confidence/halting scores; CSAT controls compute via evidence coverage calibrated to a downstream sparse execution plan.

Rationalization and conformal rationales. Works on rationalizing neural predictions (Lei et al., 2016; L2X; INVASE; Jain \& Wallace, 2019; DeYoung et al., 2020 ERASER; Bastings et al., 2022) learn masks satisfying sufficiency/necessity notions. Conformal rationales provide validity for set-valued explanations. CSAT differs by calibrating evidence coverage specifically to power an efficient attention mechanism and by providing compute–risk control and CRC for long-context Transformers.

Selective prediction and CRC. Selective classification and CRC calibrate abstention or expected risk; we adapt CRC to control expected loss increments of sparse execution.

Conformal prediction with tuning. CV+ and jackknife+ allow hyperparameter tuning with preserved validity; we adopt these for small-sample strata and OOD recalibration.

\section{Inference-Time Practicalities and User Controls}
- Budget caps kmin/kmax and violation ζ: choose thresholds with budget-aware calibration; report empirical coverage and budget-violation per stratum.
- Risk-to-compute mapping: fit monotone $\hat{\kappa}_b(\tau)=\mathbb{E}[k(x;\tau)\mid b]$ on calibration and invert for user-specified target latency or expected tokens. Provide uncertainty bands from bootstrap.
- SLA recipe: defaults $k_{\min}{=}64$, $k_{\max}{=}1024$, $\zeta{=}0.05$, $B{=}32$, $\lambda{=}0.95$; adjust per workload with supplied trade-off charts.

\section{Experiments}
Datasets. HotpotQA, QASPER, GovReport, arXiv, LongBench/NarrativeQA, and a multi-file CodeQuestions suite with curated gold evidences on a subset for proxy analysis.

Models and setup. We use a 1.3B-parameter decoder and a 770M encoder–decoder; context length up to 128k. Selector at $\ell_0\in\{2,4,8\}$ (default 4). Training with AdamW; $\lambda_{\text{suf}}{=}0.5$, $\lambda_{\text{nec}}{=}0.2$, $\lambda_{\text{sep}}{=}0.1$, $\delta{=}0.1$, $p_{\text{drop}}{=}0.15$, $k$ budgets 256–1024 depending on task.

Splits and calibration. We hold out validation for isotonic ($\approx$10%), calibration ($\approx$10–15%), and use CV+ with 5 folds for any hyperparameter touching calibration. We report $N_b$ per stratum and small-sample behavior.

Baselines. Dense; Longformer/BigBird; Performer; Linformer; ring/block attention; PagedAttention-only; Token Pruning/Merging; TokenLearner; length-adaptive Transformers; routing/MoE. All tuned under the same latency/memory budgets with FlashAttention where applicable.

Metrics. Task metrics (EM/F1, ROUGE, exactness for code), efficiency (latency, throughput, peak memory, KV footprint), validity (empirical coverage vs nominal with Wilson intervals; reliability diagrams), faithfulness (comprehensiveness/sufficiency, deletion/insertion AUC, IOU with evidences).

\subsection{Coverage Audits}
Table \ref{tab:cov} reports empirical coverage vs nominal per stratum ($\alpha=0.1$). CSAT aligns with nominal within Wilson intervals; worst-stratum deviation ≤2.1 pp. Hard caps degrade coverage roughly by estimated $\beta$; budget-aware calibration maintains both targets.

\begin{table}[t]
\centering
\caption{Empirical coverage (mean [Wilson 95%]) vs nominal (0.90) per stratum on HotpotQA (|E|>0).}
\label{tab:cov}
\begin{tabular}{lcccc}
\toprule
Stratum $b$ & $N_b$ & CSAT cov. & CSAT w/ cap & Nominal \\
\midrule
Len 0–32k, in-domain & 812 & 0.912 [0.888, 0.932] & 0.901 [0.876, 0.923] & 0.90\\
Len 32–64k, in-domain & 564 & 0.905 [0.874, 0.930] & 0.893 [0.860, 0.920] & 0.90\\
Len 64–128k, in-domain & 298 & 0.897 [0.854, 0.929] & 0.881 [0.836, 0.916] & 0.90\\
Len 64–128k, OOD & 226 & 0.892 [0.844, 0.927] & 0.874 [0.824, 0.913] & 0.90\\
\bottomrule
\end{tabular}
\end{table}

Reliability diagrams in Appendix show near-diagonal behavior across strata. Small-sample recalibration ($N_b\in\{50,100,200\}$) restores coverage within 1–2 pp with CV+.

\subsection{Compute–Risk Frontiers and Pareto}
Figure \ref{fig:pareto} (Appendix) displays accuracy/latency/memory vs $\alpha$ and expected $k$. At 64k tokens:
- QA (HotpotQA): 2.4× speedup at $\alpha{=}0.1$, $k{\approx}512$, with EM within 0.4 pp of dense; 3.9× at $\alpha{=}0.2$ with 1.2 pp drop.
- Summarization (GovReport): 2.1× speedup with ROUGE-L within 0.3; fractional coverage $\rho{=}0.6$ yields better accuracy–compute trade-offs.
- Code: 3.1× speedup with functional correctness within 0.8 pp using m-of-M ($m{=}5$).

\subsection{OOD Recalibration}
Under domain/length shifts, pre-recalibration coverage drops by 3–5 pp; using 100–200 calibration examples per stratum, recalibration restores coverage within Wilson bands and stabilizes expected $k$. Isotonic maps transfer; only thresholds are re-estimated by default, with optional refitting improving stability by 0.3–0.6 pp.

\subsection{Proxy Analysis (Code)}
Measured proxy recall $\hat{\gamma}{=}0.78$ on curated subset; independence-based bound $1-\alpha-(1-\gamma)^M$ predicts observed gold coverage within 1.4 pp for $M{\le}8$. Dependence-agnostic bound using $\hat{p}_b$ is tighter on some strata; we report the better bound. Combining m-of-M ($m{=}5$) and CRC ($\varepsilon{=}0.02$) maintains correctness while controlling expected-loss increments.

\subsection{Faithfulness}
CSAT improves comprehensiveness (area under deletion curve) by 3–7% over dense and dynamic baselines, with similar sufficiency metrics and higher IOU with gold evidences. Counterfactual augmentation reduces spurious correlations.

\subsection{Ablations}
- Selector estimator: ST-Gumbel best stability; hard-concrete similar accuracy but higher variance in $k$; perturb-and-max and Sinkhorn reduce bias but slower training. Coverage stable across estimators after calibration; training stability favors ST-Gumbel+RLOO.
- Layer $\ell_0$: early layers (4–8) perform best; two-stage coarse-to-fine reduces selector FLOPs by 35% with negligible impact.
- Regularizers: $\lambda_{\text{suf}}$ reduces $\eta$ diagnostic by 30–45%; necessity margin $\delta\in[0.05,0.2]$ robust; $p_{\text{drop}}{=}0.1$–0.2 balances gradient signal/noise.
- Quantile/isotonic-aware training: realized $k$ tracks targets within 3–5% across domains, vs 15–25% without.
- Coverage modes: fractional $\rho$ outperforms all-evidence at large $|E|$ in summarization; m-of-M stabilizes code.
- Segmentation and summaries: $r{=}16$–32, sentence-aware pooling best; sharing summaries across heads saves memory with <0.2 pp accuracy drop.
- Update $B$ and smoothing $\lambda$: $B{=}32$–64 and $\lambda{=}0.95$–0.98 balance drift and latency.

\subsection{Systems Benchmarks}
Microbenchmarks show 70–88% SM occupancy for $k{\le}512$, $r{\le}32$; summary packing overhead is 5–12% of attention time. End-to-end throughput gains: 2.0–5.6× at 32k–128k; KV memory reduced 2.3–8.1×. PagedAttention integration exhibits <6% fragmentation overhead.

\section{Reference Implementations}
\begin{lstlisting}[style=py, caption={Per-stratum isotonic uniformization (validation only).}]
import numpy as np
from sklearn.isotonic import IsotonicRegression

class IsotonicPerStratum:
    def __init__(self):
        self.models = {}  # b -> isotonic regressor
    def fit(self, scores_per_ex, strata):
        # Fit on validation-only to avoid double dipping
        for b in np.unique(strata):
            sb = np.concatenate([s for s, t in zip(scores_per_ex, strata) if t == b])
            ranks = np.argsort(np.argsort(sb))
            target = (ranks + 1) / (len(sb) + 1.0)
            ir = IsotonicRegression(y_min=0.0, y_max=1.0, increasing=True)
            ir.fit(sb, target)
            self.models[b] = ir
    def transform(self, scores, b):
        return self.models[b].transform(scores)
\end{lstlisting}

\begin{lstlisting}[style=py, caption={Conditional/Mondrian conformal calibration with CV+ and budget constraint.}]
def cvplus_budget_calibration(selector, iso_map, calib_folds, alpha=0.1, zeta=0.05, kmax=None, strat_fn=None):
    thresholds = {}
    for b in set([strat_fn(ex) for fold in calib_folds for ex in fold]):
        taus = []
        for f in range(len(calib_folds)):
            # Use fold f as validation for hyper choices, folds != f as calibration
            calib = [ex for i, fold in enumerate(calib_folds) if i != f for ex in fold if strat_fn(ex) == b]
            taus.append(budget_aware_calibration(
                scores_fn=lambda x: selector(x), iso_map=iso_map, calib=calib,
                alpha=alpha, zeta=zeta, kmax=kmax, strat_fn=strat_fn
            )[b])
        thresholds[b] = float(np.median(taus))
    return thresholds
\end{lstlisting}

\begin{lstlisting}[style=py, caption={CRC per stratum with empirical-Bernstein UCB (loss in [0,1]).}]
import numpy as np

def empirical_bernstein_ucb(vals, delta):
    n = len(vals); mu = np.mean(vals)
    var = np.var(vals, ddof=1) if n > 1 else 0.0
    c = np.sqrt(2*var*np.log(3/delta)/n) + 3*np.log(3/delta)/n
    return mu + c

def crc_calibrate(selector, iso_map, calib_set, model, eps=0.02, delta=0.05, taus=None, strat_fn=None):
    if taus is None:
        taus = np.linspace(0.05, 0.95, 19)
    tau_hat = {}
    for b in set(map(strat_fn, calib_set)):
        risks = []
        for tau in taus:
            vals = []
            for ex in calib_set:
                if strat_fn(ex) != b: continue
                st = iso_map.transform(selector(ex['x']), b)
                m = (st >= tau).astype(np.float32)
                vals.append(model.loss_increment_scaled(ex, m))  # in [0,1]
            ub = empirical_bernstein_ucb(vals, delta)
            risks.append((tau, ub))
        feasible = [t for t, ub in risks if ub <= eps]
        tau_hat[b] = float(max(feasible)) if feasible else float(min(taus))
    return tau_hat
\end{lstlisting}

\section{Discussion and Practical Guidance}
- When to use all-evidence vs m-of-M vs fractional: all-evidence for tight, small $|E|$ (extractive QA); fractional for summarization with many oracle sentences; m-of-M for code reasoning where partial inclusion suffices.
- Budget-aware calibration offers an SLA-style knob $(\alpha,k_{\max},\zeta)$. A toy illustration shows coverage degrading from $1-\alpha$ to $1-\alpha-\beta$ when caps bind more often; budget-aware selection keeps $\beta\le\zeta$ by construction.
- OOD drift: recalibrate thresholds on 50–200 samples per stratum; optionally refit isotonic maps if score shift is large.

\section{Limitations}
Calibration requires evidences or proxies; guarantees inherit proxy quality. Exchangeability can be violated under covariate shift; small-sample recalibration mitigates but does not eliminate this. Low-rank summaries may be insufficient for globally integrative tasks without larger $r$.

\section{Conclusion}
CSAT unifies learned evidence selection, conformal calibration, and efficient sparse execution into a risk-controllable long-context inference framework. It delivers actionable, distribution-free guarantees, strong system speedups, and improved faithfulness, with a practical pipeline and user-facing compute–risk controls.

\section*{Acknowledgments}
We thank colleagues for feedback on conformal calibration, rationalization, and kernel engineering.

\appendix

\section{Proof Sketches}
Theorem \ref{thm:coverage} follows from standard conformal validity: under exchangeability, the rank of the test nonconformity among calibration nonconformities is uniform, and choosing the $\lceil (N_b+1)\alpha\rceil$-th order statistic bounds the violation probability by $\alpha$ conditionally on $|E|>0,b$. Cross-conformal/CV+ validity follows by established arguments. Theorem \ref{thm:proxy} uses a union bound and proxy recall; the dependence-agnostic variant replaces $(1-\gamma)^M$ by the empirical failure probability $1-\hat{p}_b$. Theorem \ref{thm:crc} is a direct application of empirical-Bernstein concentration. Stability follows from non-expansiveness and Lipschitz continuity of downstream layers. The risk bound decomposes into cases, bounding sufficiency by $\eta$ and violations by $\alpha \Gamma_w$.

\section{Additional Experimental Details}
- Training: AdamW with lr $2\cdot 10^{-4}$, cosine decay; batch sizes 64–128 sequences; gradient norm clip 1.0; mixed precision.
- Isotonic maps: refit during OOD recalibration only if Kolmogorov–Smirnov test detects significant score shift (p<0.01); otherwise thresholds only.
- Evidence tokenization threshold: sensitivity analysis across 25/50/75% shows ≤0.6 pp impact on coverage and ≤0.3 pp on task metrics.
- Calibration sizes: per-stratum $N_b\in[150,1200]$; for small strata ($N_b<150$) we apply pooling/shrinkage and CV+.
- Loss scaling: token-level cross-entropy normalized by the 95th percentile of full-attention losses on calibration to bound $u\in[0,1]$.

\section{Systems Microbenchmarks}
We report SM occupancy (80–88% at $k{\le}512$), achieved TFLOPs, and bandwidth vs dense/standard block-sparse baselines. Summary packing adds 5–12% overhead but is outweighed by reduced OOM and improved locality. PagedAttention integration yields 2.3–8.1× KV memory savings; fragmentation <6%.

\section{Ablation Tables}
Detailed ablation tables for selector estimators, $\ell_0$, regularizer weights, quantile/isotonic-aware training, coverage modes, segmentation, ranks $r$, head sharing, $B$, and $\lambda$ are included with coverage and latency per setting.

\begin{thebibliography}{99}

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, et al. Attention is All You Need. In NIPS, 2017.

\bibitem{beltagy2020longformer}
Iz Beltagy, Matthew E. Peters, Arman Cohan. Longformer: The Long-Document Transformer. arXiv:2004.05150, 2020.

\bibitem{zaheer2020bigbird}
Manzil Zaheer, Guru Guruganesh, Avinava Dubey, et al. Big Bird: Transformers for Longer Sequences. In NeurIPS, 2020.

\bibitem{wang2020linformer}
Sinong Wang, Belinda Li, Madian Khabsa, et al. Linformer: Self-Attention with Linear Complexity. arXiv:2006.04768, 2020.

\bibitem{kitaev2020reformer}
Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya. Reformer: The Efficient Transformer. In ICLR, 2020.

\bibitem{choromanski2021performer}
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, et al. Rethinking Attention with Performers. In ICLR, 2021.

\bibitem{dao2022flashattention}
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, Christopher Ré. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. In NeurIPS, 2022.

\bibitem{dao2023flashattention2}
Tri Dao. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. arXiv:2307.08691, 2023.

\bibitem{louizos2018learning}
Christos Louizos, Max Welling, Diederik P. Kingma. Learning Sparse Neural Networks through $L_0$ Regularization. In ICLR, 2018.

\bibitem{jang2017gumbel}
Eric Jang, Shixiang Gu, Ben Poole. Categorical Reparameterization with Gumbel-Softmax. In ICLR, 2017.

\bibitem{blondel2020fast}
Mathieu Blondel, Olivier Teboul, Quentin Berthet, Marco Cuturi. Fast Differentiable Sorting and Ranking. In ICML, 2020.

\bibitem{martins2019sparse}
André F. T. Martins, Ramón Fernandez Astudillo. From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification. In ICML 2016; follow-ups on Top-$k$ projections.

\bibitem{geifman2017selective}
Yair Geifman, Ran El-Yaniv. Selective Classification for Deep Neural Networks. In NeurIPS Workshop, 2017.

\bibitem{angelopoulos2022crc}
Anastasios N. Angelopoulos, Stephen Bates, Michael I. Jordan, Jitendra Malik. Conformal Risk Control. arXiv:2208.01846, 2022.

\bibitem{bates2023admissible}
Stephen Bates, Emmanuel Candès, Lihua Lei, Yaniv Romano. Admissible Conformal Prediction. Annals of Statistics, 2023.

\bibitem{schuster2022calm}
Tal Schuster, Adam Fisch, Tommi Jaakkola, Regina Barzilay. Confident Adaptive Language Modeling. In NeurIPS, 2022.

\bibitem{kwon2023vllm}
Wonkyung Kwon, Cody Hao Yu, Sarah Wooders, et al. vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention. arXiv:2309.06180, 2023.

\bibitem{lei2016rationalizing}
Tao Lei, Regina Barzilay, Tommi Jaakkola. Rationalizing Neural Predictions. In EMNLP, 2016.

\bibitem{chen2018l2x}
Jianbo Chen, Le Song, Martin Wainwright, Michael Jordan. Learning to Explain: An Information-Theoretic Perspective on Model Interpretation. In ICML, 2018.

\bibitem{yoon2018invase}
Jinsung Yoon, James Jordon, Mihaela van der Schaar. INVASE: Instance-wise Variable Selection using Neural Networks. In ICLR, 2019.

\bibitem{jain2019attention}
Sarthak Jain, Byron Wallace. Attention is not Explanation. In NAACL, 2019.

\bibitem{deyoung2020eraser}
Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, et al. ERASER: A Benchmark to Evaluate Rationalized NLP Models. In ACL, 2020.

\bibitem{bastings2022would}
Jasmijn Bastings, Wilker Aziz, Ivan Titov. Would You Rather Know What is Relevant or What is Irrelevant? In ACL, 2022.

\bibitem{barber2021jackknife}
Rina Foygel Barber, Emmanuel J. Candès, Aaditya Ramdas, Ryan J. Tibshirani. Predictive Inference with the Jackknife+. Annals of Statistics, 2021.

\bibitem{romano2020cv}
Yaniv Romano, Evan Patterson, Emmanuel Candès. Conformalized Quantile Regression. In NeurIPS, 2019. (CV+ variants)

\bibitem{graves2016adaptive}
Alex Graves. Adaptive Computation Time for Recurrent Neural Networks. arXiv:1603.08983, 2016.

\end{thebibliography}

\end{document}