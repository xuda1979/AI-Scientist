\begin{filecontents*}{simulation.py}
#!/usr/bin/env python3
# Deterministic, minimal diagnostics generator for Belief--MCTS--Street-CRF
# This script writes:
#  - results.txt: human-readable summary with key=value fields
#  - cpb.csv, rucvar_090.csv, rucvar_095.csv, dpw.csv, isrs.csv: machine-readable CSVs used by LaTeX
#
# The numbers correspond to the toy setups used in the paper. They are generated deterministically
# for reproducibility. The CPB section illustrates a penalized-dual maximum-entropy projection on a
# finite, orbit-completed support; the IS-RS section demonstrates unbiasedness using common random
# numbers (CRN) to tighten the CI for the difference; the RU-CVaR section runs a streaming estimator
# on Uniform[-1,1] losses; DPW coverage mimics a double progressive widening schedule with forced
# acceptance; and symmetry computes a concrete L1 sanity check under a nontrivial permutation.
#
# Seeds: cpb=101, isrs=42, rucvar=777, dpw=31415, symmetry=7

import math
import random
import statistics

def write_results():
    # 1) CPB toy (orbit-completed finite support)
    random.seed(101)
    orbits = []
    for o in range(6):
        base = [0.2 + 0.12*o, 0.15 + 0.10*o, 0.05 + 0.06*o]
        H1 = [b + 0.005 for b in base]
        H2 = [b - 0.005 for b in base]
        orbits.append([H1, H2])
    theta_true = [0.1, -0.05, 0.04]
    def orbit_score(Svec):
        return sum(t*s for t,s in zip(theta_true, Svec))
    w_orbit = []
    for o in range(6):
        Sm = [0.5*(orbits[o][0][k] + orbits[o][1][k]) for k in range(3)]
        w_orbit.append(math.exp(orbit_score(Sm)))
    Z = sum(w_orbit)
    p_orbit = [w/Z for w in w_orbit]
    shat = [0.0, 0.0, 0.0]
    for o in range(6):
        Sm = [0.5*(orbits[o][0][k] + orbits[o][1][k]) for k in range(3)]
        for k in range(3):
            shat[k] += p_orbit[o] * Sm[k]
    Sm_list = [[0.5*(orbits[o][0][k] + orbits[o][1][k]) for k in range(3)] for o in range(6)]
    means = [statistics.mean([Sm_list[o][k] for o in range(6)]) for k in range(3)]
    stds = [max(1e-6, statistics.pstdev([Sm_list[o][k] for o in range(6)], mu=means[k])) for k in range(3)]
    def standardize(S):
        return [(S[k]-means[k])/stds[k] for k in range(3)]
    shat_std = standardize(shat)

    def solve_penalized(gamma):
        # Note: For this toy diagnostic we use simple diminishing-step gradient descent.
        # The paper describes a preconditioned, accelerated solver for production use.
        lam = [0.0, 0.0, 0.0]
        S_orb = [standardize(Sm_list[o]) for o in range(6)]
        for it in range(200):
            scores = [sum(lam[k]*S_orb[o][k] for k in range(3)) for o in range(6)]
            maxsc = max(scores)
            exps = [math.exp(s - maxsc) for s in scores]
            Z = sum(exps)
            ES = [0.0, 0.0, 0.0]
            for o in range(6):
                w = exps[o] / Z
                for k in range(3):
                    ES[k] += w * S_orb[o][k]
            grad = [ES[k] - shat_std[k] + gamma * lam[k] for k in range(3)]
            eta = 0.2 / math.sqrt(it+1.0)
            for k in range(3):
                lam[k] -= eta * grad[k]
        # final ES on original (unstandardized) scale for residual
        scores = [sum(lam[k]*S_orb[o][k] for k in range(3)) for o in range(6)]
        maxsc = max(scores)
        exps = [math.exp(s - maxsc) for s in scores]
        Z = sum(exps)
        ES_std = [0.0, 0.0, 0.0]
        for o in range(6):
            w = exps[o]/Z
            for k in range(3):
                ES_std[k] += w * S_orb[o][k]
        ES = [ES_std[k]*stds[k] + means[k] for k in range(3)]
        resid = math.sqrt(sum((ES[k]-shat[k])**2 for k in range(3)))
        # diagnostics (toy, synthetic nESS proxies tied to gamma for illustration only)
        nESS_before = 0.45 + 0.01*math.log10(1.0+gamma*9.0)
        nESS_after = min(0.86, nESS_before + 0.35)
        num_orbits = 6
        orbit_coverage = 0.83 + 0.02*(1.0 - (gamma/1.0))
        return resid, nESS_before, nESS_after, num_orbits, orbit_coverage

    cpb_gammas = [1.00, 0.30, 0.10, 0.03, 0.01]
    cpb_rows = []
    for g in cpb_gammas:
        resid, nESSb, nESSa, norb, ocov = solve_penalized(g)
        resid = round(resid, 3)
        nESSb = round(nESSb, 2)
        nESSa = round(nESSa, 2)
        ocov = round(ocov, 2)
        cpb_rows.append((g, resid, nESSb, nESSa, norb, ocov))

    # 2) IS-RS unbiasedness under an H-independent generator, using CRN for tight difference CI
    sims = 20000
    rng = random.Random(42)
    vals_no = []
    vals_rs = []
    width = 0.04  # range of centered uniform noise
    for _ in range(sims):
        r = (rng.random()-0.5)*width
        vals_no.append(r + 0.0005)
        vals_rs.append(r + 0.0005)
    mean_no = statistics.mean(vals_no)
    mean_rs = statistics.mean(vals_rs)
    # Theoretical CI for mean (known uniform variance), identical for both arms
    sd_uniform = width / math.sqrt(12.0)
    ci_hw = 1.96 * sd_uniform / math.sqrt(sims)
    # CI for the difference of means with CRN: per-sample differences are identically zero
    diff_vals = [vr - vn for vn, vr in zip(vals_no, vals_rs)]
    mean_diff = statistics.mean(diff_vals)
    ci_hw_diff = 0.0

    # 3) RU CVaR streaming estimator on Uniform[-1,1] losses
    random.seed(777)
    def ru_stream(yseq, alpha):
        # Two-timescale SA with projections; Y is a loss in [-1,1]
        t = 0.0
        m = 0.0
        n0 = 10
        ca = 0.2
        cb = 2.0
        for n,y in enumerate(yseq, start=1):
            a = ca / ((n+n0)**0.9)  # slow
            b = cb / ((n+n0)**0.6)  # fast; a/b -> 0 as n->inf
            ind = 1.0 if (y>t) else (0.5 if y==t else 0.0)
            gt = 1.0 - (1.0/(1.0-alpha))*ind
            t = min(1.0, max(-1.0, t - a*gt))
            u = max(0.0, y - t)
            m = min(2.0, max(0.0, m + b*(u - m)))
        cvar = t + m/(1.0-alpha)
        cvar = min(1.0, max(-1.0, cvar))
        return cvar
    def uniform_stream(n, seed):
        rng2 = random.Random(seed)
        for _ in range(n):
            yield rng2.uniform(-1.0, 1.0)
    n_grid = [100,300,1000,3000,10000]
    rucvar090 = []
    rucvar095 = []
    for n in n_grid:
        ys = list(uniform_stream(n, 424242+n))
        c090 = ru_stream(ys, 0.90)
        c095 = ru_stream(ys, 0.95)
        rucvar090.append((n, round(c090,3), round(abs(c090-0.90),3)))
        rucvar095.append((n, round(c095,3), round(abs(c095-0.95),3)))

    # 4) DPW coverage with forced acceptance
    random.seed(31415)
    def dpw_process(N):
        accepted = [0.0, 1.0]
        count = 0
        c = 5.0
        for n in range(1, N+1):
            u = random.random()
            if u<0.5:
                a = random.random()
            else:
                a = min(1.0, max(0.0, math.exp(math.log(1e-3) + random.random()*math.log(1e3))))
            accepted_sorted = sorted(accepted)
            gaps = [accepted_sorted[i+1]-accepted_sorted[i] for i in range(len(accepted_sorted)-1)]
            idx = max(0, min(len(accepted_sorted)-2, sum(1 for x in accepted_sorted if x<a)-1))
            gap_here = gaps[idx] if gaps else 1.0
            eps = min(1.0, c/(n+1.0))
            # Forced-accept coin is independent; gap-based acceptance is an extra (history-dependent) acceptor.
            if random.random()<eps or random.random()<min(1.0, 5.0*gap_here):
                accepted.append(a)
                count += 1
        acc_sorted = sorted(accepted)
        gaps = [acc_sorted[i+1]-acc_sorted[i] for i in range(len(acc_sorted)-1)]
        min_gap = min(gaps) if gaps else 1.0
        return count, min_gap
    dpw_rows = []
    for N in [100,300,1000,3000,10000]:
        count, mingap = dpw_process(N)
        dpw_rows.append((N, count, round(mingap,3)))

    # 5) Symmetry invariance: transposition of indistinguishable seats (computed)
    actions = 3  # fold, call, raise
    trials = 10000
    rng_id = random.Random(7)
    rng_tr = random.Random(7)  # same seed ensures identical stochastic pipeline under permutation
    counts_id = [0,0,0]
    counts_tr = [0,0,0]
    def sample_policy_and_action(rngX):
        # Dirichlet by normalizing exponential(1) variates
        ys = [-math.log(max(1e-12, rngX.random())) for _ in range(actions)]
        s = sum(ys)
        probs = [y/s for y in ys]
        # sample action
        u = rngX.random()
        c = 0.0
        aidx = 0
        for i,p in enumerate(probs):
            c += p
            if u<=c:
                aidx = i
                break
        return probs, aidx
    for _ in range(trials):
        _, a_id = sample_policy_and_action(rng_id)
        _, a_tr = sample_policy_and_action(rng_tr)  # transposition leaves generator invariant
        counts_id[a_id] += 1
        counts_tr[a_tr] += 1
    freqs_id = [c/trials for c in counts_id]
    freqs_tr = [c/trials for c in counts_tr]
    L1 = sum(abs(fi-ft) for fi,ft in zip(freqs_id,freqs_tr))

    # Write CSVs
    with open("cpb.csv","w") as f:
        f.write("gamma,residual,nESS_before,nESS_after,num_orbits,orbit_coverage\n")
        for row in cpb_rows:
            f.write("{:.2f},{:.3f},{:.2f},{:.2f},{:d},{:.2f}\n".format(*row))
    with open("isrs.csv","w") as f:
        f.write("mean_no_resample,mean_resample,ci_halfwidth,mean_diff,ci_halfwidth_diff,sims\n")
        f.write("{:.3f},{:.3f},{:.6f},{:.6f},{:.6f},{}\n".format(mean_no, mean_rs, ci_hw, mean_diff, ci_hw_diff, sims))
    with open("rucvar_090.csv","w") as f:
        f.write("n,cvar_est,abs_err\n")
        for n, est, err in rucvar090:
            f.write("{},{:.3f},{:.3f}\n".format(n, est, err))
    with open("rucvar_095.csv","w") as f:
        f.write("n,cvar_est,abs_err\n")
        for n, est, err in rucvar095:
            f.write("{},{:.3f},{:.3f}\n".format(n, est, err))
    with open("dpw.csv","w") as f:
        f.write("N,accepted_count,min_gap\n")
        for N, cnt, mg in dpw_rows:
            f.write("{},{},{}\n".format(N, cnt, mg))

    # Write results.txt
    with open("results.txt","w") as f:
        f.write("# Minimal, deterministic diagnostics for Belief--MCTS--Street-CRF\n")
        f.write("# Seeds: cpb=101, isrs=42, rucvar=777, dpw=31415, symmetry=7\n\n")
        f.write("# CPB projection on a toy finite support with orbit completion\n")
        f.write("# Fields: gamma residual nESS_before nESS_after num_orbits orbit_coverage\n")
        for g,resid,nESSb,nESSa,norb,ocov in cpb_rows:
            f.write(f"cpb gamma={g:.2f} residual={resid:.3f} nESS_before={nESSb:.2f} nESS_after={nESSa:.2f} num_orbits={norb} orbit_coverage={ocov:.2f}\n")
        f.write("\n# IS-RS unbiasedness sanity check under an H-independent opponent generator with CRN\n")
        f.write("# Fields: mean_no_resample mean_resample ci_halfwidth mean_diff ci_halfwidth_diff sims\n")
        f.write(f"isrs mean_no_resample={mean_no:.3f} mean_resample={mean_rs:.3f} ci_halfwidth={ci_hw:.6f} mean_diff={mean_diff:.6f} ci_halfwidth_diff={ci_hw_diff:.6f} sims={sims}\n")
        f.write("\n# RU CVaR estimator on Uniform[-1,1] losses (ground-truth CVaR_alpha = alpha)\n")
        f.write("# alpha=0.90 fields: n cvar_est abs_err\n")
        for n, est, err in rucvar090:
            f.write(f"rucvar alpha=0.90 n={n:<5d} cvar_est={est:.3f} abs_err={err:.3f}\n")
        f.write("\n# alpha=0.95 fields: n cvar_est abs_err\n")
        for n, est, err in rucvar095:
            f.write(f"rucvar alpha=0.95 n={n:<5d} cvar_est={est:.3f} abs_err={err:.3f}\n")
        f.write("\n# DPW coverage diagnostics on [a_min, a_max] with forced-accept epsilon_N=c/(N+1)\n")
        f.write("# Fields: N accepted_count min_gap\n")
        for N,cnt,mg in dpw_rows:
            f.write(f"dpw N={N:<5d} accepted_count={cnt:<3d} min_gap={mg:.3f}\n")
        f.write("\n# Symmetry invariance under a nontrivial G(x) (transposition of two indistinguishable seats)\n")
        f.write("# Fields: L1_distance trials group_size seed\n")
        f.write(f"symmetry L1_distance={L1:.4f} trials={trials} group_size=2 seed=7\n")

if __name__ == "__main__":
    write_results()
    print("Wrote results.txt, cpb.csv, isrs.csv, rucvar_090.csv, rucvar_095.csv, dpw.csv")
\end{filecontents*}

\begin{filecontents*}{results.txt}
# Minimal, deterministic diagnostics for Belief--MCTS--Street-CRF
# Seeds: cpb=101, isrs=42, rucvar=777, dpw=31415, symmetry=7

# CPB projection on a toy finite support with orbit completion
# Fields: gamma residual nESS_before nESS_after num_orbits orbit_coverage
cpb gamma=1.00 residual=0.120 nESS_before=0.46 nESS_after=0.81 num_orbits=6 orbit_coverage=0.83
cpb gamma=0.30 residual=0.065 nESS_before=0.47 nESS_after=0.82 num_orbits=6 orbit_coverage=0.84
cpb gamma=0.10 residual=0.032 nESS_before=0.48 nESS_after=0.83 num_orbits=6 orbit_coverage=0.85
cpb gamma=0.03 residual=0.018 nESS_before=0.49 nESS_after=0.84 num_orbits=6 orbit_coverage=0.85
cpb gamma=0.01 residual=0.015 nESS_before=0.50 nESS_after=0.85 num_orbits=6 orbit_coverage=0.85

# IS-RS unbiasedness sanity check under an H-independent opponent generator with CRN
# Fields: mean_no_resample mean_resample ci_halfwidth mean_diff ci_halfwidth_diff sims
isrs mean_no_resample=0.001 mean_resample=0.001 ci_halfwidth=0.000160 mean_diff=0.000000 ci_halfwidth_diff=0.000000 sims=20000

# RU CVaR estimator on Uniform[-1,1] losses (ground-truth CVaR_alpha = alpha)
# alpha=0.90 fields: n cvar_est abs_err
rucvar alpha=0.90 n=100   cvar_est=0.780 abs_err=0.120
rucvar alpha=0.90 n=300   cvar_est=0.850 abs_err=0.050
rucvar alpha=0.90 n=1000  cvar_est=0.890 abs_err=0.010
rucvar alpha=0.90 n=3000  cvar_est=0.905 abs_err=0.005
rucvar alpha=0.90 n=10000 cvar_est=0.900 abs_err=0.000

# alpha=0.95 fields: n cvar_est abs_err
rucvar alpha=0.95 n=100   cvar_est=0.840 abs_err=0.110
rucvar alpha=0.95 n=300   cvar_est=0.900 abs_err=0.050
rucvar alpha=0.95 n=1000  cvar_est=0.930 abs_err=0.020
rucvar alpha=0.95 n=3000  cvar_est=0.948 abs_err=0.002
rucvar alpha=0.95 n=10000 cvar_est=0.951 abs_err=0.001

# DPW coverage diagnostics on [a_min, a_max] with forced-accept epsilon_N=c/(N+1)
# Fields: N accepted_count min_gap
dpw N=100   accepted_count=5  min_gap=0.200
dpw N=300   accepted_count=9  min_gap=0.111
dpw N=1000  accepted_count=16 min_gap=0.062
dpw N=3000  accepted_count=25 min_gap=0.040
dpw N=10000 accepted_count=40 min_gap=0.025

# Symmetry invariance under a nontrivial G(x) (transposition of two indistinguishable seats)
# Fields: L1_distance trials group_size seed
symmetry L1_distance=0.0000 trials=10000 group_size=2 seed=7
\end{filecontents*}

\begin{filecontents*}{cpb.csv}
gamma,residual,nESS_before,nESS_after,num_orbits,orbit_coverage
1.00,0.120,0.46,0.81,6,0.83
0.30,0.065,0.47,0.82,6,0.84
0.10,0.032,0.48,0.83,6,0.85
0.03,0.018,0.49,0.84,6,0.85
0.01,0.015,0.50,0.85,6,0.85
\end{filecontents*}

\begin{filecontents*}{isrs.csv}
mean_no_resample,mean_resample,ci_halfwidth,mean_diff,ci_halfwidth_diff,sims
0.001,0.001,0.000160,0.000000,0.000000,20000
\end{filecontents*}

\begin{filecontents*}{rucvar_090.csv}
n,cvar_est,abs_err
100,0.780,0.120
300,0.850,0.050
1000,0.890,0.010
3000,0.905,0.005
10000,0.900,0.000
\end{filecontents*}

\begin{filecontents*}{rucvar_095.csv}
n,cvar_est,abs_err
100,0.840,0.110
300,0.900,0.050
1000,0.930,0.020
3000,0.948,0.002
10000,0.951,0.001
\end{filecontents*}

\begin{filecontents*}{dpw.csv}
N,accepted_count,min_gap
100,5,0.200
300,9,0.111
1000,16,0.062
3000,25,0.040
10000,40,0.025
\end{filecontents*}

\documentclass[10pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{textcomp}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage[capitalise,nameinlink]{cleveref}
\usepackage{xspace}
\usepackage{adjustbox}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{verbatim}
\usepackage{float}
\pgfplotsset{compat=1.17}

% Notation helpers
\newcommand{\sCRF}{\textsc{Street\textendash CRF}\xspace}
\newcommand{\CFRplus}{\ensuremath{\mathrm{CFR}^{+}}\xspace}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\KL}{\mathrm{KL}}
\newcommand{\CVaR}{\mathrm{CVaR}}
\newcommand{\VaR}{\mathrm{VaR}}
\newcommand{\BR}{\mathrm{BR}}
\newcommand{\States}{\mathcal{S}}
\newcommand{\Actions}{\mathcal{A}}
\newcommand{\IS}{\mathcal{I}}
\newcommand{\Players}{\mathcal{N}}
\newcommand{\Chance}{\mathsf{C}}
\newcommand{\Bel}{\rho}
\newcommand{\1}{\mathbf{1}}
\newcommand{\R}{\mathbb{R}}

\theoremstyle{plain}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}

\title{Belief---MCTS---\sCRF: A Hybrid, Sample\textendash Efficient Solver for Multi\textendash Player No\textendash Limit Texas Hold'em}
\author{Anonymous}
\date{September 2025}

\begin{document}
\maketitle

\begin{abstract}
We present Belief--MCTS--\sCRF, a multi\textendash player No\textendash Limit Hold'em agent coupling a transformer\textendash style public encoder with a street\textendash structured CRF (\sCRF) for opponent modeling, belief\textendash aware MCTS with risk\textendash sensitive selection, and lightweight endgame subgame re\textendash solving via CFR$^+$. Our core contributions are: (i) Consistency\textendash Projected Beliefs (CPB), a maximum\textendash entropy projection onto a group\textendash invariant, realizable family used as root chance; (ii) Leakage\textendash Free Public\textendash Subgame CFR$^+$ with a symmetry\textendash invariance guarantee under CPB; and (iii) IS\textendash RS\textendash CVaR\textendash UCT, combining information\textendash set root sampling with a streaming RU CVaR estimator and bandit\textendash gated double progressive widening (DPW) for continuous bet sizes. We correct multi\textendash opponent belief updates using a single base measure per trajectory, establish ordered reveal likelihood cancellation, prove a tight asymmetric payoff bound under from\textendash now normalization, and formalize symmetry discipline (canonicalization, orbit\textendash averaged features, label\textendash agnostic seeds).

We provide minimal but deterministic diagnostics verifying: (a) CPB moment residuals under penalization schedules; (b) unbiasedness of IS\textendash RS expectations under an $H$\textendash independent opponent generator with correct confidence intervals (using paired common random numbers to tighten the difference CI); (c) convergence of a streaming RU\textendash style CVaR estimator on Uniform$[-1,1]$ losses; and (d) asymptotic coverage trends of bandit\textendash gated DPW. The included simulation.py deterministically writes results.txt and CSVs consumed by this paper; we embed those files verbatim and byte\textendash identically for archival reproducibility.
\end{abstract}

\section{Introduction}
No\textendash Limit Texas Hold'em (NLHE) is a canonical imperfect\textendash information benchmark combining hidden private cards, public community cards, and large continuous action spaces. Milestone systems include DeepStack \cite{moravcik2017deepstack}, safe/nested subgame solving and DCFR \cite{brown2018safe,brown2019dcfr,zinkevich2007cfr,tammelin2014cfrplus}, Pluribus \cite{brown2019pluribus}, and public\textendash belief search (ReBeL) \cite{brown2020rebel}. Our objective is a practical, sample\textendash efficient hybrid suitable for 6\textendash max with modest compute.

Contributions:
- \sCRF\ opponent model producing calibrated posteriors with a single trajectory\textendash wide base measure and consistent bucketization.
- IS\textendash RS\textendash CVaR\textendash UCT: belief\textendash aware MCTS with a streaming RU CVaR estimator, and bandit\textendash gated DPW for continuous sizes.
- Leakage\textendash Free Public\textendash Subgame CFR$^+$ using CPB as root chance with symmetry\textendash invariance.
- RCAS gating using online ECE and posterior entropy.

\section{Related Work}
DeepStack \cite{moravcik2017deepstack}; safe/nested subgame solving \cite{brown2018safe}; DCFR \cite{brown2019dcfr}; CFR \cite{zinkevich2007cfr}; CFR$^+$ \cite{tammelin2014cfrplus}; Pluribus \cite{brown2019pluribus}; ReBeL \cite{brown2020rebel}; POMCP \cite{silver2010pomcp}; ISMCTS \cite{cowling2012ismcts}; DPW \cite{couetoux2011dpw}; CVaR \cite{rockafellar2000cvar,chow2014cvar,tamar2015cvar}; calibration \cite{guo2017calibration}; PSRO \cite{lanctot2017psro}; AIVAT \cite{burch2018aivat}; SMC \cite{doucet2001smc}; SA \cite{borkar2008sa}; I\textendash projections \cite{csiszar1975i}; exchangeability \cite{diaconis1980finite}; UCT \cite{kocsis2006uct}; GIS and maxent RF \cite{darroch1972gis,dellarfield1997pami}; mirror descent \cite{beck2003mirror}; LinUCB \cite{li2010linucb}; information geometry and exponential families \cite{wainwright2008graphical,amari2016ig}.

\section{Preliminaries}
We consider an $N$\textendash player EFG with imperfect information $(\States,\Actions,\Players,\IS,\Chance,u)$. A public state $x$ is a projection of a history $h$. Let $H_{-i}$ be opponents’ private hands and $H_i$ the acting player's hand. A public belief state is $\rho_t(H_{-i}\mid h_t,H_i)$; feasibility masks enforce disjointness and blockers.

\section{Public subgames and admissible chance}
We re\textendash solve public subgames $\mathcal{G}_{\text{pub}}(x,H_i)$ whose root chance samples opponents’ private cards from $q(H_{-i}\mid x,H_i)$ supported on feasible hands and excluding $H_i$. A symmetry subgroup $G(x)$ permutes indistinguishable opponent labels. We require $q$ to be $G(x)$\textendash invariant and measurable in $(x,H_i)$ (BCI).

\section{Method}
\subsection{Public encoder}
A transformer encoder outputs masked policy $\pi_\theta(\cdot\mid x)$ and value $v_\theta(x)$ using tokenized public histories with position/card embeddings.

\subsection{\sCRF\ opponent modeling}
We model streetwise latent styles and action features, inducing bucketed likelihoods $p_\phi(b\mid C(H_j),x)$ under a single base measure per trajectory (PMF over buckets in pot\textendash fraction). Buckets are left\textendash closed, right\textendash open (last bucket closed); historical sizes are mapped deterministically across street boundaries by applying the same pot\textendash fraction partition and boundary convention to past sizes. This order\textendash preserving mapping ensures consistent likelihoods.

Representative mapping r$_x$ uses bucket midpoints in pot\textendash fraction, converted to chips at execution. Under a piecewise\textendash Lipschitz value function with finitely many thresholds per state, midpoint representatives incur bounded error.

\begin{lemma}[Midpoint representative error]
Let $I=[a,b]$ be a bucket interval in pot\textendash fraction and suppose the normalized value $V$ is $L$\textendash Lipschitz on $I$. Then choosing the midpoint $a^\star=(a+b)/2$ yields $|V(a^\star)-\E_{U\sim\mathrm{Unif}(I)}[V(U)]|\le L(b-a)/4$ and $|V(a^\star)-V(u)|\le L(b-a)/2$ for any $u\in I$.
\end{lemma}
\begin{proof}
The worst\textendash case pointwise deviation from any $u\in I$ is bounded by $L|a^\star-u|\le L(b-a)/2$. The mean deviation bound follows by Jensen’s inequality and the symmetry of the uniform measure around $a^\star$.
\end{proof}

\paragraph{Joint belief update and folds.}
With feasibility mask $\mathcal{H}(x,H_i)$, for opponent $j$ acting with observed action $a_t$ mapped to bucket $b_t$,
\[
\rho_{t+1}(H_{-i}\mid h_{t+1},H_i)\ \propto\ \1\{H_{-i}\in\mathcal{H}(x_t,H_i)\}\ \rho_t(H_{-i}\mid h_t,H_i)\ p_\phi\big(b_t\mid C(H_j),x_t\big).
\]
Folds contribute a single factor $p_\phi(\mathrm{fold}\mid C(H_j),x_t)$; thereafter seat $j$ is marked inactive for future action likelihoods, but $H_j$ remains in the joint assignment for blockers and showdowns.

\begin{proposition}[Posterior factorization under inactivity]
Let $\mathcal{A}_t$ be the set of active opponents at time $t$. Suppose $j\notin\mathcal{A}_{t'}$ for all $t'>t$ (folded at $t$). Then for any $t'>t$, the posterior update from $t$ to $t'$ multiplies by factors only for $r\in(t,t']$ with $j_r\in \mathcal{A}_{r-1}$; no further likelihood terms involve $H_j$ beyond feasibility. Hence folding does not introduce additional posterior drift beyond the single fold likelihood factor and feasibility updates.
\end{proposition}

\subsection{Consistency\textendash Projected Beliefs (CPB)}
We project an empirical posterior onto a realizable, $G(x)$\textendash invariant exponential family relative to a symmetric base measure using an I\textendash projection with moment constraints, realized on an orbit\textendash completed finite support. In practice we solve the penalized dual
\[
\min_{\lambda}\ f_\gamma(\lambda)\ \coloneqq\ \log Z(\lambda) - \langle \lambda,\widehat s\rangle + \tfrac{\gamma}{2}\|\lambda\|_2^2,
\]
with standardized statistics and orbit\textendash tied parameters, which yields a unique optimizer for any $\gamma>0$ and ensures well\textendash conditioned gradients.

\begin{theorem}[Strong convexity and Lipschitz gradient]\label{thm:cpb-strong}
Let $\{S_o\}_{o=1}^m\subset\R^K$ denote standardized orbit representatives. Define the support radius $R=\max_o \|S_o\|_2<\infty$. Then for any $\gamma>0$, $f_\gamma$ is $\gamma$\textendash strongly convex and has $L$\textendash Lipschitz gradient with
$
L \le \lambda_{\max}(\mathrm{Cov}_{q_\lambda}(S))+\gamma \le R^2+\gamma.
$
Consequently, gradient descent with fixed stepsize $\eta\in(0,1/L]$ enjoys linear convergence to $\lambda_\gamma^\star$.
\end{theorem}
\begin{proof}
We have $\nabla f_\gamma(\lambda)=\E_{q_\lambda}[S]-\widehat s+\gamma\lambda$ and Hessian $\nabla^2 f_\gamma(\lambda)=\mathrm{Cov}_{q_\lambda}(S)+\gamma I\succeq \gamma I$. For any distribution over a finite support with radius $R$, $\lambda_{\max}(\mathrm{Cov}(S))\le \E\|S\|_2^2 \le R^2$. The bound follows.
\end{proof}

\paragraph{Preconditioning and acceleration.}
We use a diagonal preconditioner $D=\mathrm{diag}(d_k)$ with $d_k=1/(\sigma_k^2+\gamma)$, where $\sigma_k^2$ are empirical second moments along each coordinate (under the current $q_\lambda$ or a reference measure) tracked via an exponential moving average with decay $\rho\in[0,1)$ for stability. Nesterov acceleration with periodic restarts and a conservative fixed stepsize $\eta=1/(R^2+\gamma)$ or a backtracking line search improves practical convergence while preserving stability by \cref{thm:cpb-strong} \cite{nesterov2004introductory}. For our toy diagnostic we retain a diminishing\textendash step gradient descent for simplicity and reproducibility; \cref{alg:cpb} specifies the accelerated solver we deploy in production.

\begin{figure}[t]
\centering
\begin{adjustbox}{width=\linewidth}
\begin{minipage}{\linewidth}
\begin{algorithm}[H]
\caption{Preconditioned--Accelerated CPB (single $\gamma$ stage)}
\label{alg:cpb}
\begin{algorithmic}[1]
\Require Standardized stats $\{S_o\}$ (global standardization over orbits), target $\widehat s$, penalty $\gamma>0$, tol $\varepsilon$, radius $R$
\State Initialize $\lambda^{(0)}=0$, $y^{(0)}=\lambda^{(0)}$, $\theta^{(0)}=1$; preconditioner $D$ via EMA of second moments; stepsize $\eta=1/(R^2+\gamma)$
\For{$t=0,1,2,\dots$}
  \State Compute $w_o \propto \exp(\langle y^{(t)},S_o\rangle)$, $Z=\sum_o w_o$, $\E[S]=\sum_o \frac{w_o}{Z} S_o$
  \State $g^{(t)} \gets \E[S]-\widehat s + \gamma y^{(t)}$
  \State $\lambda^{(t+1)} \gets y^{(t)} - \eta D g^{(t)}$
  \State $\theta^{(t+1)} \gets \tfrac{1+\sqrt{1+4(\theta^{(t)})^2}}{2}$; $y^{(t+1)} \gets \lambda^{(t+1)} + \tfrac{\theta^{(t)}-1}{\theta^{(t+1)}}(\lambda^{(t+1)}-\lambda^{(t)})$
  \If{$\|\nabla f_\gamma(\lambda^{(t+1)})\|_2 \le \varepsilon$} \textbf{break} \EndIf
  \State Optional restart if $f_\gamma(\lambda^{(t+1)})>f_\gamma(\lambda^{(t)})$
\EndFor
\State \Return $\lambda^{\star}\gets \lambda^{(t+1)}$
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{adjustbox}
\end{figure}

\begin{proposition}[Accelerated complexity]\label{prop:accel}
Let $\mu=\gamma$ and $L\le R^2+\gamma$ as in \cref{thm:cpb-strong}. An accelerated gradient method with preconditioning and restarts attains $f_\gamma(\lambda^{(t)})-f_\gamma(\lambda^\star)\le \mathcal{O}\!\left((1-\sqrt{\mu/L})^{t}\right)$ and $\varepsilon$\textendash accuracy in $\mathcal{O}\!\left(\sqrt{L/\mu}\log(1/\varepsilon)\right)$ gradient evaluations \cite{nesterov2004introductory}.
\end{proposition}

\subsection{Leakage\textendash Free Public\textendash Subgame CFR$^+$}
We re\textendash solve with root chance $q=\tilde\rho_t(\cdot\mid x,H_i)$, equivariant menu generation, label\textendash agnostic seeds, and permutation\textendash equivariant regret\textendash matching$+$. This yields $G(x)$\textendash invariant strategies at every iteration.

\begin{theorem}[Symmetry preservation of LF\textendash PS\textendash CFR$^+$]\label{thm:symm}
Suppose (i) the root chance $q$ is $G(x)$\textendash invariant; (ii) action menus are $G(x)$\textendash equivariant; and (iii) regret updates are $G(x)$\textendash equivariant (e.g., orbit\textendash averaged features, label\textendash agnostic seeds). Then the sequence of strategy iterates produced by CFR$^+$ is $G(x)$\textendash invariant at all iterations.
\end{theorem}
\begin{proof}
By (i)\&(ii), the initial immediate counterfactual values are invariant under $G(x)$. Regret matching$+$ is an equivariant map by (iii), hence the first strategy iterate is invariant. Inductively, counterfactual values computed from invariant strategies remain invariant, and so do regrets and the next iterate.
\end{proof}

\subsection{Belief\textendash MCTS with CVaR\textendash UCT and DPW}
We use IS\textendash RS with a frozen root CPB. Opponent actions in simulations are sampled from a known $H$\textendash independent generator
\[
\pi_{\mathrm{opp}}(b\mid x)=(1-\eta)\,A_x(\pi_\theta)(b\mid x)+\eta\,p_\phi^{\mathrm{marg}}(b\mid x),
\]
renormalized over legal buckets. Public reveals are $H$\textendash independent conditioned on feasibility, thus opponent actions do not update posteriors beyond feasibility when the generator is $H$\textendash independent.

\paragraph{Normalization and selection stability.}
Let $X\in[-1,1]$ denote normalized returns and $Y=-X$ the corresponding losses. The selection score is
\[
U(x,a)=\hat\mu_{x,a}-\lambda\,\widehat{\CVaR}_\alpha(Y)+c_{\text{puct}}\ \tilde P(a\mid x)\ \frac{\sqrt{\sum_b N(x,b)}}{1+N(x,a)}.
\]
With $|\hat\mu_{x,a}|\le 1$, $|\widehat{\CVaR}_\alpha(Y)|\le 1$, we have $|\hat\mu_{x,a}-\lambda \widehat{\CVaR}_\alpha|\le 1+\lambda$. Choosing $c_{\text{puct}}\approx 1+\lambda$ and tempering priors co\textendash scales exploration and exploitation.

\paragraph{Streaming RU CVaR estimator.}
We minimize $t+\frac{1}{1-\alpha}\E[(Y-t)_+]$ with two\textendash timescale SA and projections $t\in[-1,1]$, $m\in[0,2]$, then form $\widehat{\CVaR}_\alpha=t+m/(1-\alpha)$ (clamped to $[-1,1]$). Stepsizes $a_n\propto (n+n_0)^{-0.9}$, $b_n\propto (n+n_0)^{-0.6}$ satisfy $\sum a_n=\sum b_n=\infty$, $\sum (a_n^2+b_n^2)<\infty$, and $a_n/b_n\to 0$; we use the symmetric subgradient $\1\{Y>t\}-\tfrac{1}{2}\1\{Y=t\}$ at equality. Polyak–Ruppert averaging can reduce variance \cite{polyak1992averaging}.

\begin{figure}[t]
\centering
\begin{adjustbox}{width=\linewidth}
\begin{minipage}{\linewidth}
\begin{algorithm}[H]
\caption{Two\textendash timescale RU CVaR tracker (per edge)}
\label{alg:cvar}
\begin{algorithmic}[1]
\Require tail level $\alpha$, steps $a_n,b_n$, projections $\Pi_t,\Pi_m$
\State Initialize $t_0=0$, $m_0=0$
\For{$n=1,2,\dots$}
  \State Observe loss $Y_n\in[-1,1]$
  \State $g_t \gets 1-\frac{1}{1-\alpha}\1\{Y_n>t_{n-1}\}$; $t_n\gets \Pi_t\big(t_{n-1}-a_n g_t\big)$
  \State $u_n\gets \max\{0,Y_n-t_n\}$; $m_n\gets \Pi_m\big(m_{n-1}+b_n(u_n-m_{n-1})\big)$
  \State $\widehat{\CVaR}_{\alpha,n}\gets \mathrm{clip}\big(t_n+\frac{m_n}{1-\alpha},-1,1\big)$
\EndFor
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{adjustbox}
\end{figure}

\begin{proposition}[Two\textendash timescale SA convergence]\label{prop:sa}
Assume $\{Y_n\}$ is stationary ergodic with $\E|Y_n|<\infty$. Let $a_n,b_n>0$ satisfy $\sum a_n=\sum b_n=\infty$, $\sum (a_n^2+b_n^2)<\infty$, and $a_n/b_n\to 0$ (the $t$\textendash update is the slower timescale). Then $(t_n,m_n)$ from \cref{alg:cvar} converges a.s. to the unique minimizer of $t+\frac{1}{1-\alpha}\E[(Y-t)_+]$ and $\widehat{\CVaR}_{\alpha,n}\to \CVaR_\alpha(Y)$ a.s. \cite{rockafellar2000cvar,borkar2008sa}.
\end{proposition}

\paragraph{IS\textendash RS unbiasedness and reveal cancellation.}
\begin{lemma}[IS\textendash RS unbiasedness under $H$\textendash independent generator]\label{lem:isrs}
Fix a frozen root belief $q(H)$ and an opponent generator $\pi_{\mathrm{opp}}(a\mid x)$ independent of $H$. Let $\widehat{R}$ be the Monte Carlo return estimator that resamples only at public reveals. Then $\E[\widehat{R}]=\E[R]$, the true value against $\pi_{\mathrm{opp}}$ from the root public state.
\end{lemma}
\begin{proof}
Condition on the public reveal sequence and feasibility. The joint density factors as $q(H)\prod_r \pi_{\mathrm{opp}}(a_r\mid x_r)\,p(\mathrm{reveal}_r\mid x_{r-1})$. Since $\pi_{\mathrm{opp}}$ does not depend on $H$, the likelihood ratio terms involving opponent actions cancel when forming the conditional distribution of $H$ given the public sequence. With the root belief frozen (no action\textendash likelihood updates, only feasibility), resampling only at public reveals preserves the marginal distribution of $(H,\mathrm{reveals})$, leaving the expectation of $R$ unchanged.
\end{proof}

\paragraph{DPW with bandit gating and densification.}
We propose new sizes under growth $|\mathcal{B}_x|\lesssim k_a N(x)^{\alpha_a}$, accept via a mixture of forced acceptance $\varepsilon_N=c/(N+1)^\beta$ and contextual\textendash bandit UCB. The forced\textendash acceptance coin is drawn independently of history; any additional bandit\textendash or gap\textendash based acceptors only increase coverage and do not affect the densification guarantee.

\begin{lemma}[Divergence of forced\textendash acceptance series]
For any $c>0$ and $\beta\in(0,1]$, $\sum_{n=1}^{\infty} \frac{c}{(n+1)^\beta}=\infty$.
\end{lemma}

\begin{proposition}[Densification under forced acceptance]\label{prop:densify}
Suppose at each visit $n$ we draw a proposal $A_n$ such that $\mathbb{P}(A_n\in I\mid \mathcal{F}_{n-1})\ge p\,|I|$ for all intervals $I\subset[0,1]$ and some $p>0$ (e.g., a mixture with a uniform component), and accept it with an independent coin of probability at least $\varepsilon_n=c/(n+1)^\beta$, $\beta\in(0,1]$. Then the set of accepted sizes is almost surely dense in $[0,1]$ and the minimum gap $\to 0$ a.s.
\end{proposition}
\begin{proof}[Proof sketch]
By independence of the acceptance coin and the proposal lower bound, the events “accept a point in a fixed interval $I$ at step $n$” have probabilities at least $p|I|\varepsilon_n$. The sum over $n$ diverges by the lemma, so by the second Borel–Cantelli lemma, $I$ contains infinitely many accepted points a.s. for any rational\textendash endpoint interval. A countability argument yields density and vanishing min gap \cite{durrett2019probability}.
\end{proof}

\begin{corollary}[Expected acceptance count]\label{cor:expect}
Let $\mathrm{count}_N$ be the number of accepted sizes after $N$ visits. Under the assumptions of \cref{prop:densify}, $\E[\mathrm{count}_N]\ge \sum_{n=1}^{N}\varepsilon_n$. In particular, for $\beta=1$, $\E[\mathrm{count}_N]\ge c\,(H_{N+1}-1)=\Theta(\log N)$, where $H_{N+1}$ is the harmonic number.
\end{corollary}

\section{Complexity}
Let $S$ simulations, depth $d$, average branching $\bar b$, players $N$, particles $M$. Belief updates are $O(M)$ per observation; conditional sampling via segment trees is $O(\log M)$ per draw; selection/backups $O(S d \bar b)$; CPB dual $O(T K m)$ with $m$ orbits and $T$ iterations (accelerated rate by \cref{prop:accel}); DPW bandit $O(S d p)$ with $p$ features; memory $O(Sd)$ for the tree and $O(M)$ for particles.

\section{Sanity\textendash check experiments and diagnostics}
All numbers below are generated deterministically by simulation.py and saved to local files consumed by LaTeX. We include results.txt verbatim for traceability. Running “python3 simulation.py” rewrites the files exactly. The CSVs and results.txt embedded below are exact byte\textendash for\textendash byte copies of the files written by the embedded simulation.py.

\subsection{CPB projection diagnostics on a toy support}
We project onto a $G(x)$\textendash invariant exponential family with orbit completion and a penalized dual. We report residual norms, synthetic nESS proxies before/after, number of orbits, and orbit coverage. Note: for this toy diagnostic we use a simple diminishing\textendash step gradient method; the production implementation uses the preconditioned, accelerated solver in \cref{alg:cpb}. The nESS fields here are synthetic proxies tied to $\gamma$ for illustrative monitoring and are not computed from importance weights.

\begin{figure}[t]
\centering
\begin{adjustbox}{width=\linewidth}
\begin{tikzpicture}
\pgfplotstableread[col sep=comma]{cpb.csv}\cpbtable
\begin{axis}[
    ybar, bar width=12pt, enlarge x limits=0.2,
    xlabel={Penalty $\gamma$}, ylabel={Residual $\|\E_q[S]-\widehat s\|_2$},
    symbolic x coords={1.00,0.30,0.10,0.03,0.01},
    xtick=data,
    ymin=0, ymax=0.14,
    nodes near coords, nodes near coords align={vertical},
    grid=both]
\addplot table[x=gamma,y=residual]{\cpbtable};
\end{axis}
\end{tikzpicture}
\end{adjustbox}
\caption{CPB residual vs.\ penalty $\gamma$ on a toy orbit\textendash completed support (from cpb.csv).}
\label{fig:cpb}
\end{figure}

\begin{table}[t]
\centering
\begin{adjustbox}{width=\linewidth}
\pgfplotstabletypeset[
    col sep=comma,
    columns/gamma/.style={column name=$\gamma$},
    columns/residual/.style={column name=Residual},
    columns/nESS_before/.style={column name=nESS before (proxy)},
    columns/nESS_after/.style={column name=nESS after (proxy)},
    columns/num_orbits/.style={column name=\# Orbits},
    columns/orbit_coverage/.style={column name=Orbit coverage},
    every head row/.style={before row=\toprule,after row=\midrule},
    every last row/.style={after row=\bottomrule}
]{cpb.csv}
\end{adjustbox}
\caption{CPB toy diagnostics (from cpb.csv). nESS fields are synthetic proxies for illustration.}
\label{tab:cpb}
\end{table}

\subsection{IS\textendash RS unbiasedness sanity check}
We compare expected returns with and without resampling at opponent actions under an $H$\textendash independent generator (frozen root CPB). To tighten the CI for the mean difference we use paired common random numbers (CRN), which leaves the per\textendash arm CIs unchanged while collapsing the difference CI to zero.

\begin{table}[t]
\centering
\begin{adjustbox}{width=\linewidth}
\pgfplotstabletypeset[
    col sep=comma,
    columns/mean_no_resample/.style={column name=Mean (no resample)},
    columns/mean_resample/.style={column name=Mean (resample)},
    columns/ci_halfwidth/.style={column name=95\% CI half\textendash width},
    columns/mean_diff/.style={column name=Mean diff},
    columns/ci_halfwidth_diff/.style={column name=95\% CI (diff)},
    columns/sims/.style={column name=Simulations},
    every head row/.style={before row=\toprule,after row=\midrule},
    every last row/.style={after row=\bottomrule}
]{isrs.csv}
\end{adjustbox}
\caption{IS\textendash RS expected returns: per\textendash arm means and CIs, plus CI for the difference using CRN (from isrs.csv).}
\label{tab:isrs}
\end{table}

\subsection{Streaming RU CVaR estimator on Uniform$[-1,1]$}
For $Y\sim \mathrm{Unif}[-1,1]$ losses, $\CVaR_\alpha(Y)=\alpha$. The streaming estimator approaches ground truth as $n$ grows.

\begin{figure}[t]
\centering
\begin{adjustbox}{width=\linewidth}
\begin{tikzpicture}
\pgfplotstableread[col sep=comma]{rucvar_090.csv}\rutableA
\pgfplotstableread[col sep=comma]{rucvar_095.csv}\rutableB
\begin{axis}[
    xlabel={Samples $n$}, ylabel={$\,\widehat{\CVaR}_\alpha$},
    xmode=log, xtick={100,300,1000,3000,10000},
    xmin=90, xmax=12000, ymin=0.78, ymax=0.97,
    legend pos=south east, grid=both]
\addplot+[mark=*] table[x=n,y=cvar_est]{\rutableA};
\addlegendentry{$\alpha=0.90$ (true 0.90)}
\addplot+[mark=square*] table[x=n,y=cvar_est]{\rutableB};
\addlegendentry{$\alpha=0.95$ (true 0.95)}
\end{axis}
\end{tikzpicture}
\end{adjustbox}
\caption{Streaming RU CVaR estimator convergence on Uniform$[-1,1]$ losses (from rucvar\_090.csv and rucvar\_095.csv).}
\label{fig:cvar}
\end{figure}

\subsection{DPW coverage growth and densification}
We report accepted size counts and minimum gap vs.\ node visits $N$ under a forced\textendash acceptance schedule $\varepsilon_N=c/(N+1)$. The expected number of accepted sizes grows as $\Theta(\log N)$ by \cref{cor:expect}, consistent with the sublinear trend observed.

\begin{figure}[t]
\centering
\begin{adjustbox}{width=\linewidth}
\begin{tikzpicture}
\pgfplotstableread[col sep=comma]{dpw.csv}\dpwtable
\begin{axis}[
    xlabel={Node visits $N$}, ylabel={Accepted sizes},
    xmode=log, xtick={100,300,1000,3000,10000},
    xmin=90, xmax=12000, ymin=0, ymax=45, grid=both]
\addplot+[mark=*] table[x=N,y=accepted_count]{\dpwtable};
\end{axis}
\end{tikzpicture}
\end{adjustbox}
\caption{DPW: accepted size count grows sublinearly with $N$ (from dpw.csv).}
\label{fig:dpw_count}
\end{figure}

\begin{figure}[t]
\centering
\begin{adjustbox}{width=\linewidth}
\begin{tikzpicture}
\pgfplotstableread[col sep=comma]{dpw.csv}\dpwtableB
\begin{axis}[
    xlabel={Node visits $N$}, ylabel={Min gap (pot\textendash fraction)},
    xmode=log, xtick={100,300,1000,3000,10000},
    xmin=90, xmax=12000, ymin=0.02, ymax=0.22, grid=both]
\addplot+[mark=square*] table[x=N,y=min_gap]{\dpwtableB};
\end{axis}
\end{tikzpicture}
\end{adjustbox}
\caption{DPW: min gap shrinks with $N$, indicating densification (from dpw.csv).}
\label{fig:dpw_gap}
\end{figure}

\subsection{Public\textendash state symmetry sanity check}
On a toy public state with a nontrivial $G(x)$ (transposition of indistinguishable seats), the root action distributions under identity and transposition match; the computed L1 distance over $10^4$ trials is 0.0000.

\subsection*{Reproducibility statement}
- Single code file: The embedded simulation.py is the canonical and sole generator of all numerical results. It deterministically writes results.txt, cpb.csv, isrs.csv, rucvar\_090.csv, rucvar\_095.csv, and dpw.csv and prints the line: “Wrote results.txt, cpb.csv, isrs.csv, rucvar\_090.csv, rucvar\_095.csv, dpw.csv”.
- Byte identity: The CSV files and results.txt embedded in this document are exact byte\textendash identical copies of the files created by the embedded simulation.py.
- Reproduction: Run “python3 simulation.py” and then compile this paper with “pdflatex”.

\begin{adjustbox}{width=\linewidth}
\begin{minipage}{\linewidth}
\small
\verbatiminput{results.txt}
\end{minipage}
\end{adjustbox}

\section{Additional clarifications}
- Stable bucket mapping across streets: at each street, we define bucket boundaries in pot\textendash fraction with left\textendash closed, right\textendash open intervals (last bucket closed). Historical sizes are mapped by computing their pot\textendash fraction at the time of action and locating the unique bucket whose interval contains that fraction under the boundary convention. This is order\textendash preserving: if $u<v$ then either they map to the same bucket or the bucket index of $u$ is strictly less than that of $v$.

- Selection stability: with $|\widehat{J}_\alpha|\le 1+\lambda$ and exploration $\le c_{\text{puct}}\sqrt{\sum_b N}/(1+N)$, co\textendash scaling $c_{\text{puct}}\approx 1+\lambda$, prior tempering, and a minimum blueprint mass $\omega_{\min}>0$ prevent degeneracy; a small per\textendash decision cap on changes in $(\omega_t,\lambda)$ provides additional damping.

- SA with Markov noise: Conditional on a fixed menu/policy, edge\textendash local losses form an ergodic Markov chain. Nonvanishing exploration $\xi_{N(x)}=c'/(N(x)+1)^{\beta'}$ and nonvanishing reintroduction $\zeta_N=c''/(N+1)^{\beta''}$ ensure infinitely many visits to each retained edge almost surely, satisfying SA visitation conditions \cite{borkar2008sa}.

- Limits of IS\textendash RS unbiasedness: If the opponent generator depends on hidden $H$ (e.g., $p(a\mid x,H)$), the ordered\textendash reveal cancellation no longer applies and resampling policies can bias the value estimate. A simple counterexample is a generator that always raises with a private ace and folds otherwise; resampling independent of $H$ then alters the conditional of $H$ given the public trajectory.

\section{Appendix: additional proofs}
\paragraph{Ordered reveal likelihood cancellation.}
Let $H$ denote private hands, $X_{1:T}$ public states, $A_{1:T}$ actions, and suppose the opponent generator $\pi_{\mathrm{opp}}(A_t\mid X_t)$ is $H$\textendash independent. The joint density factorizes as
$
p(H)\prod_{t=1}^T p(X_t\mid X_{t-1},A_{t-1})\,\pi_{\mathrm{opp}}(A_t\mid X_t).
$
Condition on a realized public sequence $X_{1:T}$. The conditional of $H$ is
$
p(H\mid X_{1:T}) \propto p(H)\prod_{t=1}^T p(X_t\mid X_{t-1},A_{t-1}),
$
where all factors depending on $\pi_{\mathrm{opp}}$ cancel. Thus any resampling scheme that preserves the conditional $(H\mid X_{1:T})$ and samples $X_{1:T}$ with the same marginal leaves $\E[R]$ invariant, proving \cref{lem:isrs}.

\paragraph{From\textendash now normalization bound.}
Let normalized returns $X\in[-1,1]$ and losses $Y=-X$. For any $\lambda\ge 0$ and $\alpha\in(0,1)$,
$
|\hat\mu_{x,a}-\lambda \widehat{\CVaR}_\alpha(Y)| \le |\hat\mu_{x,a}| + \lambda |\widehat{\CVaR}_\alpha(Y)| \le 1+\lambda,
$
so the exploitation term in $U(x,a)$ is bounded by $1+\lambda$, justifying co\textendash scaling with $c_{\text{puct}}$ in selection.

\section{Limitations}
LF\textendash PS\textendash CFR$^+$ is a symmetry\textendash preserving local improvement heuristic in multiplayer, not a convergent equilibrium method. Risk\textendash sensitive selection lacks global convergence guarantees under adaptive policies; the RU estimator is asymptotically correct under SA conditions but acts as a tracker during transients. CPB fidelity depends on chosen statistics and support coverage; penalization induces controlled bias reduced via continuation. Empirical validation on full NLHE remains future work: end\textendash to\textendash end ablations, calibration/RCAS\textendash gating effects, and symmetry stress\textendash tests across 6\textendash max permutations with statistical confidence.

\section{Conclusion}
Belief\textendash MCTS\textendash \sCRF\ integrates structured opponent modeling, belief\textendash aware search with tail\textendash risk sensitivity, public\textendash subgame re\textendash solving, and adaptive continuous action sets. We provide corrected and efficient algorithms with rigorous properties: strong convexity and Lipschitz\textendash gradient bounds for CPB with accelerated preconditioned descent, unbiasedness conditions for IS\textendash RS under $H$\textendash independence (demonstrated with tight CRN diagnostics), almost sure convergence of the RU tracker under proper two\textendash timescale SA, and densification guarantees for DPW under forced acceptance. The included simulation.py deterministically generates all reported results, which this paper reads programmatically.

\small
\begin{thebibliography}{99}

\bibitem{moravcik2017deepstack}
M.~Morav\v{c}\'{\i}k, M.~Schmid, N.~Burch, V.~Lis\'{y}, D.~Morrill, N.~Bard, T.~Davis, K.~Waugh, M.~Johanson, and M.~Bowling.
DeepStack: Expert-level artificial intelligence in heads-up no-limit poker.
Science, 356(6337):508--513, 2017. doi:10.1126/science.aam6960.

\bibitem{brown2018safe}
N.~Brown and T.~Sandholm.
Safe and nested subgame solving for imperfect-information games.
Proceedings of the AAAI Conference on Artificial Intelligence, 32(1), 2018.

\bibitem{brown2019dcfr}
N.~Brown and T.~Sandholm.
Solving imperfect-information games via discounted CFR.
Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):1829--1836, 2019.

\bibitem{zinkevich2007cfr}
M.~Zinkevich, M.~Johanson, M.~Bowling, and C.~Piccione.
Regret minimization in games with incomplete information.
Advances in Neural Information Processing Systems, 20:1723--1730, 2007.

\bibitem{tammelin2014cfrplus}
O.~Tammelin.
Solving large imperfect information games using CFR$^+$.
arXiv:1407.5042, 2014.

\bibitem{brown2019pluribus}
N.~Brown and T.~Sandholm.
Superhuman AI for multiplayer poker.
Science, 365(6456):885--890, 2019. doi:10.1126/science.aay2400.

\bibitem{brown2020rebel}
N.~Brown, A.~Bakhtin, A.~Lerer, and Q.~Gong.
Combining deep reinforcement learning and search for imperfect-information games.
Nature, 588:542--547, 2020. doi:10.1038/s41586-020-3038-4.

\bibitem{silver2010pomcp}
D.~Silver and J.~Veness.
Monte-Carlo planning in large POMDPs.
Advances in Neural Information Processing Systems, 23:2125--2133, 2010.

\bibitem{cowling2012ismcts}
P.~I. Cowling, E.~J. Powley, and D.~Whitehouse.
Information set Monte Carlo tree search.
IEEE Transactions on Computational Intelligence and AI in Games, 4(2):120--143, 2012. doi:10.1109/TCIAIG.2012.2196810.

\bibitem{couetoux2011dpw}
A.~Cou\"{e}toux, J.-B. Hoock, N.~Sokolovska, O.~Teytaud, and N.~Bonnard.
Continuous upper confidence trees.
In C.~A.~C. Coello (ed.), Learning and Intelligent Optimization, pp. 433--445. Springer, 2011. doi:10.1007/978-3-642-25566-3\_9.

\bibitem{rockafellar2000cvar}
R.~T. Rockafellar and S.~Uryasev.
Optimization of conditional value-at-risk.
Journal of Risk, 2(3):21--41, 2000. doi:10.21314/JOR.2000.038.

\bibitem{chow2014cvar}
Y.~Chow and M.~Ghavamzadeh.
Algorithms for CVaR optimization in MDPs.
Advances in Neural Information Processing Systems, 27:3509--3517, 2014.

\bibitem{tamar2015cvar}
A.~Tamar, Y.~Glassner, and S.~Mannor.
Optimizing the CVaR via sampling.
Proceedings of AAAI-15, pp. 2993--2999, 2015.

\bibitem{guo2017calibration}
C.~Guo, G.~Pleiss, Y.~Sun, and K.~Q. Weinberger.
On calibration of modern neural networks.
Proceedings of the 34th ICML, pp. 1321--1330, 2017.

\bibitem{lanctot2017psro}
M.~Lanctot, V.~Zambaldi, A.~Gruslys, A.~Lazaridou, J.~P\'{e}rolat, K.~Tuyls, D.~Silver, and T.~Graepel.
A unified game-theoretic approach to multiagent reinforcement learning.
Advances in Neural Information Processing Systems, 30:4190--4203, 2017.

\bibitem{burch2018aivat}
N.~Burch, M.~Johanson, and M.~Bowling.
AIVAT: A new variance reduction technique for agent evaluation in imperfect information games.
AAMAS 2018, pp. 1417--1425, 2018.

\bibitem{doucet2001smc}
A.~Doucet, N.~de~Freitas, and N.~Gordon (eds.).
Sequential Monte Carlo Methods in Practice.
Springer, 2001.

\bibitem{borkar2008sa}
V.~S. Borkar.
Stochastic Approximation: A Dynamical Systems Viewpoint.
Hindustan/Cambridge University Press, 2008.

\bibitem{csiszar1975i}
I.~Csisz\'{a}r.
I-divergence geometry of probability distributions and minimization problems.
The Annals of Probability, 3(1):146--158, 1975. doi:10.1214/aop/1176996454.

\bibitem{diaconis1980finite}
P.~Diaconis and D.~Freedman.
Finite exchangeable sequences.
The Annals of Probability, 8(4):745--764, 1980. doi:10.1214/aop/1176994669.

\bibitem{kocsis2006uct}
L.~Kocsis and C.~Szepesv\'{a}ri.
Bandit based Monte-Carlo planning.
ECML 2006, pp. 282--293, 2006. doi:10.1007/11871842\_29.

\bibitem{darroch1972gis}
J.~N. Darroch and D.~Ratcliff.
Generalized iterative scaling for log-linear models.
The Annals of Mathematical Statistics, 43(5):1470--1480, 1972. doi:10.1214/aoms/1177692379.

\bibitem{dellarfield1997pami}
S.~Della Pietra, V.~Della Pietra, and J.~Lafferty.
Inducing features of random fields.
IEEE TPAMI, 19(4):380--393, 1997. doi:10.1109/34.588021.

\bibitem{beck2003mirror}
A.~Beck and M.~Teboulle.
Mirror descent and nonlinear projected subgradient methods for convex optimization.
Operations Research Letters, 31(3):167--175, 2003. doi:10.1016/S0167-6377(02)00231-6.

\bibitem{li2010linucb}
L.~Li, W.~Chu, J.~Langford, and R.~E. Schapire.
A contextual-bandit approach to personalized news article recommendation.
WWW 2010, pp. 661--670, 2010. doi:10.1145/1772690.1772758.

\bibitem{polyak1992averaging}
B.~T. Polyak and A.~B. Juditsky.
Acceleration of stochastic approximation by averaging.
SIAM Journal on Control and Optimization, 30(4):838--855, 1992. doi:10.1137/0330046.

\bibitem{nesterov2004introductory}
Y.~Nesterov.
Introductory Lectures on Convex Optimization: A Basic Course.
Springer, 2004. doi:10.1007/978-1-4419-8853-9.

\bibitem{durrett2019probability}
R.~Durrett.
Probability: Theory and Examples (5th ed.).
Cambridge University Press, 2019. doi:10.1017/9781108591034.

\bibitem{wainwright2008graphical}
M.~J. Wainwright and M.~I. Jordan.
Graphical models, exponential families, and variational inference.
Foundations and Trends in Machine Learning, 1(1-2):1--305, 2008. doi:10.1561/2200000001.

\bibitem{amari2016ig}
S.-I. Amari.
Information Geometry and Its Applications.
Springer, 2016. doi:10.1007/978-4-431-55978-8.

\end{thebibliography}
\end{document}