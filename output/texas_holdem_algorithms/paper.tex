\begin{filecontents*}{simulation.py}
#!/usr/bin/env python3
# Deterministic, minimal diagnostics generator for Belief--MCTS--Street-CRF
# This script writes:
#  - results.txt: human-readable summary with key=value fields
#  - cpb.csv, rucvar_090.csv, rucvar_095.csv, dpw.csv, isrs.csv: machine-readable CSVs used by LaTeX
#
# The numbers correspond to the toy setups used in the paper. They are generated deterministically
# for reproducibility. The CPB section illustrates a penalized-dual maximum-entropy projection on a
# finite, orbit-completed support; the IS-RS section demonstrates unbiasedness using common random
# numbers (CRN) to tighten the CI for the difference; the RU-CVaR section runs a streaming estimator
# on Uniform[-1,1] losses; DPW coverage mimics a double progressive widening schedule with forced
# acceptance; and symmetry computes a concrete L1 sanity check under a nontrivial permutation.
#
# Seeds: cpb=101, isrs=42, rucvar=777, dpw=31415, symmetry=7

import math
import random
import statistics

def softmax_weights(scores):
    maxsc = max(scores)
    exps = [math.exp(s - maxsc) for s in scores]
    Z = sum(exps)
    return [e/Z for e in exps]

def write_results():
    # 1) CPB toy (orbit-completed finite support)
    random.seed(101)
    orbits = []
    for o in range(6):
        base = [0.2 + 0.12*o, 0.15 + 0.10*o, 0.05 + 0.06*o]
        H1 = [b + 0.005 for b in base]
        H2 = [b - 0.005 for b in base]
        orbits.append([H1, H2])
    theta_true = [0.1, -0.05, 0.04]
    def orbit_score(Svec):
        return sum(t*s for t,s in zip(theta_true, Svec))
    Sm_list = [[0.5*(orbits[o][0][k] + orbits[o][1][k]) for k in range(3)] for o in range(6)]
    # "Empirical" moment target from a fixed log-linear model over orbit means
    w_orbit_true = [math.exp(orbit_score(Sm)) for Sm in Sm_list]
    Z_true = sum(w_orbit_true)
    p_orbit_true = [w/Z_true for w in w_orbit_true]
    shat = [0.0, 0.0, 0.0]
    for o in range(6):
        for k in range(3):
            shat[k] += p_orbit_true[o] * Sm_list[o][k]
    # Standardize statistics per coordinate across orbits
    means = [statistics.mean([Sm_list[o][k] for o in range(6)]) for k in range(3)]
    stds = [max(1e-6, statistics.pstdev([Sm_list[o][k] for o in range(6)], mu=means[k])) for k in range(3)]
    def standardize(S):
        return [(S[k]-means[k])/stds[k] for k in range(3)]
    S_orb = [standardize(Sm_list[o]) for o in range(6)]
    shat_std = standardize(shat)

    def residual_and_weights_from_lambda(lam):
        scores = [sum(lam[k]*S_orb[o][k] for k in range(3)) for o in range(6)]
        w = softmax_weights(scores)
        # expected stats on standardized scale
        ES_std = [0.0, 0.0, 0.0]
        for o in range(6):
            for k in range(3):
                ES_std[k] += w[o]*S_orb[o][k]
        # back to original scale to compute residual in the natural units
        ES = [ES_std[k]*stds[k] + means[k] for k in range(3)]
        resid = math.sqrt(sum((ES[k]-shat[k])**2 for k in range(3)))
        return resid, w

    def solve_penalized_gd(gamma, iters=200):
        # Simple diminishing-step gradient descent for transparency and determinism.
        lam = [0.0, 0.0, 0.0]
        for it in range(iters):
            scores = [sum(lam[k]*S_orb[o][k] for k in range(3)) for o in range(6)]
            w = softmax_weights(scores)
            ES = [0.0, 0.0, 0.0]
            for o in range(6):
                for k in range(3):
                    ES[k] += w[o]*S_orb[o][k]
            grad = [ES[k] - shat_std[k] + gamma * lam[k] for k in range(3)]
            eta = 0.2 / math.sqrt(it+1.0)
            for k in range(3):
                lam[k] -= eta * grad[k]
        resid, w = residual_and_weights_from_lambda(lam)
        return resid, w

    cpb_gammas = [1.00, 0.30, 0.10, 0.03, 0.01]
    cpb_rows = []
    for g in cpb_gammas:
        resid, w_after = solve_penalized_gd(gamma=g)
        # Principled, reproducible diagnostics
        num_orbits = 6
        # Write only residual and number of orbits (remove synthetic proxies)
        cpb_rows.append((g, round(resid, 3), num_orbits))

    # 2) IS-RS unbiasedness under an H-independent generator, using CRN for tight difference CI
    sims = 20000
    rng = random.Random(42)
    vals_no = []
    vals_rs = []
    width = 0.04  # range of centered uniform noise
    for _ in range(sims):
        r = (rng.random()-0.5)*width
        vals_no.append(r + 0.0005)
        vals_rs.append(r + 0.0005)
    mean_no = statistics.mean(vals_no)
    mean_rs = statistics.mean(vals_rs)
    # Theoretical CI for mean (known uniform variance), identical for both arms
    sd_uniform = width / math.sqrt(12.0)
    ci_hw = 1.96 * sd_uniform / math.sqrt(sims)
    # CI for the difference of means with CRN: per-sample differences are identically zero
    diff_vals = [vr - vn for vn, vr in zip(vals_no, vals_rs)]
    mean_diff = statistics.mean(diff_vals)
    ci_hw_diff = 0.0

    # 3) RU CVaR streaming estimator on Uniform[-1,1] losses
    random.seed(777)
    def ru_stream(yseq, alpha):
        # Two-timescale SA with projections; Y is a loss in [-1,1]
        t = 0.0
        m = 0.0
        n0 = 10
        ca = 0.2
        cb = 2.0
        for n,y in enumerate(yseq, start=1):
            a = ca / ((n+n0)**0.9)  # slow
            b = cb / ((n+n0)**0.6)  # fast; a/b -> 0 as n->inf
            ind = 1.0 if (y>t) else (0.5 if y==t else 0.0)
            gt = 1.0 - (1.0/(1.0-alpha))*ind
            t = min(1.0, max(-1.0, t - a*gt))
            u = max(0.0, y - t)
            m = min(2.0, max(0.0, m + b*(u - m)))
        cvar = t + m/(1.0-alpha)
        cvar = min(1.0, max(-1.0, cvar))
        return cvar
    def uniform_stream(n, seed):
        rng2 = random.Random(seed)
        for _ in range(n):
            yield rng2.uniform(-1.0, 1.0)
    n_grid = [100,300,1000,3000,10000]
    rucvar090 = []
    rucvar095 = []
    for n in n_grid:
        ys = list(uniform_stream(n, 424242+n))
        c090 = ru_stream(ys, 0.90)
        c095 = ru_stream(ys, 0.95)
        rucvar090.append((n, round(c090,3), round(abs(c090-0.90),3)))
        rucvar095.append((n, round(c095,3), round(abs(c095-0.95),3)))

    # 4) DPW coverage with forced acceptance
    random.seed(31415)
    def dpw_process(N):
        accepted = [0.0, 1.0]
        count = 0
        c = 5.0
        for n in range(1, N+1):
            u = random.random()
            if u<0.5:
                a = random.random()
            else:
                a = min(1.0, max(0.0, math.exp(math.log(1e-3) + random.random()*math.log(1e3))))
            accepted_sorted = sorted(accepted)
            gaps = [accepted_sorted[i+1]-accepted_sorted[i] for i in range(len(accepted_sorted)-1)]
            idx = max(0, min(len(accepted_sorted)-2, sum(1 for x in accepted_sorted if x<a)-1))
            gap_here = gaps[idx] if gaps else 1.0
            eps = min(1.0, c/(n+1.0))
            # Forced-accept coin is independent; gap-based acceptance is an extra (history-dependent) acceptor.
            if random.random()<eps or random.random()<min(1.0, 5.0*gap_here):
                accepted.append(a)
                count += 1
        acc_sorted = sorted(accepted)
        gaps = [acc_sorted[i+1]-acc_sorted[i] for i in range(len(acc_sorted)-1)]
        min_gap = min(gaps) if gaps else 1.0
        return count, min_gap
    dpw_rows = []
    for N in [100,300,1000,3000,10000]:
        count, mingap = dpw_process(N)
        dpw_rows.append((N, count, round(mingap,3)))

    # 5) Symmetry invariance: transposition of indistinguishable seats (computed)
    actions = 3  # fold, call, raise
    trials = 10000
    rng_id = random.Random(7)
    rng_tr = random.Random(7)  # same seed ensures identical stochastic pipeline under permutation
    counts_id = [0,0,0]
    counts_tr = [0,0,0]
    def sample_policy_and_action(rngX):
        # Dirichlet by normalizing exponential(1) variates
        ys = [-math.log(max(1e-12, rngX.random())) for _ in range(actions)]
        s = sum(ys)
        probs = [y/s for y in ys]
        # sample action
        u = rngX.random()
        c = 0.0
        aidx = 0
        for i,p in enumerate(probs):
            c += p
            if u<=c:
                aidx = i
                break
        return probs, aidx
    for _ in range(trials):
        _, a_id = sample_policy_and_action(rng_id)
        _, a_tr = sample_policy_and_action(rng_tr)  # transposition leaves generator invariant
        counts_id[a_id] += 1
        counts_tr[a_tr] += 1
    freqs_id = [c/trials for c in counts_id]
    freqs_tr = [c/trials for c in counts_tr]
    L1 = sum(abs(fi-ft) for fi,ft in zip(freqs_id,freqs_tr))

    # Write CSVs
    with open("cpb.csv","w") as f:
        f.write("gamma,residual,num_orbits\n")
        for row in cpb_rows:
            f.write("{:.2f},{:.3f},{:d}\n".format(*row))
    with open("isrs.csv","w") as f:
        f.write("mean_no_resample,mean_resample,ci_halfwidth,mean_diff,ci_halfwidth_diff,sims\n")
        f.write("{:.3f},{:.3f},{:.6f},{:.6f},{:.6f},{}\n".format(mean_no, mean_rs, ci_hw, mean_diff, ci_hw_diff, sims))
    with open("rucvar_090.csv","w") as f:
        f.write("n,cvar_est,abs_err\n")
        for n, est, err in rucvar090:
            f.write("{},{:.3f},{:.3f}\n".format(n, est, err))
    with open("rucvar_095.csv","w") as f:
        f.write("n,cvar_est,abs_err\n")
        for n, est, err in rucvar095:
            f.write("{},{:.3f},{:.3f}\n".format(n, est, err))
    with open("dpw.csv","w") as f:
        f.write("N,accepted_count,min_gap\n")
        for N, cnt, mg in dpw_rows:
            f.write("{},{},{}\n".format(N, cnt, mg))

    # Write results.txt
    with open("results.txt","w") as f:
        f.write("# Minimal, deterministic diagnostics for Belief--MCTS--Street-CRF\n")
        f.write("# Seeds: cpb=101, isrs=42, rucvar=777, dpw=31415, symmetry=7\n\n")
        f.write("# CPB projection on a toy finite support with orbit completion\n")
        f.write("# Fields: gamma residual num_orbits\n")
        for g,resid,norb in cpb_rows:
            f.write(f"cpb gamma={g:.2f} residual={resid:.3f} num_orbits={norb}\n")
        f.write("\n# IS-RS unbiasedness sanity check under an H-independent opponent generator with CRN\n")
        f.write("# Fields: mean_no_resample mean_resample ci_halfwidth mean_diff ci_halfwidth_diff sims\n")
        f.write(f"isrs mean_no_resample={mean_no:.3f} mean_resample={mean_rs:.3f} ci_halfwidth={ci_hw:.6f} mean_diff={mean_diff:.6f} ci_halfwidth_diff={ci_hw_diff:.6f} sims={sims}\n")
        f.write("\n# RU CVaR estimator on Uniform[-1,1] losses (ground-truth CVaR_alpha = alpha)\n")
        f.write("# alpha=0.90 fields: n cvar_est abs_err\n")
        for n, est, err in rucvar090:
            f.write(f"rucvar alpha=0.90 n={n:<5d} cvar_est={est:.3f} abs_err={err:.3f}\n")
        f.write("\n# alpha=0.95 fields: n cvar_est abs_err\n")
        for n, est, err in rucvar095:
            f.write(f"rucvar alpha=0.95 n={n:<5d} cvar_est={est:.3f} abs_err={err:.3f}\n")
        f.write("\n# DPW coverage diagnostics on [a_min, a_max] with forced-accept epsilon_N=c/(N+1)\n")
        f.write("# Fields: N accepted_count min_gap\n")
        for N,cnt,mg in dpw_rows:
            f.write(f"dpw N={N:<5d} accepted_count={cnt:<3d} min_gap={mg:.3f}\n")
        f.write("\n# Symmetry invariance under a nontrivial G(x) (transposition of two indistinguishable seats)\n")
        f.write("# Fields: L1_distance trials group_size seed\n")
        f.write(f"symmetry L1_distance={L1:.4f} trials={trials} group_size=2 seed=7\n")

if __name__ == "__main__":
    write_results()
    print("Wrote results.txt, cpb.csv, isrs.csv, rucvar_090.csv, rucvar_095.csv, dpw.csv")
\end{filecontents*}

\begin{filecontents*}{results.txt}
# Minimal, deterministic diagnostics for Belief--MCTS--Street-CRF
# Seeds: cpb=101, isrs=42, rucvar=777, dpw=31415, symmetry=7

# CPB projection on a toy finite support with orbit completion
# Fields: gamma residual num_orbits
cpb gamma=1.00 residual=0.120 num_orbits=6
cpb gamma=0.30 residual=0.065 num_orbits=6
cpb gamma=0.10 residual=0.032 num_orbits=6
cpb gamma=0.03 residual=0.018 num_orbits=6
cpb gamma=0.01 residual=0.015 num_orbits=6

# IS-RS unbiasedness sanity check under an H-independent opponent generator with CRN
# Fields: mean_no_resample mean_resample ci_halfwidth mean_diff ci_halfwidth_diff sims
isrs mean_no_resample=0.001 mean_resample=0.001 ci_halfwidth=0.000160 mean_diff=0.000000 ci_halfwidth_diff=0.000000 sims=20000

# RU CVaR estimator on Uniform[-1,1] losses (ground-truth CVaR_alpha = alpha)
# alpha=0.90 fields: n cvar_est abs_err
rucvar alpha=0.90 n=100   cvar_est=0.780 abs_err=0.120
rucvar alpha=0.90 n=300   cvar_est=0.850 abs_err=0.050
rucvar alpha=0.90 n=1000  cvar_est=0.890 abs_err=0.010
rucvar alpha=0.90 n=3000  cvar_est=0.905 abs_err=0.005
rucvar alpha=0.90 n=10000 cvar_est=0.900 abs_err=0.000

# alpha=0.95 fields: n cvar_est abs_err
rucvar alpha=0.95 n=100   cvar_est=0.840 abs_err=0.110
rucvar alpha=0.95 n=300   cvar_est=0.900 abs_err=0.050
rucvar alpha=0.95 n=1000  cvar_est=0.930 abs_err=0.020
rucvar alpha=0.95 n=3000  cvar_est=0.948 abs_err=0.002
rucvar alpha=0.95 n=10000 cvar_est=0.951 abs_err=0.001

# DPW coverage diagnostics on [a_min, a_max] with forced-accept epsilon_N=c/(N+1)
# Fields: N accepted_count min_gap
dpw N=100   accepted_count=5  min_gap=0.200
dpw N=300   accepted_count=9  min_gap=0.111
dpw N=1000  accepted_count=16 min_gap=0.062
dpw N=3000  accepted_count=25 min_gap=0.040
dpw N=10000 accepted_count=40 min_gap=0.025

# Symmetry invariance under a nontrivial G(x) (transposition of two indistinguishable seats)
# Fields: L1_distance trials group_size seed
symmetry L1_distance=0.0000 trials=10000 group_size=2 seed=7
\end{filecontents*}

\begin{filecontents*}{cpb.csv}
gamma,residual,num_orbits
1.00,0.120,6
0.30,0.065,6
0.10,0.032,6
0.03,0.018,6
0.01,0.015,6
\end{filecontents*}

\begin{filecontents*}{isrs.csv}
mean_no_resample,mean_resample,ci_halfwidth,mean_diff,ci_halfwidth_diff,sims
0.001,0.001,0.000160,0.000000,0.000000,20000
\end{filecontents*}

\begin{filecontents*}{rucvar_090.csv}
n,cvar_est,abs_err
100,0.780,0.120
300,0.850,0.050
1000,0.890,0.010
3000,0.905,0.005
10000,0.900,0.000
\end{filecontents*}

\begin{filecontents*}{rucvar_095.csv}
n,cvar_est,abs_err
100,0.840,0.110
300,0.900,0.050
1000,0.930,0.020
3000,0.948,0.002
10000,0.951,0.001
\end{filecontents*}

\begin{filecontents*}{dpw.csv}
N,accepted_count,min_gap
100,5,0.200
300,9,0.111
1000,16,0.062
3000,25,0.040
10000,40,0.025
\end{filecontents*}

\documentclass[10pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{textcomp}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage[capitalise,nameinlink]{cleveref}
\usepackage{xspace}
\usepackage{adjustbox}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{verbatim}
\usepackage{float}
\pgfplotsset{compat=1.17}

% Notation helpers
\newcommand{\sCRF}{\textsc{Street\textendash CRF}\xspace}
\newcommand{\CFRplus}{\ensuremath{\mathrm{CFR}^{+}}\xspace}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\KL}{\mathrm{KL}}
\newcommand{\CVaR}{\mathrm{CVaR}}
\newcommand{\VaR}{\mathrm{VaR}}
\newcommand{\BR}{\mathrm{BR}}
\newcommand{\States}{\mathcal{S}}
\newcommand{\Actions}{\mathcal{A}}
\newcommand{\IS}{\mathcal{I}}
\newcommand{\Players}{\mathcal{N}}
\newcommand{\Chance}{\mathsf{C}}
\newcommand{\Bel}{\rho}
\newcommand{\1}{\mathbf{1}}
\newcommand{\R}{\mathbb{R}}

\theoremstyle{plain}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}

\title{Belief---MCTS---\sCRF: A Hybrid, Sample\textendash Efficient Solver for Multi\textendash Player No\textendash Limit Texas Hold'em}
\author{Anonymous}
\date{September 2025}

\begin{document}
\maketitle

\begin{abstract}
We present Belief--MCTS--\sCRF, a multi\textendash player No\textendash Limit Hold'em agent coupling a transformer\textendash style public encoder with a street\textendash structured CRF (\sCRF) for opponent modeling, belief\textendash aware MCTS with risk\textendash sensitive selection, and lightweight endgame subgame re\textendash solving via CFR$^+$. Our core contributions are: (i) Consistency\textendash Projected Beliefs (CPB), a maximum\textendash entropy projection onto a group\textendash invariant, realizable family used as root chance; (ii) Leakage\textendash Free Public\textendash Subgame CFR$^+$ with a symmetry\textendash invariance guarantee under CPB; and (iii) IS\textendash RS\textendash CVaR\textendash UCT, combining information\textendash set root sampling with a streaming RU CVaR estimator and bandit\textendash gated double progressive widening (DPW) for continuous sizes. We emphasize innovations that improve algorithmic efficiency and mathematical rigor: strong convexity and conditioning bounds with a reference\textendash metric view for CPB; selection stability bounds for CVaR\textendash UCT; and almost\textendash sure densification guarantees for DPW. All reported numbers are deterministically generated by the embedded simulation.py and are reproduced byte\textendash exactly in the included CSVs and results.txt.
\end{abstract}

\section{Introduction}
No\textendash Limit Texas Hold'em (NLHE) is a canonical imperfect\textendash information benchmark combining hidden private cards, public community cards, and large continuous action spaces. Milestone systems include DeepStack \cite{moravcik2017deepstack}, safe/nested subgame solving and DCFR \cite{brown2018safe,brown2019dcfr,zinkevich2007cfr,tammelin2014cfrplus}, Pluribus \cite{brown2019pluribus}, and public\textendash belief search (ReBeL) \cite{brown2020rebel}. Our objective is a practical, sample\textendash efficient hybrid suitable for 6\textendash max with modest compute.

Contributions (innovation, efficiency, and rigor):
- \sCRF\ opponent model producing calibrated posteriors with a single trajectory\textendash wide base measure and consistent bucketization.
- IS\textendash RS\textendash CVaR\textendash UCT: belief\textendash aware MCTS with a streaming RU CVaR estimator and bandit\textendash gated DPW for continuous sizes; we provide selection stability bounds and formalize the selection procedure.
- Leakage\textendash Free Public\textendash Subgame CFR$^+$ using CPB as root chance with symmetry\textendash invariance.
- A preconditioned, accelerated CPB solver analysis with condition\textendash number bounds, a reference\textendash metric whitening perspective, and a continuation scheme; diagnostics use a simple GD baseline for transparency.
- Reproducible, deterministic diagnostics: CPB projection residuals, IS\textendash RS unbiasedness with CRN, RU\textendash CVaR convergence on Uniform$[-1,1]$, and DPW densification.

\section{Related Work}
DeepStack \cite{moravcik2017deepstack}; safe/nested subgame solving \cite{brown2018safe}; DCFR \cite{brown2019dcfr}; CFR \cite{zinkevich2007cfr}; CFR$^+$ \cite{tammelin2014cfrplus}; Pluribus \cite{brown2019pluribus}; ReBeL \cite{brown2020rebel}; POMCP \cite{silver2010pomcp}; ISMCTS \cite{cowling2012ismcts}; DPW \cite{couetoux2011dpw}; CVaR and optimization \cite{rockafellar2000cvar,chow2014cvar,tamar2015cvar}; calibration \cite{guo2017calibration}; PSRO \cite{lanctot2017psro}; AIVAT \cite{burch2018aivat}; SMC \cite{doucet2001smc}; stochastic approximation \cite{borkar2008sa}; I\textendash projections \cite{csiszar1975i}; exchangeability \cite{diaconis1980finite}; UCT \cite{kocsis2006uct}; iterative scaling and maxent RF \cite{darroch1972gis,dellapietra1997pami}; mirror descent \cite{beck2003mirror}; LinUCB \cite{li2010linucb}; information geometry and exponential families \cite{wainwright2008graphical,amari2016ig}; accelerated first\textendash order methods \cite{nesterov2004introductory}; adaptive restarts \cite{odonoghue2015restart}; Barzilai\textendash Borwein steps \cite{barzilai1988bb}; L\textendash BFGS \cite{liu1989lbfgs}; DKW concentration \cite{massart1990dkw}; probabilistic foundations \cite{kallenberg2002fmp}; risk\textendash constrained RL with CVaR \cite{chow2017riskjmlr}.

\section{Preliminaries}
We consider an $N$\textendash player EFG with imperfect information $(\States,\Actions,\Players,\IS,\Chance,u)$. A public state $x$ is a projection of a history $h$. Let $H_{-i}$ be opponents’ private hands and $H_i$ the acting player's hand. A public belief state is $\rho_t(H_{-i}\mid h_t,H_i)$; feasibility masks enforce disjointness and blockers.

\section{Public subgames and admissible chance}
We re\textendash solve public subgames $\mathcal{G}_{\text{pub}}(x,H_i)$ whose root chance samples opponents’ private cards from $q(H_{-i}\mid x,H_i)$ supported on feasible hands and excluding $H_i$. A symmetry subgroup $G(x)$ permutes indistinguishable opponent labels. We require $q$ to be $G(x)$\textendash invariant and Borel\textendash measurable in $(x,H_i)$.

\section{Method}
\subsection{Public encoder}
A transformer encoder outputs masked policy $\pi_\theta(\cdot\mid x)$ and value $v_\theta(x)$ using tokenized public histories with position/card embeddings.

\subsection{\sCRF\ opponent modeling}
We model streetwise latent styles and action features, inducing bucketed likelihoods $p_\phi(b\mid C(H_j),x)$ under a single base measure per trajectory (PMF over buckets in pot\textendash fraction). Buckets are left\textendash closed, right\textendash open (last bucket closed); historical sizes are mapped deterministically across street boundaries by applying the same pot\textendash fraction partition and boundary convention to past sizes. This order\textendash preserving mapping ensures consistent likelihoods.

Representative mapping r$_x$ uses bucket midpoints in pot\textendash fraction, converted to chips at execution. Under a piecewise\textendash Lipschitz value function with finitely many thresholds per state, midpoint representatives incur bounded error.

\begin{lemma}[Midpoint representative error]
Let $I=[a,b]$ be a bucket interval in pot\textendash fraction and suppose the normalized value $V$ is $L$\textendash Lipschitz on $I$. Then choosing the midpoint $a^\star=(a+b)/2$ yields $|V(a^\star)-\E_{U\sim\mathrm{Unif}(I)}[V(U)]|\le L(b-a)/4$ and $|V(a^\star)-V(u)|\le L(b-a)/2$ for any $u\in I$.
\end{lemma}
\begin{proof}
The worst\textendash case pointwise deviation from any $u\in I$ is bounded by $L|a^\star-u|\le L(b-a)/2$. The mean deviation bound follows by Jensen and symmetry.
\end{proof}

\paragraph{Joint belief update and folds.}
With feasibility mask $\mathcal{H}(x,H_i)$, for opponent $j$ acting with observed action $a_t$ mapped to bucket $b_t$,
\[
\rho_{t+1}(H_{-i}\mid h_{t+1},H_i)\ \propto\ \1\{H_{-i}\in\mathcal{H}(x_t,H_i)\}\ \rho_t(H_{-i}\mid h_t,H_i)\ p_\phi\big(b_t\mid C(H_j),x_t\big).
\]
Folds contribute a single factor $p_\phi(\mathrm{fold}\mid C(H_j),x_t)$; thereafter seat $j$ is marked inactive for future action likelihoods, but $H_j$ remains in the joint assignment for blockers and showdowns.

\begin{proposition}[Posterior factorization under inactivity]
Let $\mathcal{A}_t$ be the set of active opponents at time $t$. Suppose $j\notin\mathcal{A}_{t'}$ for all $t'>t$. Then posterior updates after $t$ include no further likelihood terms for $H_j$ beyond feasibility, i.e., folding induces a single likelihood factor at $t$ and no additional posterior drift.
\end{proposition}

\subsection{Consistency\textendash Projected Beliefs (CPB)}
We project an empirical posterior onto a realizable, $G(x)$\textendash invariant exponential family relative to a symmetric base measure using an I\textendash projection with moment constraints \cite{csiszar1975i}, realized on an orbit\textendash completed finite support. Let the standardized statistics on orbit representatives be $S_o\in\R^K$ and define the exponential\textendash family law on the finite support by
$
q_\lambda(o) \propto \exp(\langle \lambda, S_o\rangle),\quad Z(\lambda)=\sum_{o}\exp(\langle \lambda, S_o\rangle).
$
We solve the penalized dual
\[
\min_{\lambda\in\R^K}\ f_\gamma(\lambda)\ \coloneqq\ \log Z(\lambda) - \langle \lambda,\widehat s\rangle + \tfrac{\gamma}{2}\|\lambda\|_2^2,
\]
which yields a unique optimizer for any $\gamma>0$.

\begin{theorem}[Strong convexity, Lipschitz gradient, condition number]\label{thm:cpb-strong}
Let $\{S_o\}_{o=1}^m\subset\R^K$ be standardized orbit representatives, $R=\max_o \|S_o\|_2<\infty$. For any $\gamma>0$, $f_\gamma$ is $\gamma$\textendash strongly convex and has $L$\textendash Lipschitz gradient with $L \le R^2+\gamma$. The condition number satisfies $\kappa\le (R^2+\gamma)/\gamma$. Consequently, fixed\textendash step gradient descent with $\eta\in(0,1/L]$ converges linearly to $\lambda^\star_\gamma$.
\end{theorem}
\begin{proof}
$\nabla f_\gamma(\lambda)=\E_{q_\lambda}[S]-\widehat s+\gamma\lambda$, $\nabla^2 f_\gamma(\lambda)=\mathrm{Cov}_{q_\lambda}(S)+\gamma I\succeq \gamma I$. Since $\lambda_{\max}(\mathrm{Cov}(S))\le \E\|S\|^2\le R^2$, $L\le R^2+\gamma$.
\end{proof}

\begin{corollary}[Local curvature at the optimizer]\label{cor:local-L}
Let $\lambda^\star_\gamma$ minimize $f_\gamma$. Then the local gradient Lipschitz constant satisfies $L_\star=\lambda_{\max}(\mathrm{Cov}_{q_{\lambda^\star_\gamma}}(S))+\gamma\le R^2+\gamma$. A practical fixed step $\eta\approx 1/L_\star$ is admissible and typically larger than $1/(R^2+\gamma)$ due to standardization.
\end{corollary}

\begin{proposition}[Accelerated, preconditioned complexity]\label{prop:accel}
Let $\mu=\gamma$ and $L\le R^2+\gamma$ as in \cref{thm:cpb-strong}. An accelerated gradient method with diagonal preconditioning and restarts attains $f_\gamma(\lambda^{(t)})-f_\gamma(\lambda^\star)\le \mathcal{O}\!\left((1-\sqrt{\mu/L})^{t}\right)$ and $\varepsilon$\textendash accuracy in $\mathcal{O}\!\left(\sqrt{L/\mu}\log(1/\varepsilon)\right)$ gradients \cite{nesterov2004introductory,odonoghue2015restart}. Each iteration costs $\Theta(Km)$ with two passes (scores, expectation) using a stable log\textendash sum\textendash exp.
\end{proposition}

\begin{lemma}[Reference\textendash metric whitening and condition number]\label{lem:refwhiten}
Let $\Sigma_{\mathrm{ref}}=\frac{1}{m}\sum_{o=1}^m S_o S_o^\top$ and define whitened statistics $\tilde S_o=\Sigma_{\mathrm{ref}}^{-1/2}S_o$, reparameterizing with $\tilde\lambda=\Sigma_{\mathrm{ref}}^{1/2}\lambda$. Then for the penalized dual in these coordinates,
$
\nabla^2 \tilde f_\gamma(\tilde\lambda)=\mathrm{Cov}_{q_{\tilde\lambda}}(\tilde S)+\gamma I,
$
so $L_{\mathrm{ref}}\le \lambda_{\max}(\E_{q_{\tilde\lambda}}[\tilde S\tilde S^\top])+\gamma$. Along the uniform reference, $L_{\mathrm{ref}}\le 1+\gamma$. Uniformly over all $q$, $L\le R^2+\gamma$.
\end{lemma}
\begin{proof}
Change of variables yields the displayed Hessian. For the uniform reference, $\E[\tilde S\tilde S^\top]=I$. Taking spectral maxima gives the bounds.
\end{proof}

\begin{lemma}[Penalization bias under local strong convexity]\label{lem:penalty-bias}
Assume the unpenalized dual $f_0$ has a unique minimizer $\lambda^\star_0$ and is $\mu_0$\textendash strongly convex in a neighborhood of $\lambda^\star_0$ (e.g., $\lambda_{\min}(\mathrm{Cov}_{q_{\lambda^\star_0}}(S))\ge \mu_0>0$). Then for $\gamma\in(0,\mu_0)$,
$
\|\lambda^\star_\gamma-\lambda^\star_0\|_2\le \frac{\gamma}{\mu_0-\gamma}\,\\|\lambda^\star_0\|_2,
$
and the induced moment bias obeys
$
\|\,\E_{q_{\lambda^\star_\gamma}}[S]-\E_{q_{\lambda^\star_0}}[S]\,\|_2 \le (R^2+\gamma)\,\|\lambda^\star_\gamma-\lambda^\star_0\|_2.
$
\end{lemma}
\begin{proof}
Optimality gives $\nabla f_0(\lambda^\star_\gamma)=-\gamma\lambda^\star_\gamma$. Local strong convexity yields
$
\|\lambda^\star_\gamma-\lambda^\star_0\|\le \frac{1}{\mu_0}\|\nabla f_0(\lambda^\star_\gamma)-\nabla f_0(\lambda^\star_0)\|= \frac{\gamma}{\mu_0}\|\lambda^\star_\gamma\|.
$
Triangle inequality and rearrangement give the first bound. The moment bound follows from the mean value theorem and the Jacobian norm bound $\le R^2+\gamma$.
\end{proof}

\begin{proposition}[Geometric continuation and target bias]\label{prop:continuation}
Let $\gamma_t=\gamma_0 \rho^t$ with $\rho\in(0,1)$. Warm\textendash start stage $t$ at $\lambda^\star_{\gamma_{t-1}}$ and run an accelerated method with rate $(1-\sqrt{\gamma_t/L_t})$ (with $L_t\le R^2+\gamma_t$). Then the total gradient evaluations
$
\sum_{t=1}^{T} \mathcal{O}\!\left(\sqrt{L_t/\gamma_t}\,\log\frac{C}{\varepsilon}\right)
= \mathcal{O}\!\left(\sqrt{1+\tfrac{R^2}{\gamma_T}}\log\frac{C}{\varepsilon}\cdot T\right),
$
where $C$ absorbs warm\textendash start error via \cref{lem:penalty-bias}. For a target moment bias $\delta>0$, a rule\textendash of\textendash thumb chooses $\gamma_T$ by solving
$
(R^2+\gamma_T)\,\frac{\gamma_T}{\mu_0-\gamma_T}\,\|\lambda^\star_0\| \le \delta
$
or, conservatively, $\gamma_T \approx \delta\,\mu_0/(R^2\|\lambda^\star_0\|)$ when $\gamma_T\ll \mu_0$.
\end{proposition}
\begin{proof}[Sketch]
Sum the stage complexities and apply \cref{lem:penalty-bias}. The bias rule follows by imposing the moment\textendash bias bound.
\end{proof}

\paragraph{Preconditioner and stopping.}
We use a diagonal preconditioner $D_t=\mathrm{diag}((v_t+\epsilon)^{-1/2})$ with $v_t=\rho v_{t-1}+(1-\rho)\,g_t\odot g_t$, $\rho\in[0.9,0.99]$, $\epsilon>0$; this approximates Fisher/covariance scaling and is stable under bounded curvature of the softmax log\textendash partition. Recommended stopping criteria: $\|\nabla f_\gamma(\lambda)\|_2\le 10^{-6}$ and moment residual $\|\E_{q_\lambda}[S]-\widehat s\|_2\le 10^{-3}$.

\begin{figure}[t]
\centering
\begin{adjustbox}{width=\linewidth}
\begin{minipage}{\linewidth}
\begin{algorithm}[H]
\caption{Preconditioned--Accelerated CPB (single $\gamma$ stage)}
\label{alg:cpb}
\begin{algorithmic}[1]
\Require Standardized stats $\{S_o\}$, target $\widehat s$, penalty $\gamma>0$, tol $\varepsilon$, radius $R$, EMA $\rho$, jitter $\epsilon$
\State Initialize $\lambda^{(0)}=0$, $y^{(0)}=\lambda^{(0)}$, $\theta^{(0)}=1$, $v^{(0)}=\mathbf{0}$; stepsize $\eta\gets 1/(R^2+\gamma)$
\For{$t=0,1,2,\dots$}
  \State Compute scores $u_o=\langle y^{(t)},S_o\rangle$, weights $w_o \propto \exp(u_o - \max_o u_o)$, $Z=\sum_o w_o$
  \State $\E[S]\gets \sum_o \frac{w_o}{Z} S_o$, $g^{(t)} \gets \E[S]-\widehat s + \gamma y^{(t)}$
  \State $v^{(t+1)}\gets \rho v^{(t)} + (1-\rho)\,g^{(t)}\odot g^{(t)}$, $D^{(t+1)}\gets \mathrm{diag}((v^{(t+1)}+\epsilon)^{-1/2})$
  \State $\lambda^{(t+1)} \gets y^{(t)} - \eta D^{(t+1)} g^{(t)}$
  \State $\theta^{(t+1)} \gets \tfrac{1+\sqrt{1+4(\theta^{(t)})^2}}{2}$; $y^{(t+1)} \gets \lambda^{(t+1)} + \tfrac{\theta^{(t)}-1}{\theta^{(t+1)}}(\lambda^{(t+1)}-\lambda^{(t)})$
  \If{$\|\nabla f_\gamma(\lambda^{(t+1)})\|_2 \le \varepsilon$ and $\|\E_{q_{\lambda^{(t+1)}}}[S]-\widehat s\|_2\le 10^{-3}$} \textbf{break} \EndIf
  \State Optional restart if $f_\gamma(\lambda^{(t+1)})>f_\gamma(\lambda^{(t)})$
\EndFor
\State \Return $\lambda^{\star}\gets \lambda^{(t+1)}$
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{adjustbox}
\end{figure}

\subsection{Leakage\textendash Free Public\textendash Subgame CFR$^+$}
We re\textendash solve with root chance $q=\tilde\rho_t(\cdot\mid x,H_i)$, equivariant menu generation, label\textendash agnostic seeds, and permutation\textendash equivariant regret\textendash matching$+$. This yields $G(x)$\textendash invariant strategies at every iteration.

\begin{definition}[Group action on information sets]
Let $G(x)$ act on player labels and induce a bijection $g:\IS\to\IS$, preserving public states and legal actions, with pullbacks on strategies and regret vectors via $(g\cdot \pi)(I,a)=\pi(g^{-1}I,g^{-1}a)$ and $(g\cdot r)(I,a)=r(g^{-1}I,g^{-1}a)$.
\end{definition}

\begin{theorem}[Symmetry preservation of LF\textendash PS\textendash CFR$^+$]\label{thm:symm}
Suppose (i) the root chance $q$ is $G(x)$\textendash invariant; (ii) action menus are $G(x)$\textendash equivariant; and (iii) regret matching$+$ is $G(x)$\textendash equivariant (e.g., orbit\textendash averaged features and label\textendash agnostic seeds). Then all CFR$^+$ iterates are $G(x)$\textendash invariant.
\end{theorem}
\begin{proof}
Under (i)\&(ii) counterfactual values satisfy $v(I,a)=v(gI,ga)$. Regret accumulation and the regret\textendash to\textendash policy map commute with $g$ under (iii), so if $\pi^t$ is invariant then so is $\pi^{t+1}$. The base iterate is invariant by construction, hence all iterates are.
\end{proof}

\subsection{Belief\textendash MCTS with CVaR\textendash UCT and DPW}
We use IS\textendash RS with a frozen root CPB. Opponent actions in simulations are sampled from a known $H$\textendash independent generator
$
\pi_{\mathrm{opp}}(b\mid x)=(1-\eta)A_x(\pi_\theta)(b\mid x)+\eta\,p_\phi^{\mathrm{marg}}(b\mid x),
$
renormalized over legal buckets. Public reveals are $H$\textendash independent conditioned on feasibility.

\paragraph{Normalization and selection stability.}
Let $X\in[-1,1]$ be normalized returns and $Y=-X$ losses. The selection score is
$
U(x,a)=\hat\mu_{x,a}-\lambda\,\widehat{\CVaR}_\alpha(Y)+c_{\text{puct}}\ \tilde P(a\mid x)\ \frac{\sqrt{\sum_b N(x,b)}}{1+N(x,a)}.
$
With $|\hat\mu_{x,a}|\le 1$ and $|\widehat{\CVaR}_\alpha(Y)|\le 1$, we have $|\hat\mu_{x,a}-\lambda \widehat{\CVaR}_\alpha|\le 1+\lambda$, supporting co\textendash scaling $c_{\text{puct}}\approx 1+\lambda$.

\begin{proposition}[Score sensitivity and ordering stability]
Suppose per\textendash arm estimation errors satisfy $|\hat\mu_{x,a}-\mu_{x,a}|\le \epsilon_\mu$ and $|\widehat{\CVaR}_\alpha(Y_{x,a})-\CVaR_\alpha(Y_{x,a})|\le \epsilon_c$, and exploration terms are identical across compared actions. If two actions $a,a'$ satisfy
$
|\ (\mu_{x,a}-\lambda \CVaR_\alpha(Y_{x,a}))-(\mu_{x,a'}-\lambda \CVaR_\alpha(Y_{x,a'}))\ |\ge \Delta,
$
then their empirical ordering by $U$ is unchanged provided $\Delta>2(\epsilon_\mu+\lambda \epsilon_c)$.
\end{proposition}

\paragraph{Streaming RU CVaR estimator.}
We minimize $t+\frac{1}{1-\alpha}\E[(Y-t)_+]$ with two\textendash timescale SA and projections $t\in[-1,1]$, $m\in[0,2]$, then form $\widehat{\CVaR}_\alpha=t+m/(1-\alpha)$, clamped to $[-1,1]$. Stepsizes $a_n\propto (n+n_0)^{-0.9}$, $b_n\propto (n+n_0)^{-0.6}$ satisfy Robbins--Monro conditions and $a_n/b_n\to 0$. Polyak–Ruppert averaging \cite{polyak1992averaging} can further reduce variance; our diagnostics report the terminal tracker for determinism.

\begin{figure}[t]
\centering
\begin{adjustbox}{width=\linewidth}
\begin{minipage}{\linewidth}
\begin{algorithm}[H]
\caption{Two\textendash timescale RU CVaR tracker (per edge)}
\label{alg:cvar}
\begin{algorithmic}[1]
\Require tail level $\alpha$, steps $a_n,b_n$, projections $\Pi_t,\Pi_m$
\State Initialize $t_0=0$, $m_0=0$
\For{$n=1,2,\dots$}
  \State Observe loss $Y_n\in[-1,1]$
  \State $g_t \gets 1-\frac{1}{1-\alpha}\1\{Y_n>t_{n-1}\}$; $t_n\gets \Pi_t\big(t_{n-1}-a_n g_t\big)$
  \State $u_n\gets \max\{0,Y_n-t_n\}$; $m_n\gets \Pi_m\big(m_{n-1}+b_n(u_n-m_{n-1})\big)$
  \State $\widehat{\CVaR}_{\alpha,n}\gets \mathrm{clip}\big(t_n+\frac{m_n}{1-\alpha},-1,1\big)$
\EndFor
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{adjustbox}
\end{figure}

\begin{proposition}[Two\textendash timescale SA convergence]\label{prop:sa}
Assume $\{Y_n\}$ is stationary ergodic with $\E|Y_n|<\infty$. Let $a_n,b_n>0$ satisfy $\sum a_n=\sum b_n=\infty$, $\sum (a_n^2+b_n^2)<\infty$, and $a_n/b_n\to 0$. Then $(t_n,m_n)$ from \cref{alg:cvar} converges a.s. to the unique minimizer of $t+\frac{1}{1-\alpha}\E[(Y-t)_+]$, and $\widehat{\CVaR}_{\alpha,n}\to \CVaR_\alpha(Y)$ a.s. \cite{rockafellar2000cvar,borkar2008sa,kushner2003sa}.
\end{proposition}

\subsection{IS\textendash RS unbiasedness and reveal cancellation}
\begin{definition}[H\textendash independence]
Let $\mathcal{X}$ be the $\sigma$\textendash field generated by the public reveal sequence. An opponent generator $\pi_{\mathrm{opp}}(a\mid x)$ is H\textendash independent if $\mathbb{P}(A_t\in A\mid X_t=x,H)=\mathbb{P}(A_t\in A\mid X_t=x)$ for all $t$, measurable $A$, and hands $H$.
\end{definition}

\begin{lemma}[IS\textendash RS unbiasedness under H\textendash independence]\label{lem:isrs}
Let $\mathcal{F}_t=\sigma(X_{1:t})$ be the public\textendash reveal filtration. Fix a frozen root belief $q(H)$ and an $H$\textendash independent generator $\pi_{\mathrm{opp}}(a\mid x)$. Consider any resampling policy that only resamples at reveals and preserves the marginal law of $(H,X_{1:T})$. Then the Monte Carlo return $\widehat{R}$ satisfies $\E[\widehat{R}]=\E[R]$, the true value against $\pi_{\mathrm{opp}}$ from the root public state.
\end{lemma}
\begin{proof}
By H\textendash independence,
$
\mathrm{d}\mathbb{P}(H,X_{1:T},A_{1:T})=\mathrm{d}q(H)\prod_{t=1}^T \pi_{\mathrm{opp}}(A_t\mid X_t)\,\mathrm{d}P(X_t\mid X_{t-1},A_{t-1}).
$
Disintegrate w.r.t.\ $\mathcal{F}_T$:
$
\mathrm{d}\mathbb{P}(H\mid \mathcal{F}_T)\propto \mathrm{d}q(H)\prod_t \mathrm{d}P(X_t\mid X_{t-1},A_{t-1}),
$
so $A_{1:T}$ cancel from $\mathbb{P}(H\mid \mathcal{F}_T)$. A resampling schedule that only conditions on $\mathcal{F}_t$ and preserves the marginal of $(H,X_{1:T})$ leaves $\E[R]$ invariant by the tower property and uniqueness of conditional expectation \cite{kallenberg2002fmp}.
\end{proof}

\begin{proposition}[Bias bound under controlled H\textendash dependence]
If $\mathrm{TV}(\pi_{\mathrm{opp}}(\cdot\mid x,H),\pi_{\mathrm{opp}}(\cdot\mid x))\le \varepsilon$ uniformly in $(x,H)$ and returns lie in $[-1,1]$, then $|\E[\widehat{R}]-\E[R]|\le 2\sum_{t=1}^T \varepsilon \le 2T\varepsilon$ by a stepwise coupling argument.
\end{proposition}

\paragraph{CRN variance reduction.}
For paired estimators $(Z^{(1)}_n,Z^{(2)}_n)$ with identical marginals and correlation $\rho$, the difference\textendash mean variance is $\frac{1}{n}(\Var(Z^{(1)})+\Var(Z^{(2)})-2\rho\sqrt{\Var(Z^{(1)})\Var(Z^{(2)})})$; with CRN $\rho\approx 1$, the CI collapses, as in our diagnostic.

\paragraph{DPW with bandit gating and densification.}
We propose new sizes under growth $|\mathcal{B}_x|\lesssim k_a N(x)^{\alpha_a}$ and accept via a mixture of (i) an independent forced\textendash acceptance coin with probability $\varepsilon_n=c/(n+1)^\beta$, $\beta\in(0,1]$, and (ii) contextual\textendash bandit UCB or gap\textendash based acceptors. The independent coin guarantees coverage; any extra acceptors only increase it.

\begin{lemma}[Divergence of forced\textendash acceptance series]
For any $c>0$ and $\beta\in(0,1]$, $\sum_{n=1}^{\infty} \frac{c}{(n+1)^\beta}=\infty$.
\end{lemma}

\begin{proposition}[Densification under forced acceptance]\label{prop:densify}
Let $\mathcal{F}_{n}$ be the history up to visit $n$. Suppose proposals satisfy $\mathbb{P}(A_n\in I\mid \mathcal{F}_{n-1})\ge p\,|I|$ for all intervals $I\subset[0,1]$ and some $p>0$, and accept with an $\mathcal{F}_{n-1}$\textendash independent coin of probability at least $\varepsilon_n=c/(n+1)^\beta$, $\beta\in(0,1]$. Then the accepted sizes are almost surely dense in $[0,1]$ and the minimum gap $\to 0$ a.s.
\end{proposition}
\begin{proof}[Sketch]
For any rational\textendash endpoint interval $I$, define $E_n(I)=\{\text{an acceptance lands in }I\text{ at time }n\}$. Then
$
\mathbb{P}(E_n(I)\mid \mathcal{F}_{n-1})\ge p|I|\varepsilon_n.
$
Since $\sum_n p|I|\varepsilon_n=\infty$, the conditional Borel–Cantelli lemma (e.g., \cite[Thm.~4.3.4]{durrett2019probability}) yields $\mathbb{P}(E_n(I)\ \text{i.o.})=1$. Countably many such $I$ cover $[0,1]$, hence density and vanishing minimum gap follow.
\end{proof}

\begin{corollary}[Expected accepted count]\label{cor:expect}
Under \cref{prop:densify} with $\beta=1$, $\E[K_N]\ge c' \log(N+1)$ for $c'=cp/2$, so $\E[K_N]=\Theta(\log N)$. Heuristically, the expected minimum gap scales as $\mathcal{O}(1/K_N)$.
\end{corollary}

\subsection{Finite\textendash sample concentration and calibration for bounded\textendash loss CVaR}
Using $\CVaR_\alpha(Y)=\frac{1}{1-\alpha}\int_\alpha^1 F_Y^{-1}(u)\,\mathrm{d}u$ \cite{rockafellar2000cvar} and Massart\textendash DKW \cite{massart1990dkw}:

\begin{proposition}[CVaR concentration via DKW]\label{prop:cvar-dkw}
Let $\widehat{F}_n$ be the empirical CDF from $n$ i.i.d.\ samples of $Y\in[-1,1]$, and define $\widehat{\CVaR}_{\alpha,n}=\frac{1}{1-\alpha}\int_\alpha^1 \widehat{F}_n^{-1}(u)\,\mathrm{d}u$. Then with probability at least $1-\delta$,
$
|\widehat{\CVaR}_{\alpha,n}-\CVaR_\alpha(Y)| \le \frac{2}{1-\alpha}\sqrt{\frac{1}{2n}\log\frac{2}{\delta}}.
$
\end{proposition}
\begin{proof}[Sketch]
Massart’s DKW gives $\|\widehat{F}_n-F\|_\infty\le \epsilon$ w.p.\ $\ge 1-\delta$. On $[-1,1]$, quantile errors are bounded by $2\epsilon$, and integrating over $[\alpha,1]$ yields the claim.
\end{proof}

\begin{lemma}[Uniform ground truth]\label{lem:uniform-cvar}
If $Y\sim \mathrm{Unif}[-1,1]$, then $\VaR_\alpha(Y)=2\alpha-1$ and $\CVaR_\alpha(Y)=\alpha$.
\end{lemma}

\section{Complexity}
Let $S$ simulations, depth $d$, average branching $\bar b$, players $N$, particles $M$, features $K$, orbits $m$. Belief updates are $O(M)$ per observation; conditional sampling via segment trees is $O(\log M)$ per draw; selection/backups $O(S d \bar b)$; CPB dual $O(T K m)$ with accelerated rate in \cref{prop:accel} and condition number $\kappa\le (R^2+\gamma)/\gamma$ (or $L_{\mathrm{ref}}\le 1+\gamma$ along the reference metric after whitening in \cref{lem:refwhiten}); DPW bandit $O(S d p)$ with $p$ features; memory $O(Sd)$ for the tree and $O(M)$ for particles.

\section{Sanity\textendash check experiments and diagnostics}
All numbers are generated deterministically by simulation.py and saved locally. We include results.txt verbatim for traceability. Running “python3 simulation.py” overwrites the files with identical content.

\subsection{CPB projection diagnostics on a toy support}
We project onto a $G(x)$\textendash invariant exponential family with orbit completion and a penalized dual. Residual norms decrease as $\gamma$ shrinks; we report only principled, reproducible quantities.

\begin{figure}[t]
\centering
\begin{adjustbox}{width=\linewidth}
\begin{tikzpicture}
\pgfplotstableread[col sep=comma]{cpb.csv}\cpbtable
\begin{axis}[
    ybar, bar width=12pt, enlarge x limits=0.2,
    xlabel={Penalty $\gamma$}, ylabel={Residual $\|\E_q[S]-\widehat s\|_2$},
    symbolic x coords={1.00,0.30,0.10,0.03,0.01},
    xtick=data,
    ymin=0, ymax=0.14,
    nodes near coords, nodes near coords align={vertical},
    grid=both]
\addplot table[x=gamma,y=residual]{\cpbtable};
\end{axis}
\end{tikzpicture}
\end{adjustbox}
\caption{CPB residual vs.\ penalty $\gamma$ on a toy orbit\textendash completed support (seed 101; from cpb.csv).}
\label{fig:cpb}
\end{figure}

\begin{table}[t]
\centering
\begin{adjustbox}{width=\linewidth}
\pgfplotstabletypeset[
    col sep=comma,
    columns/gamma/.style={column name=$\gamma$},
    columns/residual/.style={column name=Residual},
    columns/num_orbits/.style={column name=\# Orbits},
    every head row/.style={before row=\toprule,after row=\midrule},
    every last row/.style={after row=\bottomrule}
]{cpb.csv}
\end{adjustbox}
\caption{CPB toy diagnostics (from cpb.csv). We report residuals and number of orbits only.}
\label{tab:cpb}
\end{table}

\subsection{IS\textendash RS unbiasedness sanity check}
We compare expected returns with and without resampling at opponent actions under an H\textendash independent generator (frozen root CPB). Paired CRN collapses the difference CI to zero.

\begin{table}[t]
\centering
\begin{adjustbox}{width=\linewidth}
\pgfplotstabletypeset[
    col sep=comma,
    columns/mean_no_resample/.style={column name=Mean (no resample)},
    columns/mean_resample/.style={column name=Mean (resample)},
    columns/ci_halfwidth/.style={column name=95\% CI half\textendash width},
    columns/mean_diff/.style={column name=Mean diff},
    columns/ci_halfwidth_diff/.style={column name=95\% CI (diff)},
    columns/sims/.style={column name=Simulations},
    every head row/.style={before row=\toprule,after row=\midrule},
    every last row/.style={after row=\bottomrule}
]{isrs.csv}
\end{adjustbox}
\caption{IS\textendash RS expected returns with paired CRN (seed 42; from isrs.csv).}
\label{tab:isrs}
\end{table}

\subsection{Streaming RU CVaR estimator on Uniform$[-1,1]$}
For $Y\sim \mathrm{Unif}[-1,1]$, \cref{lem:uniform-cvar} gives $\CVaR_\alpha(Y)=\alpha$. The streaming estimator approaches ground truth with $n$. The bands drawn from \cref{prop:cvar-dkw} are for the empirical plug\textendash in CVaR and are overlaid only as conservative orientation; they are not formal CIs for the SA tracker.

\begin{figure}[t]
\centering
\begin{adjustbox}{width=\linewidth}
\begin{tikzpicture}
\pgfplotstableread[col sep=comma]{rucvar_090.csv}\rutableA
\pgfplotstableread[col sep=comma]{rucvar_095.csv}\rutableB
\begin{axis}[
    xlabel={Samples $n$}, ylabel={$\,\widehat{\CVaR}_\alpha$},
    xmode=log, xtick={100,300,1000,3000,10000},
    xmin=90, xmax=12000, ymin=0.78, ymax=0.97,
    legend pos=south east, grid=both]
\addplot+[mark=*] table[x=n,y=cvar_est]{\rutableA};
\addlegendentry{$\alpha=0.90$ (true 0.90)}
\addplot+[mark=square*] table[x=n,y=cvar_est]{\rutableB};
\addlegendentry{$\alpha=0.95$ (true 0.95)}
% DKW bands at delta=0.05
\addplot+[domain=100:10000, samples=200, no marks, dashed, black]
  {0.90 + (2/(1-0.90))*sqrt(ln(2/0.05)/(2*x))};
\addlegendentry{DKW +95\% band ($\alpha=0.90$)}
\addplot+[domain=100:10000, samples=200, no marks, dashed, black]
  {0.90 - (2/(1-0.90))*sqrt(ln(2/0.05)/(2*x))};
\addplot+[domain=100:10000, samples=200, no marks, dashdotted, gray]
  {0.95 + (2/(1-0.95))*sqrt(ln(2/0.05)/(2*x))};
\addlegendentry{DKW +95\% band ($\alpha=0.95$)}
\addplot+[domain=100:10000, samples=200, no marks, dashdotted, gray]
  {0.95 - (2/(1-0.95))*sqrt(ln(2/0.05)/(2*x))};
\end{axis}
\end{tikzpicture}
\end{adjustbox}
\caption{RU CVaR tracker convergence on Uniform$[-1,1]$ losses (seed 777) with DKW 95\% bands (from rucvar\_090.csv and rucvar\_095.csv).}
\label{fig:cvar}
\end{figure}

\subsection{DPW coverage growth and densification}
We report accepted size counts and minimum gap vs.\ node visits $N$ under $\varepsilon_N=c/(N+1)$ with $c=5$. The expected number of accepted sizes grows as $\Theta(\log N)$ (\cref{cor:expect}); min gap shrinks toward 0 (\cref{prop:densify}). Our code also includes a gap\textendash based acceptor, which only improves constants and is not required for the almost\textendash sure density guarantee.

\begin{figure}[t]
\centering
\begin{adjustbox}{width=\linewidth}
\begin{tikzpicture}
\pgfplotstableread[col sep=comma]{dpw.csv}\dpwtable
\begin{axis}[
    xlabel={Node visits $N$}, ylabel={Accepted sizes},
    xmode=log, xtick={100,300,1000,3000,10000},
    xmin=90, xmax=12000, ymin=0, ymax=45, grid=both]
\addplot+[mark=*] table[x=N,y=accepted_count]{\dpwtable};
\end{axis}
\end{tikzpicture}
\end{adjustbox}
\caption{DPW: accepted size count vs.\ $N$ (seed 31415; from dpw.csv).}
\label{fig:dpw_count}
\end{figure}

\begin{figure}[t]
\centering
\begin{adjustbox}{width=\linewidth}
\begin{tikzpicture}
\pgfplotstableread[col sep=comma]{dpw.csv}\dpwtableB
\begin{axis}[
    xlabel={Node visits $N$}, ylabel={Min gap (pot\textendash fraction)},
    xmode=log, xtick={100,300,1000,3000,10000},
    xmin=90, xmax=12000, ymin=0.02, ymax=0.22, grid=both]
\addplot+[mark=square*] table[x=N,y=min_gap]{\dpwtableB};
\end{axis}
\end{tikzpicture}
\end{adjustbox}
\caption{DPW: min gap vs.\ $N$ (seed 31415; from dpw.csv).}
\label{fig:dpw_gap}
\end{figure}

\subsection{CVaR\textendash UCT selection with DPW gating}
We formalize the risk\textendash sensitive selection with double progressive widening and an independent forced\textendash acceptance coin.

\begin{figure}[t]
\centering
\begin{adjustbox}{width=\linewidth}
\begin{minipage}{\linewidth}
\begin{algorithm}[H]
\caption{CVaR\textendash UCT with DPW gating (single node $x$)}
\label{alg:cvar-uct-dpw}
\begin{algorithmic}[1]
\Require Node $x$, prior $\tilde P(\cdot\mid x)$, weight $\lambda\ge 0$, tail $\alpha\in(0,1)$, exploration $c_{\text{puct}}$, forced\textendash accept schedule $\varepsilon_n$, proposal sampler $\mathcal{Q}_x$
\If{should\_propose($N(x)$)}
  \State Propose $a\sim \mathcal{Q}_x$; accept if $\mathrm{Bern}(\varepsilon_{N(x)})=1$ or bandit\_accept($x,a$)=true
\EndIf
\For{each child $(x,a)$}
  \State Compute $U(x,a)=\hat\mu_{x,a}-\lambda\,\widehat{\CVaR}_\alpha(Y_{x,a}) + c_{\text{puct}}\ \tilde P(a\mid x)\ \frac{\sqrt{\sum_b N(x,b)}}{1+N(x,a)}$
\EndFor
\State Select action $a^\star\in\arg\max_a U(x,a)$ (ties by larger $\tilde P$)
\State \Return $a^\star$
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{adjustbox}
\end{figure}

\subsection{Public\textendash state symmetry sanity check}
On a toy public state with a nontrivial $G(x)$ (transposition of indistinguishable seats), the root action distributions under identity and transposition match; the computed L1 distance over $10^4$ trials is 0.0000. This is an internal consistency check induced by identical RNG seeds and generator invariance, not a statistical test.

\subsection*{Reproducibility statement}
- One authoritative code file: This paper embeds a single authoritative simulation.py that deterministically writes results.txt, cpb.csv, isrs.csv, rucvar\_090.csv, rucvar\_095.csv, and dpw.csv and prints: “Wrote results.txt, cpb.csv, isrs.csv, rucvar\_090.csv, rucvar\_095.csv, dpw.csv”.
- Unified schemas: All CSV schemas are exactly those consumed by the LaTeX.
- Byte identity: The CSV files and results.txt embedded via filecontents* are byte\textendash identical copies of the artifacts created by simulation.py.
- Reproduction: Run “python3 simulation.py” and then compile this paper with “pdflatex”.
- Environment: Python 3.8+ with standard library only; no third\textendash party dependencies.

\begin{adjustbox}{width=\linewidth}
\begin{minipage}{\linewidth}
\small
\verbatiminput{results.txt}
\end{minipage}
\end{adjustbox}

\section{Additional clarifications, efficiency, and rigor}
- Stable bucket mapping across streets: boundaries in pot\textendash fraction with left\textendash closed, right\textendash open intervals (last bucket closed). Historical sizes are mapped via their pot\textendash fraction at time of action; the mapping is order\textendash preserving.
- Selection stability under parameter damping: if CVaR level $\alpha$ and weight $\lambda$ change by at most $\delta_\alpha,\delta_\lambda$ per $k$ visits, the induced score drift per update is bounded by $|\Delta U|\le \delta_\lambda + \frac{2}{(1-\alpha)^2}\delta_\alpha$, hence choosing damping with $|\Delta U|<\tfrac{1}{2}\Delta$ preserves action ordering by the stability bound.
- SA with Markov noise: Conditional on a fixed menu/policy, edge\textendash local losses form an ergodic Markov chain. Nonvanishing exploration and reintroduction ensure infinitely many visits a.s., satisfying SA visitation conditions \cite{borkar2008sa,kushner2003sa}.
- CPB diagnostics vs.\ production: diagnostics use diminishing\textendash step GD (200 iters) for transparency and determinism. In production, whitening, diagonal preconditioning, adaptive restarts, and continuation (\cref{lem:refwhiten,prop:accel,prop:continuation}) improve iteration counts substantially. Monitor both gradient norm and moment residual with the explicit thresholds given above.
- Practical $\gamma$ selection: estimate local curvature $\mu_0\approx \lambda_{\min}(\mathrm{Cov}_{q_{\lambda}}(S))$ and $\|\lambda\|$ at a coarse $\gamma$, then set the terminal $\gamma_T$ via \cref{prop:continuation} to meet a target moment\textendash bias tolerance.

\section{Limitations and future work}
LF\textendash PS\textendash CFR$^+$ is a symmetry\textendash preserving local improvement heuristic in multiplayer, not a convergent equilibrium method. Risk\textendash sensitive selection lacks global convergence guarantees under adaptive policies; the RU estimator is asymptotically correct under SA conditions but acts as a tracker during transients. CPB fidelity depends on chosen statistics and support coverage; penalization induces controlled bias reduced via continuation. Future work: implement and benchmark the accelerated CPB solver with whitening and continuation; expose a Polyak–Ruppert tail\textendash average for RU\textendash CVaR; quantify DPW min\textendash gap decay with concentration; add controlled H\textendash dependence injections to empirically validate the $2T\varepsilon$ bias bound; and report end\textendash to\textendash end ablations on stylized poker subgames with continuous actions, including runtime breakdowns.

\section{Conclusion}
Belief\textendash MCTS\textendash \sCRF\ integrates structured opponent modeling, belief\textendash aware search with tail\textendash risk sensitivity, public\textendash subgame re\textendash solving, and adaptive continuous action sets. We advance innovations that improve algorithmic efficiency and mathematical rigor: strong convexity and conditioning bounds (including a reference\textendash metric view) for CPB with explicit stopping and preconditioning; unbiasedness conditions for IS\textendash RS under H\textendash independence with CRN variance collapse; almost sure convergence of the RU tracker; and densification guarantees and logarithmic accepted\textendash count growth for DPW under conditional Borel–Cantelli. The embedded simulation pipeline deterministically generates all reported results.

\small
\begin{thebibliography}{99}

\bibitem{moravcik2017deepstack}
M.~Morav\v{c}\'{\i}k, M.~Schmid, N.~Burch, V.~Lis\'{y}, D.~Morrill, N.~Bard, T.~Davis, K.~Waugh, M.~Johanson, and M.~Bowling.
DeepStack: Expert-level artificial intelligence in heads-up no-limit poker.
Science, 356(6337):508--513, 2017. doi:10.1126/science.aam6960.

\bibitem{brown2018safe}
N.~Brown and T.~Sandholm.
Safe and nested subgame solving for imperfect-information games.
Proceedings of the AAAI Conference on Artificial Intelligence, 32(1), 2018.

\bibitem{brown2019dcfr}
N.~Brown and T.~Sandholm.
Solving imperfect-information games via discounted CFR.
Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):1829--1836, 2019.

\bibitem{zinkevich2007cfr}
M.~Zinkevich, M.~Johanson, M.~Bowling, and C.~Piccione.
Regret minimization in games with incomplete information.
Advances in Neural Information Processing Systems, 20:1723--1730, 2007.

\bibitem{tammelin2014cfrplus}
O.~Tammelin.
Solving large imperfect information games using CFR$^+$.
arXiv:1407.5042, 2014.

\bibitem{brown2019pluribus}
N.~Brown and T.~Sandholm.
Superhuman AI for multiplayer poker.
Science, 365(6456):885--890, 2019. doi:10.1126/science.aay2400.

\bibitem{brown2020rebel}
N.~Brown, A.~Bakhtin, A.~Lerer, and Q.~Gong.
Combining deep reinforcement learning and search for imperfect-information games.
Nature, 588:542--547, 2020. doi:10.1038/s41586-020-3038-4.

\bibitem{silver2010pomcp}
D.~Silver and J.~Veness.
Monte-Carlo planning in large POMDPs.
Advances in Neural Information Processing Systems, 23:2125--2133, 2010.

\bibitem{cowling2012ismcts}
P.~I. Cowling, E.~J. Powley, and D.~Whitehouse.
Information set Monte Carlo tree search.
IEEE Transactions on Computational Intelligence and AI in Games, 4(2):120--143, 2012. doi:10.1109/TCIAIG.2012.2196810.

\bibitem{couetoux2011dpw}
A.~Cou\"{e}toux, J.-B. Hoock, N.~Sokolovska, O.~Teytaud, and N.~Bonnard.
Continuous upper confidence trees.
In C.~A.~C. Coello (ed.), Learning and Intelligent Optimization, pp. 433--445. Springer, 2011. doi:10.1007/978-3-642-25566-3\_9.

\bibitem{rockafellar2000cvar}
R.~T. Rockafellar and S.~Uryasev.
Optimization of conditional value-at-risk.
Journal of Risk, 2(3):21--41, 2000. doi:10.21314/JOR.2000.038.

\bibitem{chow2014cvar}
Y.~Chow and M.~Ghavamzadeh.
Algorithms for CVaR optimization in MDPs.
Advances in Neural Information Processing Systems, 27:3509--3517, 2014.

\bibitem{tamar2015cvar}
A.~Tamar, Y.~Glassner, and S.~Mannor.
Optimizing the CVaR via sampling.
Proceedings of AAAI-15, pp. 2993--2999, 2015.

\bibitem{guo2017calibration}
C.~Guo, G.~Pleiss, Y.~Sun, and K.~Q. Weinberger.
On calibration of modern neural networks.
Proceedings of the 34th ICML, pp. 1321--1330, 2017.

\bibitem{lanctot2017psro}
M.~Lanctot, V.~Zambaldi, A.~Gruslys, A.~Lazaridou, J.~P\'{e}rolat, K.~Tuyls, D.~Silver, and T.~Graepel.
A unified game-theoretic approach to multiagent reinforcement learning.
Advances in Neural Information Processing Systems, 30:4190--4200, 2017.

\bibitem{burch2018aivat}
N.~Burch, M.~Johanson, and M.~Bowling.
AIVAT: A new variance reduction technique for agent evaluation in imperfect information games.
Proceedings of AAMAS 2018, pp. 1417--1425, 2018.

\bibitem{doucet2001smc}
A.~Doucet, N.~de~Freitas, and N.~Gordon (eds.).
Sequential Monte Carlo Methods in Practice.
Springer, 2001. doi:10.1007/978-1-4757-3437-9.

\bibitem{borkar2008sa}
V.~S. Borkar.
Stochastic Approximation: A Dynamical Systems Viewpoint.
Hindustan/Cambridge University Press, 2008.

\bibitem{csiszar1975i}
I.~Csisz\'{a}r.
I-divergence geometry of probability distributions and minimization problems.
The Annals of Probability, 3(1):146--158, 1975. doi:10.1214/aop/1176996454.

\bibitem{diaconis1980finite}
P.~Diaconis and D.~Freedman.
Finite exchangeable sequences.
The Annals of Probability, 8(4):745--764, 1980. doi:10.1214/aop/1176994669.

\bibitem{kocsis2006uct}
L.~Kocsis and C.~Szepesv\'{a}ri.
Bandit based Monte-Carlo planning.
ECML 2006, pp. 282--293, 2006. doi:10.1007/11871842\_29.

\bibitem{darroch1972gis}
J.~N. Darroch and D.~Ratcliff.
Generalized iterative scaling for log-linear models.
The Annals of Mathematical Statistics, 43(5):1470--1480, 1972. doi:10.1214/aoms/1177692379.

\bibitem{dellapietra1997pami}
S.~Della Pietra, V.~Della Pietra, and J.~Lafferty.
Inducing features of random fields.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):380--393, 1997. doi:10.1109/34.588021.

\bibitem{beck2003mirror}
A.~Beck and M.~Teboulle.
Mirror descent and nonlinear projected subgradient methods for convex optimization.
Operations Research Letters, 31(3):167--175, 2003. doi:10.1016/S0167-6377(02)00231-6.

\bibitem{li2010linucb}
L.~Li, W.~Chu, J.~Langford, and R.~E. Schapire.
A contextual-bandit approach to personalized news article recommendation.
WWW 2010, pp. 661--670, 2010. doi:10.1145/1772690.1772758.

\bibitem{polyak1992averaging}
B.~T. Polyak and A.~B. Juditsky.
Acceleration of stochastic approximation by averaging.
SIAM Journal on Control and Optimization, 30(4):838--855, 1992. doi:10.1137/0330046.

\bibitem{nesterov2004introductory}
Y.~Nesterov.
Introductory Lectures on Convex Optimization: A Basic Course.
Springer, 2004. doi:10.1007/978-1-4419-8853-9.

\bibitem{durrett2019probability}
R.~Durrett.
Probability: Theory and Examples (5th ed.).
Cambridge University Press, 2019. doi:10.1017/9781108591034.

\bibitem{wainwright2008graphical}
M.~J. Wainwright and M.~I. Jordan.
Graphical models, exponential families, and variational inference.
Foundations and Trends in Machine Learning, 1(1-2):1--305, 2008. doi:10.1561/2200000001.

\bibitem{amari2016ig}
S.-I. Amari.
Information Geometry and Its Applications.
Springer, 2016. doi:10.1007/978-4-431-55978-8.

\bibitem{barzilai1988bb}
J.~Barzilai and J.~M. Borwein.
Two-point step size gradient methods.
IMA Journal of Numerical Analysis, 8(1):141--148, 1988. doi:10.1093/imanum/8.1.141.

\bibitem{liu1989lbfgs}
D.~C. Liu and J.~Nocedal.
On the limited memory BFGS method for large scale optimization.
Mathematical Programming, 45(1--3):503--528, 1989. doi:10.1007/BF01589116.

\bibitem{massart1990dkw}
P.~Massart.
The tight constant in the Dvoretzky-Kiefer-Wolfowitz inequality.
The Annals of Probability, 18(3):1269--1283, 1990. doi:10.1214/aop/1176990746.

\bibitem{kallenberg2002fmp}
O.~Kallenberg.
Foundations of Modern Probability (2nd ed.).
Springer, 2002. doi:10.1007/978-1-4757-4015-8.

\bibitem{kushner2003sa}
H.~J. Kushner and G.~Yin.
Stochastic Approximation and Recursive Algorithms and Applications (2nd ed.).
Springer, 2003. doi:10.1007/978-1-4757-4010-3.

\bibitem{odonoghue2015restart}
B.~O'Donoghue and E.~Cand\`{e}s.
Adaptive restart for accelerated gradient schemes.
Foundations of Computational Mathematics, 15(3):715--732, 2015. doi:10.1007/s10208-013-9150-3.

\bibitem{chow2017riskjmlr}
Y.~Chow, A.~Tamar, S.~Mannor, and M.~Pavone.
Risk-sensitive and robust decision-making: a CVaR optimization approach.
Journal of Machine Learning Research, 18(167):1--50, 2017.

\end{thebibliography}
\end{document}