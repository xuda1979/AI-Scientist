\documentclass[10pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage{natbib}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage[capitalise,nameinlink]{cleveref}
\usepackage{xspace}
\usepackage{adjustbox}

% Notation helpers
\newcommand{\sCRF}{\textsc{Street\textendash CRF}\xspace}
\newcommand{\CFRplus}{\ensuremath{\mathrm{CFR}^{+}}\xspace}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\KL}{\mathrm{KL}}
\newcommand{\CVaR}{\mathrm{CVaR}}
\newcommand{\VaR}{\mathrm{VaR}}
\newcommand{\BR}{\mathrm{BR}}
\newcommand{\States}{\mathcal{S}}
\newcommand{\Actions}{\mathcal{A}}
\newcommand{\IS}{\mathcal{I}}
\newcommand{\Players}{\mathcal{N}}
\newcommand{\Chance}{\mathsf{C}}
\newcommand{\Bel}{\rho}
\newcommand{\1}{\mathbf{1}}
\newcommand{\R}{\mathbb{R}}

\theoremstyle{plain}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}

\title{Belief---MCTS---\sCRF: A Hybrid, Sample\textendash Efficient Solver for Multi\textendash Player No\textendash Limit Texas Hold'em}
\author{Anonymous}
\date{September 2025}

\begin{document}
\maketitle

\begin{abstract}
We present \textbf{Belief--MCTS--\sCRF}, a practical multi\textendash player No\textendash Limit Hold'em agent that couples a transformer\textendash style \emph{public information\textendash state} encoder with a \emph{street\textendash structured conditional random field} (\sCRF) for opponent modeling, \emph{belief\textendash aware} Monte Carlo tree search (MCTS), and lightweight endgame subgame solving via CFR$^+$. The \sCRF converts observed betting sequences into calibrated hand\textendash range posteriors for all opponents; MCTS expands on samples from these beliefs with progressive widening for continuous bet sizes; and a short CFR$^+$ pass refines endgame play. 

Beyond engineering, we introduce foundational contributions with formal statements: (i) \textbf{Consistency\textendash Projected Beliefs (CPB)}, a maximum\textendash entropy projection of learned posteriors onto a \emph{group\textendash invariant, realizable} family used as chance in re\textendash solving; (ii) \textbf{Leakage\textendash Free Public\textendash Subgame CFR$^+$} (LF\textendash PS\textendash CFR$^+$) with an invariance guarantee under CPB; and (iii) \textbf{Information\textendash Set Root\textendash Sampled CVaR\textendash UCT} (IS\textendash RS\textendash CVaR\textendash UCT) that marries public\textendash belief sampling with a streaming CVaR estimator. We further propose \textbf{Range\textendash Calibration\textendash Aware Search} (RCAS) that gates reliance on beliefs using online calibration metrics, and a \textbf{bandit\textendash gated double progressive widening} scheme for continuous bet sizes. We provide precise definitions, corrected belief updates for multi\textendash opponent joint distributions, invariance proofs under a symmetry group of the public state, complexity analysis, and a discussion of convergence caveats under risk\textendash sensitive objectives.

Scope note: this version focuses on mathematical and algorithmic correctness. No empirical results are reported and no external artifacts are required.
\end{abstract}

\section{Introduction}
No\textendash Limit Texas Hold'em (NLHE) is a canonical imperfect\textendash information benchmark with hidden private cards, public community cards, and large, continuous action spaces. Milestone systems such as DeepStack, Libratus, and Pluribus demonstrated expert and superhuman play via neural evaluation with subgame solving (DeepStack) \citep{moravcik2017deepstack}, safe/nested subgame solving and improved regret minimization (Libratus era) \citep{brown2018safe,brown2019dcfr,zinkevich2007cfr,tammelin2014cfrplus}, and scalable multi\textendash player search (Pluribus) \citep{brown2019pluribus}. ReBeL unified search and learning around public\textendash belief states \citep{brown2020rebel}. Our goal is a \emph{practical} hybrid that captures much of this power with modest compute, suitable for 6\textendash max play and research iteration.

\paragraph{Contributions (mathematical and algorithmic).}
- A \textbf{\sCRF\ opponent model} that conditions on position\textendash aware betting sequences (preflop$\to$flop$\to$turn$\to$river) to infer latent variables and produce calibrated posterior hand ranges with a single, consistent base measure per trajectory.
- \textbf{IS\textendash RS\textendash CVaR\textendash UCT}: information\textendash set root sampling of joint private hands, belief/blueprint prior mixing, a streaming CVaR estimator with two\textendash timescale stochastic approximation, and bandit\textendash gated double progressive widening for continuous bet sizes with asymptotic coverage.
- \textbf{LF\textendash PS\textendash CFR$^+$} that re\textendash solves public subgames using \textbf{Consistency\textendash Projected Beliefs (CPB)} as the root chance kernel. We formalize a public\textendash state symmetry group $G(x)$ and prove invariance of the re\textendash solved strategy under $G(x)$, ruling out private\textendash information leakage.
- \textbf{RCAS} with a precise gating function based on online Expected Calibration Error (ECE) and posterior entropy with hysteresis.

\section{Related Work}
DeepStack introduces depth\textendash limited continual re\textendash solving with learned value functions \citep{moravcik2017deepstack}. Work underpinning Libratus formalized safe and nested subgame solving \citep{brown2018safe} and improved regret minimization via DCFR \citep{brown2019dcfr} building on CFR \citep{zinkevich2007cfr} and CFR$^+$ \citep{tammelin2014cfrplus}. Pluribus scales to multiplayer poker using efficient search with blueprints \citep{brown2019pluribus}. Public\textendash belief search is exemplified by ReBeL \citep{brown2020rebel}. For imperfect\textendash information planning, POMCP \citep{silver2010pomcp} and ISMCTS \citep{cowling2012ismcts} formalize information\textendash set root sampling. Continuous action exploration uses double progressive widening (DPW) \citep{couetoux2011dpw}. Risk measures follow Rockafellar and Uryasev \citep{rockafellar2000cvar}. Calibration metrics follow \citet{guo2017calibration}. Population training connects to PSRO \citep{lanctot2017psro}. AIVAT reduces evaluation variance \citep{burch2018aivat}. We use SMC fundamentals for joint particle updates \citep{doucet2001smc}. For stochastic approximation under dependent noise we reference \citet{borkar2008sa}. For I\textendash projections and exchangeability we use \citet{csiszar1975i,diaconis1980finite}. UCT is due to \citet{kocsis2006uct}. For CVaR in RL/planning see also \citet{chow2014cvar,tamar2015cvar}. For maximum entropy random fields and GIS see \citet{dellarfield1997pami}.

\section{Preliminaries}
We consider an $N$\textendash player NLHE extensive\textendash form game with imperfect information $(\States,\Actions,\Players,\IS,\Chance,u)$. A \emph{history} $h$ is a sequence of chance and player actions; a \emph{public state} $x$ is the projection of $h$ to public features (street, board, pot, stacks, action sequence, positions), written $x=\mathrm{proj}(h)$. When unambiguous, we write beliefs as functions of $x$ as shorthand for dependence on the public projection. Let $H_{-i}$ denote the joint private hands of opponents of player $i$ and $H_i$ the acting player's private hand (known to player $i$). A \emph{public belief state} (PBS) at time $t$ is $\rho_t(H_{-i}\mid h_t,H_i)$.

Notation: we use $\tilde H_{-i}$ to denote a sampled joint assignment of opponents' private hands drawn from a specified belief distribution; $H_{-i}$ without tilde denotes a symbolic variable or a concrete assignment in the support of a belief.

At $t=0$ we take $\rho_0$ to be an exchangeable prior over feasible joint assignments that is consistent with the deck, public cards, and $H_i$ (e.g., uniform over all blockers\textendash consistent $H_{-i}$); later updates preserve feasibility.

\section{Public subgames and admissible chance}
\label{sec:public_subgames}
\begin{definition}[Public subgame]
Given public state $x$ and acting player's hand $H_i$, the \emph{public subgame} $\mathcal{G}_{\text{pub}}(x,H_i)$ is the restriction of the EFG to histories consistent with $x$, whose root augments $x$ with a chance node that samples private cards for all \emph{opponents} from a distribution $q(H_{-i}\mid x,H_i)$ supported on hand assignments disjoint from the public board and $H_i$. The acting player’s private cards are known to themselves and are not sampled at the root. This root construction prevents leakage through chance: the acting player’s cards are fixed and not randomized at the subgame root.
\end{definition}

The public state $x$ fixes seat positions and action order. Hence full exchangeability of opponents is generally incompatible with $x$. We formalize symmetry via a subgroup $G(x)$ of permutations of opponent labels that preserves $x$ and the chance semantics.

\begin{definition}[Public\textendash state symmetry group]
Let $G(x)\subseteq S_{N-1}$ be the set of permutations $g$ of opponent indices such that: (i) the mapping $g$ preserves the public state $x$ (e.g., it permutes labels only among opponents that are indistinguishable under $x$), and (ii) card\textendash dealing feasibility (disjointness, blockers) is preserved under $g$ acting on $H_{-i}$. A distribution $q(\cdot\mid x,H_i)$ is \emph{$G(x)$\textendash invariant} if $q(H_{-i}\mid x,H_i)=q(g\cdot H_{-i}\mid x,H_i)$ for all $g\in G(x)$.
\end{definition}

\begin{assumption}[Belief\textendash as\textendash Chance Invariance (BCI)]
\label{ass:bci}
The root chance kernel $q(\cdot\mid x,H_i)$ is measurable w.r.t.\ the $\sigma$\textendash algebra generated by $(x,H_i)$, is supported on hands disjoint from the public board and $H_i$, and is $G(x)$\textendash invariant.
\end{assumption}

\paragraph{Concrete examples of $G(x)$ in 6\textendash max.}
- Postflop with two folded opponents and identical stack sizes behind the button: $G(x)$ may include the transposition of those folded seats since their labels are indistinguishable given $x$.
- Preflop UTG open, MP call, BTN fold, blinds to act: MP and blinds are not interchangeable due to position and action history; hence $G(x)$ is the identity (only) on those active labels.

\section{Method}
\subsection{Public information\textendash state encoder}
We represent $h_t$ as a sequence of tokenized events: blind postings, bets/raises (with a scalar size), calls, folds, checks; positional embeddings for seats; and card embeddings for board reveals. A transformer encoder outputs
\[
(\pi_\theta(\cdot\mid h_t),\; v_\theta(h_t)) \;=\; f_\theta(\text{tokens}(h_t), \text{masks}),
\]
where $\pi_\theta$ is a masked policy over legal actions (including a dynamic bet\textendash size menu; see \cref{sec:adaptive_abstraction}), and $v_\theta$ is a public\textendash state value.

\subsection{\sCRF\ opponent modeling and calibrated posteriors}
We use a conditional random field across streets with latent variables $Z^{(r)}$ (hand\textendash category/style) and observed action features $A^{(r)}$ (aggression, size vs.\ pot, pressure, SPR, position):
\[
p_\phi(Z^{(1:4)}, A^{(1:4)}\mid \text{ctx}) \;=\; \frac{1}{\mathcal{Z}} \prod_r \psi_r(Z^{(r)},A^{(r)},\text{ctx}^{(r)})\;\prod_{r=1}^{3}\psi_{r,r+1}(Z^{(r)},Z^{(r+1)}).
\]
Let $C(H)$ be a coarse hand\textendash category. We model $p_\phi(a\mid C,h_t)$ and induce $p(a\mid H,h_t)=p_\phi(a\mid C(H),h_t)$.

\paragraph{Units, base measures, and Jacobians for continuous sizes.}
We measure sizes primarily in \emph{pot\textendash fraction} units on each public state $x$, with the pot $P(x)$ fixed at $x$. When a density over sizes is used, the base measure is Lebesgue on pot\textendash fraction and the Radon--Nikodym derivative is taken consistently across the entire trajectory. If chip units are used instead, let $f_x:\text{chips}\to\text{pot\textendash fraction}$ be $f_x(a)=a/P(x)$; then $p_{\text{chips}}(a)=p_{\text{frac}}(f_x(a))\,|df_x/da|=p_{\text{frac}}(a/P(x))\cdot \tfrac{1}{P(x)}$. Switching between units without applying the Jacobian is prohibited.

\paragraph{Likelihood for continuous sizes and base measures.}
For belief updates with potentially continuous bet sizes, we use a categorical likelihood over a \emph{discretized size menu}. Specifically, each public state $x$ is associated with a finite size\textendash bucket map $\mathcal{B}^{\mathrm{BEL}}_x:\text{(legal sizes)}\to\{1,\ldots,B_x\}$ with codomain $\{1,\ldots,B_x\}$ that is \emph{fixed by the belief model and independent of the search’s dynamic action menu}. To avoid boundary ambiguity, we adopt a left\textendash closed, right\textendash open convention for bucket intervals in pot\textendash fraction space; the last bucket is closed on the right. Then
\[
p_\phi\big(a_t\mid H_j,h_t\big)\ :=\ p_\phi\big(\mathcal{B}^{\mathrm{BEL}}_x(a_t)\mid C(H_j),h_t\big),
\]
a proper probability mass function over buckets. Illegal actions have zero likelihood. If the bucketization changes at street boundaries, historical continuous sizes are deterministically mapped to the new buckets for likelihood consistency. Within any \emph{single simulated trajectory}, all likelihood factors use the \emph{same base measure}: we adopt the bucketed PMF by default, and all log\textendash likelihood increments in particle weight updates are interpreted under this discrete base. If a parametric \emph{density} over sizes is instead used, $p_\phi$ is interpreted as a density and the Bayes update uses the Radon--Nikodym derivative w.r.t.\ a \emph{single, fixed} base measure on size space (e.g., Lebesgue over pot\textendash fraction) for \emph{every} factor in the trajectory. Mixing PMFs and densities within one trajectory is prohibited unless an explicit absolute\textendash continuity relation and Jacobians are applied consistently to all likelihood factors.

\paragraph{Opponent bucket$\to$size mapping and blueprint$\to$bucket aggregation.}
To make within\textendash simulation actions well\textendash defined, we fix for each public state $x$:
- A representative mapping $r_x:\{1,\dots,B_x\}\to \text{(legal sizes at }x)$ that maps each belief bucket index $b$ to a concrete legal size $a=r_x(b)$. We define $r_x$ in \emph{pot\textendash fraction} space and take $r_x(b)$ to be the \emph{bucket midpoint} in pot\textendash fraction units, then convert to chips by multiplying with the current $P(x)$ and clipping to the legal range. This computes representatives in the same base measure as the likelihood and converts units \emph{only} at execution time, avoiding base\textendash measure inconsistencies across the trajectory. We assume leaf/rollout values are \emph{Lipschitz continuous} in the pot\textendash fraction size; midpoint representatives then minimize worst\textendash case bucketization error at fixed bucket width. Importantly, $r_x$ is used \emph{only} by the environment to instantiate concrete legal sizes for simulated play; it never appears in any likelihood factor, which remains defined on the fixed bucket PMF throughout the trajectory.
- An aggregation operator $A_x$ that maps any policy over a concrete size set $\mathcal{U}_x$ to a policy over buckets: for $b\in\{1,\dots,B_x\}$,
\[
[A_x(\pi)](b)\ :=\ \sum_{a\in\mathcal{U}_x:\ \mathcal{B}^{\mathrm{BEL}}_x(a)=b}\ \pi(a).
\]
In particular, if $\pi_\theta(\cdot\mid x)$ is defined on the current concrete action menu, then $A_x(\pi_\theta)(\cdot\mid x)$ is a PMF over belief buckets.

\paragraph{Joint belief update (actor\textendash specific) with feasibility and stable numerics.}
Let $\mathcal{H}(x,H_i)$ be the set of feasible joint assignments consistent with $x$ (disjointness, blockers) and disjoint from $H_i$. Let $j$ be the acting opponent at $t$ and $a_t$ the observed action. The joint belief is updated by
\begin{equation}
\label{eq:joint_update}
\rho_{t+1}(H_{-i}\mid h_{t+1},H_i) \;\propto\; \underbrace{\1\{H_{-i}\in \mathcal{H}(x_t,H_i)\}}_{\text{feasibility}}\;\rho_t(H_{-i}\mid h_t,H_i)\; p_\phi\!\big(a_t \mid H_j, h_t\big),
\end{equation}
where $H_j$ is $j$’s private hand in the joint assignment $H_{-i}$ and $x_t=\mathrm{proj}(h_t)$. Here and throughout, $\rho_t(\cdot\mid h_t,H_i)$ denotes the \emph{pre\textendash update} posterior after incorporating all likelihood factors strictly before time $t$ (i.e., based on $a_{<t}$), so that \eqref{eq:joint_update} correctly applies the new factor for $a_t$. For a sequence of opponent actions $a_{1:k}$ with actors $j_{1:k}$ and evolving public histories $h_{1:k}$,
\begin{equation}
\label{eq:multi_update}
\rho_{t+k}(H_{-i}\mid h_{t+k},H_i)\ \propto\ \1\{H_{-i}\in \mathcal{H}(x_{t+k},H_i)\}\;\rho_t(H_{-i}\mid h_t,H_i)\ \prod_{r=1}^k p_\phi\!\big(a_r\mid H_{j_r},h_r\big).
\end{equation}
If $a_t$ is a fold, we multiply by $p_\phi(\mathrm{fold}\mid H_j,h_t)$ and mark seat $j$ as \emph{inactive for future action likelihoods}, but we \emph{retain} $H_j$ in the joint assignment since folded cards continue to constrain chance reveals and showdowns (blockers).

We implement \eqref{eq:joint_update} with a \emph{joint} particle filter over $H_{-i}$. Let $\ell_m$ denote \emph{log\textendash weights} and $w_m$ the corresponding linear weights. Upon observing $a_t$, update $\ell_m\leftarrow \ell_m+\log p_\phi(a_t\mid H_j^{(m)},h_t)$, recenter by subtracting $\max_m \ell_m$ to mitigate under/overflow, then form $w_m=\exp(\ell_m)$. For numerical safety either clip $\ell_m\ge L_{\min}$ before exponentiation (e.g., $L_{\min}=-745$ under IEEE double), or after exponentiation floor $w_m\leftarrow \max\{w_m,\varepsilon\}$ in \emph{linear} space (e.g., $\varepsilon=10^{-300}$); do not mix domains.

We compute normalized weights $\tilde w_m=w_m/\sum_{\ell=1}^M w_\ell$ using log\textendash sum\textendash exp, and the (unnormalized) effective sample size
\[
\mathrm{ESS}\ =\ \frac{1}{\sum_{m=1}^M \tilde w_m^2}\ \in [1,M],\qquad \text{or}\quad \mathrm{nESS}\ =\ \frac{\mathrm{ESS}}{M}\ \in\Big[\frac{1}{M},1\Big].
\]
We trigger systematic resampling when $\mathrm{nESS}<\kappa$ (default $\kappa=0.5$). This avoids the misleading phrase ``normalized ESS'' for $\mathrm{ESS}$.

\paragraph{MCMC rejuvenation, detailed balance, and ergodicity.}
After reweighting (and any resampling), we apply a Metropolis\textendash Hastings kernel that preserves feasibility. We mix three proposal types with fixed mixing weights: (i) category\textendash preserving swaps; (ii) cross\textendash category local moves; (iii) global feasibility replacements. Let the (unnormalized) current target be
\[
\pi_t(H_{-i})\ \propto\ \1\{H_{-i}\in \mathcal{H}(x_t,H_i)\}\,\rho_t(H_{-i}\mid h_t,H_i)\; p_\phi\!\big(a_t \mid H_j, h_t\big),
\]
which explicitly uses the pre\textendash update posterior $\rho_t(\cdot\mid h_t,H_i)$ and the new likelihood factor for $a_t$. For proposal density $q_{\text{prop}}$, accept with probability
\[
\alpha=\min\!\left\{1,\ \frac{\pi_t(H'_{-i})}{\pi_t(H_{-i})} \cdot \frac{q_{\text{prop}}(H_{-i}\mid H'_{-i})}{q_{\text{prop}}(H'_{-i}\mid H_{-i})}\right\}.
\]
Each MH kernel satisfies detailed balance w.r.t.\ $\pi_t$; a finite mixture of such kernels with fixed weights preserves $\pi_t$; aperiodicity holds since proposals can be rejected. For irreducibility on the finite feasible support it suffices that the global replacement proposes any blockers\textendash consistent per\textendash seat hand with probability bounded away from $0$. Then any two feasible joint assignments are connected by a finite sequence of single\textendash seat replacements (and occasional swaps) with nonzero probability.

\subsection{Consistency\textendash Projected Beliefs (CPB)}
\label{sec:cpb}
Learned $\rho_t$ may violate realizability (e.g., due to independent approximations) or symmetry implied by $x$. We project to a realizable, $G(x)$\textendash invariant exponential family that is also feasible given $H_i$.

\begin{definition}[CPB via maximum entropy relative to a symmetric base]
\label{def:cpb}
Let $\mathcal{H}(x,H_i)$ be the set of feasible $H_{-i}$ consistent with $x$ (disjointness, blockers) and disjoint from $H_i$. Let $m_{x,H_i}$ be a $G(x)$\textendash invariant base measure on $\mathcal{H}(x,H_i)$ (e.g., uniform over $\mathcal{H}(x,H_i)$). Let $S_1,\dots,S_K$ be permutation\textendash invariant sufficient statistics of $H_{-i}$ measurable w.r.t.\ $(x,H_i)$ (e.g., per\textendash seat category marginals aggregated over $G(x)$\textendash orbits, blocker counts). Define $\widehat s_k=\E_{\rho_t(\cdot\mid h_t,H_i)}[S_k]$ computed using the joint particle set (with the rejuvenation kernel held fixed during this computation). The CPB is the I\textendash projection
\begin{equation}
\label{eq:cpb_me}
\tilde\rho_t \;=\; \arg\min_{q\in \mathcal{P}(\mathcal{H}(x,H_i))}\KL\!\big(q \,\|\, m_{x,H_i}\big)\quad 
\text{s.t.}\quad \E_q[S_k]=\widehat s_k \ \ \forall k,\ \ \text{and}\ \ q\ \text{is }G(x)\text{-invariant}.
\end{equation}
By convex duality \citep{csiszar1975i}, any feasible instance (nonempty moment\textendash constraint set) admits an optimizer of exponential form
\begin{equation}
\label{eq:exp_form}
\tilde\rho_t(H_{-i}\!\mid\!x,H_i) \;=\; \frac{m_{x,H_i}(H_{-i})\exp\!\Big(\sum_{k=1}^K \lambda_k S_k(H_{-i})\Big)}{\sum_{H' \in \mathcal{H}(x,H_i)} m_{x,H_i}(H')\exp\!\big(\sum_k \lambda_k S_k(H')\big)}\,,
\end{equation}
with Lagrange multipliers $\{\lambda_k\}$ chosen to match the moments. The $G(x)$\textendash invariance follows from the invariance of $m_{x,H_i}$ and $\{S_k\}$.
\end{definition}

\begin{proposition}[BCI under CPB]
\label{prop:bci}
The CPB $\tilde\rho_t$ in \eqref{eq:cpb_me} is measurable w.r.t.\ $(x,H_i)$ (and a fixed SMC seed during a re\textendash solve) and is $G(x)$\textendash invariant. Hence it satisfies Assumption~\ref{ass:bci}.
\end{proposition}
\begin{proof}
Measurability holds because the constraints and $m_{x,H_i}$ depend only on $(x,H_i)$ (and the fixed particle estimator for $\widehat s$). Invariance: for any $g\in G(x)$, $m_{x,H_i}(g\!\cdot\!H)=m_{x,H_i}(H)$ and $S_k(g\!\cdot\!H)=S_k(H)$ by construction, hence \eqref{eq:exp_form} is unchanged under $g$.
\end{proof}

\paragraph{Discrete realization on particle support and exact invariance via orbit completion.}
The feasible space $\mathcal{H}(x,H_i)$ is combinatorially large. In practice we realize $\tilde\rho_t$ on a \emph{finite} support $\mathcal{S}_t=\{H^{(1)}_{-i},\ldots,H^{(M)}_{-i}\}$ given by the current joint particle cloud after feasibility pruning. We define the orbit\textendash completed support $\mathcal{S}_t^{\text{orb}}=\bigcup_{H\in\mathcal{S}_t}\{g\!\cdot\!H:\ g\in G(x)\}$ and adopt a \emph{discrete base} $\hat m_{x,H_i}$ that is \emph{uniform over $\mathcal{S}_t^{\text{orb}}$} (or proportional to empirical proposal frequencies averaged over each orbit). Orbit completion and $\hat m_{x,H_i}$ are deterministic functions of $(x,H_i)$ and the fixed seed used to generate $\mathcal{S}_t$, preserving measurability and equivariance under canonicalization.

We then solve
\[
\tilde\rho_t \;=\; \arg\min_{q\in \Delta(\mathcal{S}_t^{\text{orb}})} \KL\!\big(q\,\|\,\hat m_{x,H_i}\big)\quad\text{s.t.}\ \E_q[S_k]=\widehat s_k,\ \forall k,\ \text{and }q\text{ is }G(x)\text{-invariant on }\mathcal{S}_t^{\text{orb}}.
\]
To enforce exact invariance, we reparameterize directly in the \emph{orbit basis} (one tied parameter per orbit), which keeps the optimization well\textendash conditioned and avoids oscillatory projections.

\paragraph{Moment feasibility; penalized dual, default, and solver notes.}
On a finite random support $\mathcal{S}_t^{\text{orb}}$, the exact constrained problem with $\gamma=0$ has a solution iff $\widehat s\in \mathrm{conv}\{S(H):H\in\mathcal{S}_t^{\text{orb}}\}$. With empirical moments from finite SMC samples, feasibility will almost surely fail without slack. We therefore \emph{solve the penalized dual by default}. Let $Z(\lambda)=\sum_{H\in\mathcal{S}_t^{\text{orb}}}\hat m_{x,H_i}(H)\exp(\sum_k \lambda_k S_k(H))$. Minimize the \emph{strongly convex} objective
\[
\mathcal{L}(\lambda)\ =\ \log Z(\lambda)\ -\ \sum_{k=1}^K \lambda_k \widehat s_k\ +\ \frac{\gamma}{2}\|\lambda\|_2^2,\qquad \gamma>0,
\]
whose gradient is $\nabla \mathcal{L}(\lambda)=\E_{q_\lambda}[S]-\widehat s+\gamma\lambda$ with $q_\lambda$ from \eqref{eq:exp_form} realized on $\mathcal{S}_t^{\text{orb}}$. For any $\gamma>0$, $\mathcal{L}$ has a \emph{unique} minimizer $\lambda^\star$; stationarity reads $\E_{q_{\lambda^\star}}[S]=\widehat s-\gamma\lambda^\star$. As $\gamma\downarrow 0$, the solution recovers the exact constraints whenever feasible. We compute $\log Z(\lambda)$ via log\textendash sum\textendash exp, pre\textendash center/scale $\{S_k\}$ to unit variance under $\hat m_{x,H_i}$, and note that, after standardization and because each $S_k$ is bounded on the finite support, $\nabla \log Z(\lambda)$ is Lipschitz on the orbit\textendash tied domain, which justifies stable stepsizes for mirror descent/GIS \citep{beck2003mirror,darroch1972gis}. In practice, monitor the residual $\| \E_{q_{\lambda}}[S]-\widehat s\|_2$; if it exceeds a tolerance and the particle ESS is high, either enlarge the orbit\textendash completed support (e.g., additional rejuvenation or targeted proposals) or reduce $\gamma$ along a \emph{continuation schedule}; decrease $\gamma$ only when nESS $\ge \kappa$ and the current moment residual is below a tolerance $\tau$. Otherwise accept the penalized solution. Report both the residual norm and an \emph{orbit coverage ratio} (fraction of orbits represented in $\mathcal{S}_t^{\text{orb}}$) as diagnostics to decide between more sampling vs.\ smaller $\gamma$. The orbit\textendash tied parameterization reduces the optimization dimension to the number of orbits, often much smaller than $|\mathcal{S}_t^{\text{orb}}|$.

\subsection{Leakage\textendash Free Public\textendash Subgame CFR$^+$}
\label{sec:lfps}
We re\textendash solve $\mathcal{G}_{\text{pub}}(x,H_i)$ using CFR$^+$ with root chance $q=\tilde\rho_t(\cdot\mid x,H_i)$.

\begin{definition}[Group action on public states and actions]
Let $G(x)$ act on public states by permuting opponent labels that are indistinguishable under $x$. The induced action\textendash space action maps: (i) seat\textendash indexed actions (who acts) by the same permutation; (ii) scalar bet sizes (e.g., pot fractions) trivially, since magnitudes are public scalars; and (iii) dynamic menus by label\textendash consistent relabeling. A menu generator $W$ is \emph{equivariant} if $W(g\!\cdot\!x;\xi)=g\!\cdot W(x;\xi)$ for all $g\in G(x)$ and fixed seed $\xi$.
\end{definition}

\begin{lemma}[Permutation equivariance of regret\textendash matching$+$]
\label{lem:rmplus_equiv}
Regret\textendash matching$+$ maps instantaneous regret vectors $r\in\mathbb{R}^A$ to mixed strategies $\sigma=\mathrm{RM}^+(r)\in\Delta(\{1,\dots,A\})$ via nonnegative regret truncation and proportional allocation. For any permutation $\pi$ of actions, $\mathrm{RM}^+(\Pi r)=\Pi\,\mathrm{RM}^+(r)$, where $\Pi$ is the permutation matrix. Thus regret\textendash matching$+$ is permutation\textendash equivariant.
\end{lemma}
\begin{proof}
Truncation and summation commute with permutation: $(\Pi r)^+=\Pi r^+$ and $\sum_a (\Pi r)^+_a=\sum_a r^+_a$. The proportional mapping is homogeneous in $r^+$ and the division by $\sum_a r^+_a$ commutes with permutation because both numerator and denominator are permuted coherently. Hence equivariance holds.
\end{proof}

\begin{lemma}[Label\textendash agnostic randomized mechanisms are $G(x)$\textendash invariant]
\label{lem:rand_inv}
Let $\Xi$ be the space of public trajectories from $x$ and let $S$ be a random seed independent of opponent labels. If a randomized mechanism (e.g., PUCT tie\textendash breaking, DPW proposals/bandit acceptances) is a measurable function $F:\Xi\times S\to$ outcomes that depends only on the public trajectory and $S$, then for all $g\in G(x)$ the distribution of outcomes is invariant under $g$.
\end{lemma}

\paragraph{Orbit\textendash averaged feature maps are equivariant.}
Let $\phi(x)$ be any seat\textendash indexed feature vector computable from the public state $x$. Define its orbit\textendash averaged version $\bar\phi(x)$ by averaging over each $G(x)$\textendash orbit of indistinguishable seats (followed by concatenation in a canonical orbit order). Then for all $g\in G(x)$, $\bar\phi(g\!\cdot\!x)=\bar\phi(x)$. Hence any mechanism that consumes only $\bar\phi(x)$ is $G(x)$\textendash equivariant.

\begin{theorem}[Group\textendash invariance of re\textendash solving]
\label{thm:noleak_group}
Fix a public state $x$ and acting hand $H_i$, and let $q(\cdot\mid x,H_i)$ satisfy Assumption~\ref{ass:bci}. Consider running CFR$^+$ on $\mathcal{G}_{\text{pub}}(x,H_i)$ with chance $q$. Let $\bar\sigma_T$ be the $T$\textendash iteration average strategy at the root information set of the acting player. Suppose further that: (a) the initialization (regrets/strategy) is $G(x)$\textendash invariant; (b) any randomness (e.g., sampling tie\textendash breakers, traversal order) is label\textendash agnostic as in \cref{lem:rand_inv}; (c) ties are broken by a symmetry\textendash preserving canonical action ordering derived from a seed computed on a canonicalized public path; (d) dynamic action menus are generated by an equivariant $W$; (e) updates use regret\textendash matching$+$, which is permutation\textendash equivariant (\cref{lem:rmplus_equiv}); and (f) \emph{all features and thresholds used by $W$, DPW proposals, and bandit acceptance are functions of public $G(x)$\textendash invariant statistics} (e.g., orbit\textendash averaged features), with deterministic seeds computed from canonicalized public paths that are constant on $G(x)$\textendash orbits. Then for any $g\in G(x)$, the action distribution of $\bar\sigma_T$ at $x$ is invariant under $g$, for all $T\ge 1$. Consequently, the limit strategy (if it exists) is $G(x)$\textendash invariant and cannot encode private\textendash information leakage beyond $(x,H_i)$.
\end{theorem}
\begin{proof}
Define a relabeling operator $\Pi_g$ acting jointly on (i) opponent hands and (ii) the entire public\textendash subgame tree, including node creation and dynamic action menus. By $G(x)$\textendash invariance of $q$ and hypotheses (b)\textendash(f) together with \cref{lem:rand_inv} and the orbit\textendash averaging observation, $\Pi_g$ preserves the chance kernel and the (random) data\textendash dependent menus pathwise up to relabeling. Counterfactual values and instantaneous regrets are equivariant under $\Pi_g$, and regret\textendash matching$+$ updates are permutation\textendash equivariant at each information set (\cref{lem:rmplus_equiv}), preserving symmetry across iterations. Induction over iterations yields invariance of per\textendash iteration and averaged root strategies.
\end{proof}

\noindent Note: CFR/CFR$^+$ variants lack general convergence guarantees in multiplayer general\textendash sum games; LF\textendash PS\textendash CFR$^+$ is a symmetry\textendash preserving local improvement heuristic rather than a solver with exploitability or equilibrium guarantees in the full game. In particular, we do not impose safe/nested subgame constraints on opponents’ counterfactual values.

\subsection{Belief\textendash MCTS with CVaR\textendash UCT and progressive widening}
\label{sec:belief_mcts}
We plan on public states with \emph{information\textendash set root sampling} \citep{silver2010pomcp,cowling2012ismcts}. Within \emph{each} simulation we \emph{freeze} a \emph{root} CPB:
\[
q_{\text{root}}(\cdot)\ :=\ \tilde\rho_t(\cdot\mid x_{\text{root}},H_i)\,,
\]
and all subsequent resampling in that simulation uses the \emph{exact conditional} of this \emph{same} $q_{\text{root}}$ given the observed public trajectory and feasibility constraints.

\paragraph{Opponent generative policy for within\textendash simulation actions.}
To ensure conditional consistency at IS\textendash RS boundaries, \emph{all opponent actions during a simulation} are sampled from a fixed, known generative policy over belief buckets,
\[
\pi_{\mathrm{opp}}(b\mid x)\ :=\ (1-\eta)\,[A_x(\pi_\theta)](b\mid x)\ +\ \eta\,p_\phi^{\text{marg}}\!\big(b\mid x\big),\quad b\in\{1,\dots,B_x\},
\]
renormalized over legal buckets; $\eta\in[0,1]$ trades blueprint and belief and $A_x$ aggregates the blueprint to buckets. Here we define the \emph{H\textendash independent} marginal
\[
p_\phi^{\text{marg}}(b\mid x)\ =\ \sum_{c} p_\phi(b\mid c,x)\,\pi_c(x),
\]
where $p_\phi(b\mid c,x)$ is the \sCRF\ bucket likelihood conditioned on coarse hand\textendash category $c$ and public state $x$, and $\pi_c(x)$ is a fixed, public\textendash state\textendash dependent category prior. To preserve $G(x)$\textendash invariance, we take $\pi_c(x)$ to be either a pooled public marginal or per\textendash seat marginals averaged over $G(x)$\textendash orbits (e.g., CPB\textendash derived public marginals \emph{frozen at the root} and then orbit\textendash averaged). By construction $p_\phi^{\text{marg}}(b\mid x)$ depends only on $x$ (and the fixed root seed if used) and is independent of opponents’ private hands $H_{-i}$. The environment uses the representative mapping $a=r_x(b)$ to convert the sampled bucket into a concrete legal size. The acting player’s actions are chosen by the MCTS selection rule on the current concrete menu; non\textendash acting opponents are \emph{always} sampled via $\pi_{\mathrm{opp}}$.

\paragraph{Normalization and bounded returns.}
Let $C_i(x)$ be chips player $i$ has already contributed to the pot at $x$, $P(x)$ the current pot, and $S_k(x)$ the remaining stack for player $k$. We measure the terminal payoff $R$ \emph{from the root onward}, excluding already committed chips; that is, $R$ is the net change in $i$'s chip count from decisions and chance events \emph{after} $x_{\mathrm{root}}$. Then
\[
-S_i(x_{\mathrm{root}})\ \le\ R\ \le\ P(x_{\mathrm{root}})\ +\ \sum_{j\neq i} \min\{S_i(x_{\mathrm{root}}),\,S_j(x_{\mathrm{root}})\}.
\]
We normalize by the deterministic bound
\begin{equation}
\label{eq:normC}
C(x_{\mathrm{root}})\ :=\ \max\!\Big\{\,S_i(x_{\mathrm{root}}),\ \ P(x_{\mathrm{root}})\ +\ \sum_{j\neq i} \min\{S_i(x_{\mathrm{root}}),\,S_j(x_{\mathrm{root}})\}\,\Big\},
\end{equation}
and set $X=R/C(x_{\mathrm{root}})\in[-1,1]$ and losses $Y=-X\in[-1,1]$. All return statistics and backups from this root use the \emph{same} bound $C(x_{\mathrm{root}})$ to maintain comparability across depths. When $v_\theta(x)$ is used for leaf evaluation, we normalize it to $[-1,1]$ under the same $C(x_{\mathrm{root}})$. (Opponents already all\textendash in at $x_{\mathrm{root}}$ contribute $\min\{S_i,S_j\}=0$ to the upper bound, consistent with side\textendash pot caps.) If desired for numerical conditioning, a tighter deterministic bound per street may be used as long as a \emph{single} bound is fixed for all backups from the same root, preserving the normalized range and comparability across depths.

\begin{lemma}[Asymmetric payoff bounds and tightness under from\textendash now baseline]
\label{lem:bound}
From any root state $x_{\mathrm{root}}$, when $R$ is measured from the root onward (excluding already contributed chips), the payoff satisfies
\[
-S_i(x_{\mathrm{root}})\ \le\ R\ \le\ P(x_{\mathrm{root}})+\sum_{j\neq i}\min\{S_i(x_{\mathrm{root}}),S_j(x_{\mathrm{root}})\}.
\]
The upper bound is tight when all remaining players go all\textendash in and $i$ wins the entire main pot and all side pots consistent with effective stacks; the lower bound is tight when $i$ commits their entire remaining stack and loses.
\end{lemma}
\begin{proof}[Proof sketch]
Chip conservation with side pots implies that for each opponent $j$, the maximum additional gain by $i$ beyond the current pot is bounded by $\min\{S_i,S_j\}$. Summation and adding the \emph{current pot} gives the upper bound; tightness holds in the described all\textendash in/win scenario. For losses measured from now, the maximum additional loss by $i$ is at most $S_i$, hence the lower bound $-S_i$; tightness holds when $i$ commits $S_i$ and loses all contributions.
\end{proof}

\paragraph{Belief\textendash conditioned priors and scaling.}
We mix encoder priors with a belief\textendash conditioned prior $\pi_{\text{bel}}$:
\[
\tilde P(a\mid x)\ =\ (1-\omega_t)\,\pi_\theta(a\mid x)\ +\ \omega_t\,\pi_{\text{bel}}(a\mid x)\,,
\]
restricted to the active action menu $\mathcal{B}_x$ and \emph{renormalized} over $\mathcal{B}_x$ after any widening/pruning. We set
\[
\pi_{\text{bel}}(a\mid x)\ \propto\ \exp\!\big(\kappa\,\widehat Q(x,a)\big)\cdot \1\{a\in\mathcal{B}_x\},\quad
\widehat Q(x,a)\ :=\ \E_{\tilde H_{-i}\sim \tilde\rho_t}\!\left[\hat X\big(x,a;\tilde H_{-i}\big)\right],
\]
where $\hat X\in[-1,1]$ is a normalized rollout value (or one\textendash step lookahead into $v_\theta$) obtained under a fixed default policy. To avoid the exploration bonus being dominated by a sharply peaked prior when $\lambda$ is large, we either (i) temperature\textendash flatten the prior with a default $\tau_{\text{prior}}\in[1.5,2.0]$ before renormalization, or (ii) use a logarithmic prior bonus variant, e.g., replacing $\tilde P$ by $\log(1+\tau\,\tilde P)$ in the PUCT term. We co\textendash scale $c_{\text{puct}}\approx 1+\lambda$ to keep exploitation/exploration magnitudes comparable; cap $\lambda$ to ensure the exploitation and exploration terms remain on comparable scales in practice. Maintain a minimum blueprint mass $\omega_{\min}>0$ to ensure persistent exploration of all retained actions.

\begin{assumption}[Rollout, menu, and randomization symmetry]
\label{ass:rollout_sym}
For symmetry preservation (\cref{thm:noleak_group}), the default rollout policy and all its randomization are $G(x)$\textendash invariant and independent of opponent labels. All random seeds used by DPW/bandits/tie\textendash breaking are label\textendash agnostic and are measurable functions of the public trajectory only (e.g., derived deterministically from a hash of a \emph{canonicalized} public path that is constant on $G(x)$\textendash orbits).
\end{assumption}

\paragraph{Risk\textendash sensitive selection.}
Let $\mathcal{D}_{x,a}$ be the empirical distribution of backed\textendash up returns on edge $(x,a)$. We maintain per\textendash edge estimates: mean $\hat\mu_{x,a}=\E[X]$, and a streaming estimate of $\CVaR_\alpha(Y)$. The selection score is
\begin{equation}
\label{eq:puct_cvar}
U(x,a)\;=\;\underbrace{\hat\mu_{x,a}-\lambda\,\widehat{\CVaR}_\alpha(Y)}_{\widehat{J}_\alpha(x,a)\in[-1-\lambda,\,1+\lambda]}\;+\;c_{\text{puct}}\;\tilde P(a\mid x)\;\frac{\sqrt{\sum_{b\in\mathcal{B}_x} N(x,b)}}{1+N(x,a)}\,,
\end{equation}
where $\lambda\!\ge\!0$ tunes risk aversion. Since $Y\in[-1,1]$ are losses, $\CVaR_\alpha(Y)\in[-1,1]$, yielding $\widehat{J}_\alpha\in[-1-\lambda,1+\lambda]$. The exploration term satisfies $0\le c_{\text{puct}}\tilde P(\cdot)\sqrt{\sum_b N}/(1+N)\le c_{\text{puct}}\sqrt{\sum_b N}$.

\paragraph{Streaming CVaR estimator with explicit surrogate (per edge).}
For backed\textendash up losses $Y_n\in[-1,1]$ on $(x,a)$, minimize the Rockafellar--Uryasev surrogate \citep{rockafellar2000cvar}
\[
\mathcal{R}_\alpha(t)\ =\ t\ +\ \frac{1}{1-\alpha}\,\E[(Y-t)_+]
\]
by \emph{two\textendash time\textendash scale} stochastic approximation \citep[Ch.~6]{borkar2008sa}: let $a_n$ update the threshold $\widehat t_n$ and $b_n$ update the auxiliary estimate $\widehat m_n\approx \E[(Y-\widehat t)_+]$, with $a_n/b_n\to 0$ so that $\widehat t$ is the \emph{slower} timescale and $\widehat m$ tracks it on the \emph{faster} timescale. Use
\[
\widehat t_{n+1}=\Pi_{[-1,1]}\!\left[\widehat t_n-a_n\!\left(1-\frac{1}{1-\alpha}\big(\1\{Y_n>\widehat t_n\}+\tfrac{1}{2}\1\{Y_n=\widehat t_n\}\big)\right)\right],
\]
\[
\widehat m_{n+1}=\Pi_{[0,2]}\!\left[\widehat m_n+b_n\!\left((Y_n-\widehat t_n)_+ - \widehat m_n\right)\right],
\]
with stepsizes satisfying $\sum_n a_n=\infty$, $\sum_n a_n^2<\infty$, $\sum_n b_n=\infty$, $\sum_n b_n^2<\infty$, and $a_n/b_n\to 0$. A concrete admissible schedule is $a_n=\frac{c_a}{(n+n_0)^{0.9}}$, $b_n=\frac{c_b}{(n+n_0)^{0.6}}$ with $c_b\ge 10\,c_a>0$ and $n_0\ge 10$; initialize $t_0$ to the empirical $\alpha$\textendash quantile from a short warm\textendash up and $m_0=0$. Since $Y\in[-1,1]$ and $t\in[-1,1]$, we have $(Y-t)_+\in[0,2]$, motivating the projection $\Pi_{[0,2]}$ for $\widehat m$. This clamping is consistent with the bounded loss range and prevents drift; Polyak--Ruppert averaging \citep{polyak1992pr} after a burn\textendash in reduces variance. We explicitly form the streaming tail\textendash risk estimate as
\begin{equation}
\label{eq:cvar_estimator}
\widehat{\CVaR}_\alpha(Y)\ :=\ \Pi_{[-1,1]}\!\left(\,\widehat t\ +\ \frac{\widehat m}{1-\alpha}\,\right),
\end{equation}
which is asymptotically correct as $(\widehat t,\widehat m)$ stabilize and numerically stable during transients due to clamping.

\paragraph{Assumption (Markov noise and sufficient visitation).}
For each retained edge $(x,a)$, the per\textendash visit loss sequence $\{Y_n\}$ is generated by a time\textendash homogeneous Markov chain (induced by the environment dynamics and the current selection/rollout policies) that is ergodic conditional on a fixed policy and action menu; furthermore, each retained edge is visited infinitely often almost surely. Under this \emph{Markov noise} model, two\textendash timescale SA converges to a stationary point of the RU surrogate \citep[Ch.~6]{borkar2008sa}. Our exploration mechanisms below ensure infinite visitation.

\paragraph{IS\textendash RS boundaries and exact conditionals.}
Let $\mathcal{F}^{\mathrm{pub}}_t$ be the $\sigma$\textendash algebra generated by the public trajectory up to time $t$ (public player actions and chance outcomes, including board cards), but \emph{excluding} the acting player's internal randomization. Within each simulation, we \emph{freeze} $q_{\mathrm{root}}=\tilde\rho_t(\cdot\mid x_{\mathrm{root}},H_i)$ at the root draw, sample $\tilde H_{-i}^{(0)}\sim q_{\mathrm{root}}$, and resample opponent private hands \emph{only after new public \emph{chance} observations} (board reveals), never at the acting player's decision points. Before any reveal likelihood is applied, we \emph{intersect the feasible support with the observed reveal} (assignments inconsistent with the revealed cards receive zero weight and are removed). The exact conditional used at IS\textendash RS boundaries is then
\begin{equation}
\label{eq:exact_conditional}
q_{\mathrm{root}}(H_{-i}\mid \mathcal{F}^{\mathrm{pub}}_t, H_i)\ \propto\ q_{\mathrm{root}}(H_{-i})\ \prod_{r\le t:\ \text{actor }j_r\neq i} \pi_{\mathrm{opp}}\!\big(b_r\mid x_r\big),
\end{equation}
where $b_r=\mathcal{B}^{\mathrm{BEL}}_{x_r}(a_r)$ are observed buckets for opponent actions. Since $\pi_{\mathrm{opp}}(\cdot\mid x)$ is measurable w.r.t.\ the public history and \emph{independent of} $H_{-i}$, the product factor is constant in $H_{-i}$ and cancels during normalization; opponent actions do not update the posterior beyond feasibility constraints. For public chance reveals, let $D_{\mathrm{rem}}(H_{-i})$ be the set of remaining undealt cards given $x$ and $H_{-i}$. Revealing $k$ new public cards uniformly at random without order yields
\[
\mathbb{P}(\text{reveal}\mid H_{-i})\ =\ \binom{|D_{\mathrm{rem}}(H_{-i})|}{k}^{-1},
\]
and $|D_{\mathrm{rem}}(H_{-i})|$ is \emph{constant} across all feasible $H_{-i}$ (the number of reserved private cards per seat is fixed by $x$). Hence such reveal likelihoods are equal across feasible assignments and cancel in Bayes’ rule as well. If cards are revealed \emph{in order} (e.g., turn then river), the ordered reveal likelihood factorizes as a product of sequential denominators that are also constant across feasible $H_{-i}$, so cancellation still holds.

\begin{lemma}[Ordered public reveals yield $H$\textendash independent likelihoods]
\label{lem:ordered_reveal}
Let $k\ge 1$ and suppose the public reveal consists of an ordered sequence of $k$ distinct cards drawn uniformly without replacement from $D_{\mathrm{rem}}(H_{-i})$. Then
\[
\mathbb{P}(\text{ordered reveal}\mid H_{-i})\ =\ \frac{1}{|D_{\mathrm{rem}}(H_{-i})|}\cdot \frac{1}{|D_{\mathrm{rem}}(H_{-i})|-1}\cdots \frac{1}{|D_{\mathrm{rem}}(H_{-i})|-k+1},
\]
which depends on $H_{-i}$ only through $|D_{\mathrm{rem}}(H_{-i})|$. If the number of reserved private cards per seat is fixed by $x$ (true in NLHE), then $|D_{\mathrm{rem}}(H_{-i})|$ is constant across all feasible $H_{-i}$, hence the ordered reveal likelihood is $H$\textendash independent and cancels in the conditional after the feasibility intersection.
\end{lemma}
\begin{proof}
Uniform sampling without replacement yields the stated product. Feasibility fixes the number of already\textendash reserved private cards and the public board; thus the size of the remaining deck is constant across feasible assignments, implying $H$\textendash independence.
\end{proof}

Seats that have folded remain in the joint assignment for blocker consistency but are \emph{permanently} marked as non\textendash actors; the feasibility mask used in conditional sampling changes only on public chance reveals and hard constraints (e.g., showdown revelations), not on opponent actions under the $H$\textendash independent generator.

On a finite particle support $\mathcal{S}_t^{\text{orb}}$, we maintain log\textendash weights with periodic renormalization for numerical stability along long action sequences. For efficiency, resampling at opponent actions is optional and typically omitted since it is a no\textendash op for the posterior under $H$\textendash independent $\pi_{\mathrm{opp}}$.

\begin{lemma}[Unbiased returns under IS\textendash RS with frozen root CPB and known opponent generator]
\label{lem:isrs}
Fix a public root state $x_{\mathrm{root}}$, acting hand $H_i$, and $q_{\mathrm{root}}=\tilde\rho_t(\cdot\mid x_{\mathrm{root}},H_i)$. In each simulation, sample $\tilde H_{-i}^{(0)}\sim q_{\mathrm{root}}$ at the root; along the trajectory, sample all non\textendash acting opponents’ actions from the known bucket generator $\pi_{\mathrm{opp}}(b\mid x)$ and resample opponent private hands only at IS\textendash RS boundaries from the exact conditional \eqref{eq:exact_conditional}; and do not resample at the acting player’s decision points. Let the acting player’s within\textendash simulation selection rule be any measurable and non\textendash anticipative function of the public trajectory and the current internal tree state (which may depend on returns from prior simulations, but not on future chance or private information). Then, conditioning on the (random) current selection rule, the backed\textendash up return is an unbiased estimator of the root expected value under $q_{\mathrm{root}}$ against $\pi_{\mathrm{opp}}$.
\end{lemma}
\begin{proof}[Proof sketch]
Condition on the acting player’s current selection policy $\pi$ (which may be nonstationary across simulations) and on the realized public filtration. The simulated private hands are drawn from the disintegration of $q_{\mathrm{root}}$ w.r.t.\ $\mathcal{F}^{\mathrm{pub}}_t$; likelihood factors equal the \emph{true} opponent generative probabilities $\pi_{\mathrm{opp}}(b_r\mid x_r)$, which are $H_{-i}$\textendash independent and cancel in the conditional. Hence the simulated trajectory distribution matches the environment distribution for private uncertainty under $\pi$ versus $\pi_{\mathrm{opp}}$, conditional on the public path. By the law of iterated expectations, the trajectory return has the correct conditional mean given $\pi$ and the public path.
\end{proof}

\paragraph{When the opponent generator depends on private hands.}
If opponent actions are sampled from a policy that depends on private hands, i.e., $\pi_{\mathrm{opp}}(b\mid x,H_{j})$, then the exact conditional becomes
\[
q_{\mathrm{root}}(H_{-i}\mid \mathcal{F}^{\mathrm{pub}}_t, H_i)\ \propto\ q_{\mathrm{root}}(H_{-i})\ \prod_{r\le t:\ \text{actor }j_r\neq i} \pi_{\mathrm{opp}}\!\big(b_r\mid x_r, H_{j_r}\big),
\]
and opponent action likelihoods no longer cancel. Unbiased estimation of root values against an \emph{environment} policy $\pi_{\mathrm{env}}(b\mid x,H_j)$ requires either (i) sampling with $\pi_{\mathrm{opp}}=\pi_{\mathrm{env}}$ and using the exact conditional above, or (ii) importance weighting simulated returns by the likelihood ratio
\[
W\ =\ \prod_{r:\ j_r\neq i}\ \frac{\pi_{\mathrm{env}}(b_r\mid x_r, H_{j_r})}{\pi_{\mathrm{opp}}(b_r\mid x_r, H_{j_r})},
\]
which can exhibit high variance. We \emph{avoid} this regime by using an $H$\textendash independent $\pi_{\mathrm{opp}}$ for planning; if ever needed, prefer clipped or self\textendash normalized weights for mean estimation and do not claim unbiased CVaR estimates without additional analysis of tail\textendash sensitivity under importance sampling. In particular, unlike means, CVaR estimates are not generally unbiased (or even consistent) under naive importance sampling without stringent tail conditions.

\paragraph{DPW with bandit gating, support, and coverage.}
At a node with visit count $N(x)$ and current continuous\textendash action set $\mathcal{B}_x$, propose a new candidate size if $|\mathcal{B}_x|<k_a N(x)^{\alpha_a}$ \citep{couetoux2011dpw}. Candidates are drawn from a log\textendash spaced proposal on $[a_{\min},a_{\max}]$ in pot\textendash fraction units plus \{all\textendash in\}, with a density that \emph{dominates} Lebesgue measure on $[a_{\min},a_{\max}]$ (strictly positive on every nondegenerate interval). The acceptance mechanism is an explicit mixture: with probability $\varepsilon_N=c/(N+1)^\beta$ (with $\beta\in(0,1]$, $c>0$; e.g., $\beta=1$ gives $\sum_N \varepsilon_N=\infty$), \emph{force\textendash accept} the proposal; with the remaining probability, a contextual bandit (e.g., LinUCB/Thompson \citep{li2010linucb}) computes an acquisition score and accepts candidates whose UCB exceeds a threshold. Proposal and acceptance features are $G(x)$\textendash invariant so that menu evolution preserves symmetry. Priors used by PUCT are always renormalized over the active $\mathcal{B}_x$.

\begin{assumption}[Exploration lower bound with dominated coupling]
\label{ass:bandit}
There exists $\epsilon(N)>0$ with $\sum_{n=1}^\infty \epsilon(n)=\infty$ such that, for each proposal at node visit count $N$, the bandit’s acceptance indicator $A_N$ can be coupled on a common probability space with an \emph{independent} Bernoulli$(\epsilon(N))$ random variable $B_N$ so that $A_N\ge B_N$ almost surely.
\end{assumption}
\begin{proposition}[Asymptotic coverage under bandit\textendash gated DPW]
Under Assumption~\ref{ass:bandit}, $|\mathcal{B}_x|\to\infty$ as $N(x)\to\infty$, and the realized growth rate is $\Omega(N(x)^{\alpha_a'})$ for some $\alpha_a'\in(0,\alpha_a]$. Moreover, if the proposal distribution has full support and there exists $\delta_I>0$ such that, for every nondegenerate interval $I\subset[a_{\min},a_{\max}]$, proposals fall in $I$ infinitely often and $\mathbb{P}(A_N=1\mid \text{proposal}\in I)\ge \delta_I$ for infinitely many $N$, then with probability $1$ every open interval $I\subset[a_{\min},a_{\max}]$ receives infinitely many accepted sizes (hence the accepted set is dense).
\end{proposition}
\begin{proof}[Proof sketch]
Infinitely many acceptances follow from the dominated coupling and $\sum_N \epsilon(N)=\infty$ via a Borel--Cantelli argument. Density follows by applying the same reasoning on a countable base of rational intervals covering $[a_{\min},a_{\max}]$ and using full support with a uniform lower bound on acceptance probabilities on each interval infinitely often.
\end{proof}

\paragraph{Sufficient per\textendash edge visits for SA.}
To ensure that the streaming estimators on each retained edge $(x,a)$ receive infinitely many updates despite dynamic menus, we adopt the following sufficient condition. Once an action $a$ enters $\mathcal{B}_x$, it cannot be pruned unless a dominance criterion is met and, even then, it is reintroduced with nonvanishing probability by DPW. During selection at node $x$, apply an exploration wrapper that, with probability $\xi_{N(x)}=c'/(N(x)+1)^{\beta'}$ for some $c'>0$, $\beta'\in(0,1]$, chooses an action among the \emph{least\textendash visited} ones at $x$ uniformly at random (breaking ties by the canonical ordering), and otherwise uses \eqref{eq:puct_cvar}. Additionally, assume a \emph{nonvanishing reintroduction rate}: every previously pruned action is proposed with probability at least $\zeta_N=c''/(N+1)^{\beta''}$ for infinitely many $N$ for some $c''>0$, $\beta''\in(0,1]$. With $\sum_n \xi_n=\infty$ and $\sum_n \zeta_n=\infty$, this guarantees infinitely many visits to every retained action almost surely, supporting the SA conditions for the per\textendash edge mean and CVaR estimators under Markov noise \citep{borkar2008sa}.

\begin{algorithm}[t]
\caption{IS\textendash RS Belief\textendash MCTS\textendash \sCRF\ (selection $\to$ expansion $\to$ evaluation)}
\label{alg:bmcrf}
\begin{adjustbox}{width=\linewidth}
\begin{minipage}{\linewidth}
\begin{algorithmic}[1]
\State \textbf{Input:} public root history $h_t$ (state $x_{\mathrm{root}}$), acting hand $H_i$, \emph{frozen root CPB} $q_{\mathrm{root}}=\tilde\rho_t(\cdot\mid h_t,H_i)$ over discrete support $\mathcal{S}_t^{\text{orb}}$, encoder $f_\theta$, \sCRF\ $p_\phi$, opponent generator $\pi_{\mathrm{opp}}$, menu generator $W(\cdot;\xi)$, risk level $\alpha$
\State \textbf{Symmetry discipline:} All proposal features and random seeds are $G(x)$\textendash invariant and label\textendash agnostic; the default rollout policy satisfies Assumption~\ref{ass:rollout_sym}. Seeds are derived from a canonicalized public path; see Implementation notes for the canonicalization rule.
\For{simulation $s=1,\dots,S$}
  \State $x \gets h_t$;\; draw $\tilde H_{-i}\sim q_{\mathrm{root}}$ via masked tree sampling or alias+rejection
  \While{$x$ not terminal}
    \If{$x$ not in tree}
      \State initialize children with legal actions and bet\textendash size menu per $W(\cdot;\xi)$
      \State set raw priors $\tilde P_{\text{raw}}(\cdot\mid x)\!\leftarrow\!(1-\omega_t)\pi_\theta(\cdot\mid x)+\omega_t \pi_{\text{bel}}(\cdot\mid x)$
      \State restrict to active menu $\mathcal{B}_x$ and renormalize: $\tilde P(\cdot\mid x)\propto \tilde P_{\text{raw}}(\cdot\mid x)\cdot \1\{\cdot\in\mathcal{B}_x\}$
      \State evaluate leaf via rollout or normalized $v_\theta(x)$; initialize $\hat\mu,\widehat t,\widehat m$; \textbf{break}
    \Else
      \If{player to act is $i$}
        \State with prob. $\xi_{N(x)}$ select a least\textendash visited $a\in\mathcal{B}_x$ uniformly; otherwise select $a \leftarrow \arg\max_{a\in\mathcal{B}_x} U(x,a)$ where $U$ is \eqref{eq:puct_cvar}
      \Else
        \State sample opponent bucket $b \sim \pi_{\mathrm{opp}}(\cdot\mid x)$ and set concrete size $a\gets r_x(b)$
      \EndIf
      \State $x \leftarrow \text{Step}(x,a,\tilde H_{-i},H_i)$; propose a new size if DPW triggers; accept via mixture with forced accept prob.\ $\varepsilon_{N(x)}$
      \If{IS\textendash RS boundary (new public \emph{chance} observation)}
         \State resample $\tilde H_{-i}\sim q_{\mathrm{root}}(\cdot\mid \mathcal{F}^{\mathrm{pub}}_t,H_i)$ via masked sampling or feasibility\textendash preserving MCMC
      \EndIf
    \EndIf
  \EndWhile
  \State $\hat v \leftarrow$ rollout or normalized $v_\theta(x)$; backpropagate and update $\hat\mu,\widehat t,\widehat m$ for all traversed edges
\EndFor
\State \Return action via temperature\textendash smoothed visit counts
\end{algorithmic}
\end{minipage}
\end{adjustbox}
\end{algorithm}

\subsection{Public\textendash Subgame\textendash Constrained CFR$^+$}
When triggers occur (e.g., river nodes with small SPR), we spawn a budgeted CFR$^+$ on $\mathcal{G}_{\text{pub}}(x,H_i)$ with $q=\tilde\rho_t(\cdot\mid x,H_i)$. We use regret\textendash matching$+$ with linear averaging and regret floor $0$, warm\textendash started from $(\pi_\theta,v_\theta)$. The returned local policy replaces priors $\tilde P(\cdot\mid x)$ (renormalized over the current $\mathcal{B}_x$) and reinitializes edge statistics while preserving visit counts. Invariance follows from \cref{thm:noleak_group}. Use full tree sweeps or symmetric sampling schedules with fixed label\textendash agnostic seeds to preserve equivariance.

\subsection{Adaptive action abstraction}
\label{sec:adaptive_abstraction}
Maintain at each node a small menu $\mathcal{B}_x$ of bet sizes (fractions of pot or all\textendash in). A contextual bandit chooses which sizes to include given $G(x)$\textendash invariant features (SPR, player count, board texture, pot). Candidates with low usage and high estimated regret are pruned; promising sizes are added. Hysteresis prevents churn: a size must exceed threshold $\tau_{\text{on}}$ for $T_{\text{on}}$ visits to enter, and remain above $\tau_{\text{off}}<\tau_{\text{on}}$ for at least $T_{\text{off}}$ visits to persist. Pruning never reduces $\mathcal{B}_x$ below a fixed seed set to preserve search viability. DPW with sufficient exploration ensures asymptotic coverage. On menu changes, preserve visit counts $N(x,a)$ for existing actions; reset or warm\textendash start per\textendash edge SA states for CVaR if the Jaccard similarity between old and new menus drops below a threshold $\tau$ (default $\tau=0.4$), or if the action identity (size) drifts beyond a tolerance (default: relative pot\textendash fraction difference $<5\%$ to warm\textendash start, else reset), to avoid carrying over mismatched statistics. To avoid feedback instability, cap per\textendash decision changes in $\omega_t$ and $\lambda$ by a small $\delta$ (e.g., $|\Delta \omega_t|,|\Delta \lambda|\le \delta$) and enforce a minimum blueprint mass $\omega_t\ge \omega_{\min}>0$.

\subsection{RCAS: Range\textendash Calibration\textendash Aware Search}
\label{sec:rcas}
Let $\mathrm{ECE}_t\in[0,1]$ be a running ECE estimate and $\mathsf{H}_t$ the normalized entropy of $\rho_t$. Define
\[
g(\mathrm{ECE}_t,\mathsf{H}_t)\;=\;\max\!\Big\{0,\; 1 - \frac{\mathrm{ECE}_t-\tau_e^-}{\tau_e^+-\tau_e^-}\Big\}\cdot \max\!\Big\{0,\; 1 - \frac{\mathsf{H}_t-\tau_h^-}{\tau_h^+-\tau_h^-}\Big\},
\]
with $0\le \tau_e^-<\tau_e^+\le 1$, $0\le \tau_h^-<\tau_h^+\le 1$. We clip $g$ to $[0,1]$ and apply exponential smoothing with factor $\beta\in(0,1)$. Hysteresis dampens oscillations: once $g$ crosses down below $g_{\min}$, we reduce $\lambda\to \gamma\lambda$ and set $\omega_t\to \gamma\omega_t$ for $\Delta T$ steps, where $\gamma\in(0,1)$. Co\textendash anneal $c_{\text{puct}}\approx 1+\lambda$ to preserve the exploitation/exploration balance. With $c_{\text{puct}}>0$ and $\omega_{\min}>0$, PUCT maintains nonzero exploration of suboptimal arms.

\paragraph{Online ECE estimator.}
We maintain $B$ fixed probability bins on $[0,1]$ and track exponentially weighted counts of predicted confidences and empirical accuracies for categorical events derived from the belief model (e.g., per\textendash category hits). A bias\textendash corrected ratio yields per\textendash bin calibration gaps; ECE is the weighted average gap.

\section{Complexity analysis}
Let $S$ be the number of simulations per decision, $d$ the effective depth, and $\bar b$ the average branching after DPW. Let $N$ be the number of players and $M$ the number of joint particles.

- Belief sampling and update: building a tree or alias table for $\tilde\rho_t$ over $\mathcal{S}_t^{\text{orb}}$ costs $O(M)$ per CPB refresh; drawing a joint particle for root determinization is $O(\log M)$ with a Fenwick/segment tree or $O(1)$ expected via alias\textendash rejection when the feasible mass is not too small. \emph{Upon each opponent action observation}, updating log\textendash weights via \eqref{eq:joint_update} costs $O(M)$ overall (one table lookup and addition per particle, with cached per\textendash bucket, per\textendash category log\textendash likelihoods reducing the constant but not the order), plus $O(M)$ for periodic resampling and optional MH rejuvenation. Between CPB refreshes, the total cost across a decision is $O(S d M)$ in the worst case.

- Conditional sampling from $q_{\mathrm{root}}$: masked sampling is $O(\log M)$ per draw with a Fenwick/segment tree supporting point updates for feasibility masks; alias\textendash rejection has $O(1/m)$ expected draws when the feasible mass fraction is $m$. Under frequent feasibility changes, a segment tree with lazy propagation is a robust alternative to maintain masks and support toggling in $O(\log M)$; periodically compact the active support to reduce overhead. The alias table must be rebuilt when the active support changes; batch toggles to amortize rebuilds.

- Selection/backups: Per step, PUCT computations are $O(|\mathcal{B}_x|)$; CVaR updates are $O(1)$; total $O(Sd\bar b)$. Renormalizing priors over $\mathcal{B}_x$ is $O(|\mathcal{B}_x|)$ per widening/pruning event, or amortized $O(1)$ if cumulative normalizers are maintained.

- DPW bandit: computing a LinUCB score is $O(p)$ with $p$ features; proposing candidates happens $O(Sd)$ times; acceptances are sublinear by DPW; overhead $O(Sdp)$. Forced\textendash acceptance with $\varepsilon_N=c/(N+1)^\beta$ keeps overhead within budget for $\beta\in(0,1]$.

- CPB projection (discrete): for $K$ statistics, $T$ dual iterations, and $M$ particles, $O(TM K)$ per refresh (performed sparsely, e.g., on street changes or ESS drops). Precompute $S(H)$ per orbit and maintain an alias/segment tree over \emph{orbit masses} to accelerate evaluations of $Z(\lambda)$ and $\nabla \log Z(\lambda)$. Orbit completion multiplies $M$ by at most the size of $G(x)$'s largest orbit (typically small in NLHE due to limited indistinguishability postflop); the orbit\textendash basis reparameterization keeps optimization in the reduced space.

- LF\textendash PS\textendash CFR$^+$: with $K_{\text{CFR}}$ iterations, average local branching $\bar b_{\text{sub}}$, and information\textendash sets $I_{\text{sub}}$, cost is $O(K_{\text{CFR}} I_{\text{sub}}\bar b_{\text{sub}})$; typically small due to endgame triggers and abstraction.

Memory: the search tree stores $O(Sd)$ nodes with per\textendash edge statistics $(\hat\mu,\widehat t,\widehat m,N)$ and action menus; the SMC maintains $O(M)$ joint particles; the orbit\textendash completed support costs at most a small constant factor $|\mathrm{orbit}_{\max}|$ (often close to 1) over $M$.

\section{Limitations and convergence caveats}
- CVaR\textendash UCT: We do not claim convergence to a fixed point; the objective is non\textendash linear and samples are non\textendash i.i.d./stationary due to adaptive policies. Our use follows risk\textendash aware selection heuristics with two\textendash timescale SA tracking \citep{borkar2008sa}. The estimator \eqref{eq:cvar_estimator} is asymptotically correct under SA conditions and Markov noise; during transients clamping to $[-1,1]$ maintains numerical stability consistent with return normalization. Post\textendash hoc off\textendash policy evaluation of CVaR should be performed only under carefully controlled logging policies or with specialized variance\textendash control techniques; unbiasedness for CVaR should not be assumed in general.

- IS\textendash RS: Unbiasedness guarantees apply to expected values under the \emph{frozen} belief\textendash as\textendash chance model (\cref{lem:isrs}) \emph{against the explicit opponent generator} $\pi_{\mathrm{opp}}$; extensions to $\CVaR$ are heuristic. When the opponent generator depends on private hands, unbiased estimation without variance blow\textendash up is delicate; we avoid this regime.

- CPB: The choice of statistics $S_k$ trades fidelity and tractability. Using too few moments may underfit; too many may overconstrain. Our construction guarantees $G(x)$\textendash invariance and realizability given $(x,H_i)$ but does not enforce consistency with any particular opponent strategy profile beyond the matched moments. The re\textendash solve optimizes the acting player’s local policy against the projected belief; RCAS modulates reliance on such beliefs. Penalization ($\gamma>0$) induces moment bias $\E_q[S]=\widehat s-\gamma\lambda^\star$; we mitigate it via continuation only when nESS and residual diagnostics indicate adequate support coverage.

\section{Implementation notes and scope}
- Step function and feasibility: The transition operator $\text{Step}(x,a,\tilde H_{-i},H_i)$ enforces legality and feasibility (stack caps, side\textendash pot formation, turn order). When a non\textendash acting opponent bucket $b$ is sampled, the environment uses $a=r_x(b)$ as the concrete legal size. Payoff normalization uses the same $C(x_{\mathrm{root}})$ in \eqref{eq:normC}. If $v_\theta(x)$ outputs unnormalized chip units, we divide by $C(x_{\mathrm{root}})$ to map leaf evaluations into $[-1,1]$.

- IS\textendash RS conditionals: Within a single simulation, freeze $q_{\mathrm{root}}$ at the time of the root sample. All resampling uses conditionals $q_{\mathrm{root}}(\cdot\mid \mathcal{F}^{\mathrm{pub}}_t,H_i)$ as in \eqref{eq:exact_conditional}. Since $\pi_{\mathrm{opp}}$ is $H$\textendash independent, opponent action likelihoods cancel; only feasibility from public chance reveals affects conditionals. For ordered or unordered reveals of $k$ cards uniformly from the remaining deck, first intersect the feasible set with the reveal; the likelihood factors are then common across feasible $H_{-i}$ and cancel during normalization (\cref{lem:ordered_reveal}). Resampling at opponent actions is optional and can be omitted to save computation. Seats eliminated by fold remain in the joint assignment for blockers and never act again.

- Likelihood discretization, base measure, and folds: The belief\textendash model bucketization $\mathcal{B}^{\mathrm{BEL}}_x$ is fixed independently of the search menu; historical sizes are mapped deterministically and order\textendash preservingly for consistent likelihoods, with left\textendash closed, right\textendash open intervals (last bucket closed on the right). Within a trajectory we use a PMF likelihood throughout. When a seat folds, we apply one likelihood factor for the fold and then mark the seat inactive for future action likelihoods while \emph{retaining} its cards in the joint assignment to preserve blockers for chance.

- Symmetry in practice and canonicalization: To preserve \cref{thm:noleak_group}, avoid per\textendash seat identifiers in bandit features; use only $G(x)$\textendash invariant aggregates (or explicitly average features across $G(x)$\textendash orbits). Derive label\textendash agnostic seeds by hashing a \emph{canonicalized} public trajectory: within each $G(x)$\textendash orbit of indistinguishable seats, relabel seats by increasing currently contributed chips, breaking ties by position (earlier to act first), then by remaining stack; as a final tiebreaker, use lexicographic order of their public action subsequences. Serialize the canonical public path (street, pot, stacks, action types and sizes after sorting indistinguishable actors) before hashing. This canonicalization map is constant on $G(x)$\textendash orbits, ensuring all derived seeds/tie\textendash breaks are orbit\textendash invariant. Traversal orders and tie\textendash breaking use this same seed and canonical ordering.

- CVaR maintenance and exploration: Maintain per\textendash edge SA iterates with admissible stepsizes; initialize $t_0$ via a short warm\textendash up $\alpha$\textendash quantile and $m_0=0$; use Polyak averaging after burn\textendash in; reset or warm\textendash start when menu/policy regimes change per \cref{sec:adaptive_abstraction}. Projections $t\in[-1,1]$, $m\in[0,2]$ are consistent with the bounded loss range. To guarantee infinitely many visits for SA updates, enable the selection exploration wrapper with $\xi_{N(x)}=c'/(N(x)+1)^{\beta'}$ and keep $\omega_{\min}>0$; ensure a nonvanishing reintroduction rate for pruned actions.

- Conditional sampling data structures: Use a Fenwick/segment tree with lazy masks to support feasibility toggles in $O(\log M)$; rebuild alias tables only when the active support actually changes and batch toggles to amortize rebuilds.

\section{Evaluation roadmap}
This revision focuses on mathematical and algorithmic correctness and does not report empirical results. A future empirical version will provide: (i) toy CPB projections on reduced decks with feasibility diagnostics and exact orbit completion; (ii) CVaR estimator sanity checks under discrete/continuous return mixtures; and (iii) ablations against blueprint and belief\textendash blind baselines, with AIVAT\textendash adjusted estimates \citep{burch2018aivat}. A single simulation script (simulation.py) will generate all figures, save them locally, and write key numerical outputs to results.txt for reproducibility; all numbers in the paper will be traceable to this file.

\section{Conclusion}
Belief\textendash MCTS\textendash \sCRF\ combines structured opponent modeling, belief\textendash aware search, risk\textendash sensitive selection, public\textendash subgame re\textendash solving, and adaptive action sets into a cohesive solver. We corrected the belief update for multi\textendash opponent settings with bucketed likelihoods independent of search menus and a single trajectory\textendash wide base measure; formalized CPB with a symmetric base and group\textendash invariance while conditioning on the acting player's hand (including discrete feasibility on a finite support, exact invariance via orbit completion with a clarified base measure, orbit\textendash basis tying, and dual optimization with uniqueness for $\gamma>0$ and Lipschitz gradients after standardization); proved leakage\textendash free re\textendash solving under public\textendash state symmetries (with explicit conditions on randomized mechanisms, equivariant menu mappings, permutation\textendash equivariant regret\textendash matching$+$, and induced action\textendash space actions); established a corrected \emph{asymmetric} payoff bound and normalization under a from\textendash now baseline; clarified IS\textendash RS unbiasedness under a \emph{frozen} root CPB with exact conditionals aligned to a \emph{known} $H$\textendash independent opponent generator, and explained the general $H$\textendash dependent case and its importance weighting with CVaR caveats; specified fold handling that preserves blocker effects; added sufficient conditions for infinite per\textendash edge visits to support SA; made the CVaR estimator explicit with a clamped, two\textendash timescale surrogate and corrected selection\textendash score bounds; tightened the chance\textendash reveal likelihood cancellation to cover ordered reveals with explicit feasibility intersection; refined complexity claims with the correct $O(M)$ per\textendash observation belief update and concrete conditional\textendash sampling data structures and thresholds; and added a Lipschitz assumption with midpoint representatives to bound bucketization error. These ingredients integrate cleanly with self\textendash play/CFR pipelines and are designed to scale from heads\textendash up to 6\textendash max Hold'em.

\small
\begin{thebibliography}{99}

\bibitem[Morav\v{c}\'{\i}k et~al.(2017)]{moravcik2017deepstack}
M.~Morav\v{c}\'{\i}k, M.~Schmid, N.~Burch, V.~Lis\'{y}, D.~Morrill, N.~Bard, T.~Davis, K.~Waugh, M.~Johanson, and M.~Bowling.
\newblock DeepStack: Expert-level artificial intelligence in heads-up no-limit poker.
\newblock \emph{Science}, 356(6337):508--513, 2017.
\newblock doi:10.1126/science.aam6960.

\bibitem[Brown and Sandholm(2018)]{brown2018safe}
N.~Brown and T.~Sandholm.
\newblock Safe and nested subgame solving for imperfect-information games.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, 32(1), 2018.
\newblock arXiv:1705.10818.

\bibitem[Brown and Sandholm(2019a)]{brown2019dcfr}
N.~Brown and T.~Sandholm.
\newblock Solving imperfect-information games via discounted CFR.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, 33(01):1829--1836, 2019.
\newblock arXiv:1812.07587.

\bibitem[Zinkevich et~al.(2007)]{zinkevich2007cfr}
M.~Zinkevich, M.~Johanson, M.~Bowling, and C.~Piccione.
\newblock Regret minimization in games with incomplete information.
\newblock In \emph{Advances in Neural Information Processing Systems}, 20:1723--1730, 2007.

\bibitem[Tammelin(2014)]{tammelin2014cfrplus}
O.~Tammelin.
\newblock Solving large imperfect information games using CFR$^+$.
\newblock \emph{arXiv preprint arXiv:1407.5042}, 2014.

\bibitem[Brown and Sandholm(2019b)]{brown2019pluribus}
N.~Brown and T.~Sandholm.
\newblock Superhuman AI for multiplayer poker.
\newblock \emph{Science}, 365(6456):885--890, 2019.
\newblock doi:10.1126/science.aay2400.

\bibitem[Brown et~al.(2020)]{brown2020rebel}
N.~Brown, A.~Bakhtin, A.~Lerer, and Q.~Gong.
\newblock Combining deep reinforcement learning and search for imperfect-information games.
\newblock \emph{Nature}, 588:542--547, 2020.
\newblock doi:10.1038/s41586-020-3038-4.

\bibitem[Silver and Veness(2010)]{silver2010pomcp}
D.~Silver and J.~Veness.
\newblock Monte-Carlo planning in large POMDPs.
\newblock In \emph{Advances in Neural Information Processing Systems}, 23:2125--2133, 2010.

\bibitem[Cowling et~al.(2012)]{cowling2012ismcts}
P.~I. Cowling, E.~J. Powley, and D.~Whitehouse.
\newblock Information set Monte Carlo tree search.
\newblock \emph{IEEE Transactions on Computational Intelligence and AI in Games}, 4(2):120--143, 2012.
\newblock doi:10.1109/TCIAIG.2012.2196810.

\bibitem[Cou\"{e}toux et~al.(2011)]{couetoux2011dpw}
A.~Cou\"{e}toux, J.-B. Hoock, N.~Sokolovska, O.~Teytaud, and N.~Bonnard.
\newblock Continuous upper confidence trees.
\newblock In C.~A.~C. Coello (ed.), \emph{Learning and Intelligent Optimization}, pp. 433--445. Springer, 2011.
\newblock doi:10.1007/978-3-642-25566-3\_9.

\bibitem[Rockafellar and Uryasev(2000)]{rockafellar2000cvar}
R.~T. Rockafellar and S.~Uryasev.
\newblock Optimization of conditional value-at-risk.
\newblock \emph{Journal of Risk}, 2(3):21--41, 2000.
\newblock doi:10.21314/JOR.2000.038.

\bibitem[Guo et~al.(2017)]{guo2017calibration}
C.~Guo, G.~Pleiss, Y.~Sun, and K.~Q. Weinberger.
\newblock On calibration of modern neural networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine Learning}, pp. 1321--1330, 2017.

\bibitem[Lanctot et~al.(2017)]{lanctot2017psro}
M.~Lanctot, V.~Zambaldi, A.~Gruslys, A.~Lazaridou, J.~P\'{e}rolat, K.~Tuyls, D.~Silver, and T.~Graepel.
\newblock A unified game-theoretic approach to multiagent reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 30:4190--4203, 2017.

\bibitem[Burch et~al.(2018)]{burch2018aivat}
N.~Burch, M.~Johanson, and M.~Bowling.
\newblock AIVAT: A new variance reduction technique for agent evaluation in imperfect information games.
\newblock In \emph{Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems}, pp. 1417--1425, 2018.

\bibitem[Doucet et~al.(2001)]{doucet2001smc}
A.~Doucet, N.~de~Freitas, and N.~Gordon (eds.).
\newblock \emph{Sequential Monte Carlo Methods in Practice}.
\newblock Springer, 2001.

\bibitem[Borkar(2008)]{borkar2008sa}
V.~S. Borkar.
\newblock \emph{Stochastic Approximation: A Dynamical Systems Viewpoint}.
\newblock Hindustan Book Agency and Cambridge University Press, 2008.

\bibitem[Csisz\'{a}r(1975)]{csiszar1975i}
I.~Csisz\'{a}r.
\newblock I-divergence geometry of probability distributions and minimization problems.
\newblock \emph{The Annals of Probability}, 3(1):146--158, 1975.
\newblock doi:10.1214/aop/1176996454.

\bibitem[Diaconis and Freedman(1980)]{diaconis1980finite}
P.~Diaconis and D.~Freedman.
\newblock Finite exchangeable sequences.
\newblock \emph{The Annals of Probability}, 8(4):745--764, 1980.
\newblock doi:10.1214/aop/1176994669.

\bibitem[Kocsis and Szepesv\'{a}ri(2006)]{kocsis2006uct}
L.~Kocsis and C.~Szepesv\'{a}ri.
\newblock Bandit based Monte-Carlo planning.
\newblock In \emph{Proceedings of the 17th European Conference on Machine Learning}, pp. 282--293, 2006.
\newblock doi:10.1007/11871842\_29.

\bibitem[Chow and Ghavamzadeh(2014)]{chow2014cvar}
Y.~Chow and M.~Ghavamzadeh.
\newblock Algorithms for {CVaR} optimization in {MDPs}.
\newblock In \emph{Advances in Neural Information Processing Systems}, 27:3509--3517, 2014.
\newblock arXiv:1402.0633.

\bibitem[Tamar et~al.(2015)]{tamar2015cvar}
A.~Tamar, Y.~Glassner, and S.~Mannor.
\newblock Optimizing the {CVaR} via sampling.
\newblock In \emph{Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence}, pp. 2993--2999, 2015.

\bibitem[Della Pietra et~al.(1997)]{dellarfield1997pami}
S.~Della Pietra, V.~Della Pietra, and J.~Lafferty.
\newblock Inducing features of random fields.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 19(4):380--393, 1997.
\newblock doi:10.1109/34.588021.

\bibitem[Darroch and Ratcliff(1972)]{darroch1972gis}
J.~N. Darroch and D.~Ratcliff.
\newblock Generalized iterative scaling for log-linear models.
\newblock \emph{The Annals of Mathematical Statistics}, 43(5):1470--1480, 1972.
\newblock doi:10.1214/aoms/1177692379.

\bibitem[Beck and Teboulle(2003)]{beck2003mirror}
A.~Beck and M.~Teboulle.
\newblock Mirror descent and nonlinear projected subgradient methods for convex optimization.
\newblock \emph{Operations Research Letters}, 31(3):167--175, 2003.
\newblock doi:10.1016/S0167-6377(02)00231-6.

\bibitem[Li et~al.(2010)]{li2010linucb}
L.~Li, W.~Chu, J.~Langford, and R.~E. Schapire.
\newblock A contextual-bandit approach to personalized news article recommendation.
\newblock In \emph{Proceedings of the 19th International Conference on World Wide Web}, pp. 661--670, 2010.
\newblock doi:10.1145/1772690.1772758.

\bibitem[Polyak and Juditsky(1992)]{polyak1992pr}
B.~T. Polyak and A.~B. Juditsky.
\newblock Acceleration of stochastic approximation by averaging.
\newblock \emph{SIAM Journal on Control and Optimization}, 30(4):838--855, 1992.
\newblock doi:10.1137/0330046.

\end{thebibliography}
\end{document}