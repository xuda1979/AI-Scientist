@inproceedings{Bender2021StochasticParrots,
  author    = {Emily M. Bender and Timnit Gebru and Angelina McMillan-Major and Shmargaret Shmitchell},
  title     = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  year      = {2021},
  pages     = {610--623},
  publisher = {ACM},
  doi       = {10.1145/3442188.3445922}
}
@article{Ji2023HallucinationSurvey,
  author  = {Zhengbao Ji and Zhiwei Wang and Fuli Feng and Xueqi Cheng and Tat-Seng Chua},
  title   = {A Survey on Hallucination in Natural Language Generation},
  journal = {ACM Computing Surveys},
  year    = {2023},
  volume  = {55},
  number  = {12},
  pages   = {1--38},
  doi     = {10.1145/3571730}
}
@inproceedings{Lin2022TruthfulQA,
  author    = {Stephanie Lin and Jacob Hilton and Owain Evans},
  title     = {TruthfulQA: Measuring How Models Mimic Human Falsehoods},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  year      = {2022},
  pages     = {3214--3252},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2022.acl-long.229}
}
@article{OpenAI2023GPT4,
  author  = {{OpenAI}},
  title   = {GPT-4 Technical Report},
  journal = {arXiv preprint arXiv:2303.08774},
  year    = {2023},
  doi     = {10.48550/arXiv.2303.08774}
}
@article{Hoffmann2022Chinchilla,
  author  = {Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and others},
  title   = {Training Compute-Optimal Large Language Models},
  journal = {arXiv preprint arXiv:2203.15556},
  year    = {2022},
  doi     = {10.48550/arXiv.2203.15556}
}
@article{Rae2021Gopher,
  author  = {Jack W. Rae and Sebastian Borgeaud and Trevor Cai and others},
  title   = {Scaling Language Models: Methods, Analysis \& Insights from Training Gopher},
  journal = {arXiv preprint arXiv:2112.11446},
  year    = {2021},
  doi     = {10.48550/arXiv.2112.11446}
}
@article{Brown2020GPT3,
  author  = {Tom B. Brown and Benjamin Mann and Nick Ryder and others},
  title   = {Language Models are Few-Shot Learners},
  journal = {arXiv preprint arXiv:2005.14165},
  year    = {2020},
  doi     = {10.48550/arXiv.2005.14165}
}
@article{Ouyang2022RLHF,
  author  = {Long Ouyang and Jeff Wu and Xu Jiang and others},
  title   = {Training Language Models to Follow Instructions with Human Feedback},
  journal = {arXiv preprint arXiv:2203.02155},
  year    = {2022},
  doi     = {10.48550/arXiv.2203.02155}
}
@article{Bai2022ConstitutionalAI,
  author  = {Yuntao Bai and Andy Jones and Kamal Ndousse and others},
  title   = {Constitutional AI: Harmlessness from AI Feedback},
  journal = {arXiv preprint arXiv:2212.08073},
  year    = {2022},
  doi     = {10.48550/arXiv.2212.08073}
}
@article{Weidinger2021EthicalRisks,
  author  = {Laura Weidinger and John Mellor and Maribeth Rauh and others},
  title   = {Ethical and Social Risks of Harm from Language Models},
  journal = {arXiv preprint arXiv:2112.04359},
  year    = {2021},
  doi     = {10.48550/arXiv.2112.04359}
}
@inproceedings{Carlini2021ExtractingTrainingData,
  author    = {Nicholas Carlini and Florian Tram{\`e}r and Eric Wallace and others},
  title     = {Extracting Training Data from Large Language Models},
  booktitle = {30th USENIX Security Symposium (USENIX Security 21)},
  year      = {2021},
  pages     = {2633--2650},
  note      = {arXiv:2012.07805}
}
@article{Carlini2023PoisoningWebScale,
  author  = {Nicholas Carlini and Matthew Jagielski and Nicholas Papernot and others},
  title   = {Poisoning Web-Scale Training Datasets Is Practical},
  journal = {arXiv preprint arXiv:2304.10418},
  year    = {2023},
  doi     = {10.48550/arXiv.2304.10418}
}
@article{Perez2022RedTeam,
  author  = {Ethan Perez and Sam Ringer and Kamil Kosi{\'n}ski and others},
  title   = {Red Teaming Language Models with Language Models},
  journal = {arXiv preprint arXiv:2209.07858},
  year    = {2022},
  doi     = {10.48550/arXiv.2209.07858}
}
@article{Zou2023UniversalJailbreaks,
  author  = {Andy Zou and Zifan Wang and Xiangru Tang and others},
  title   = {Universal and Transferable Adversarial Attacks on Aligned Language Models},
  journal = {arXiv preprint arXiv:2305.10626},
  year    = {2023},
  doi     = {10.48550/arXiv.2305.10626}
}
@inproceedings{Kiela2021Dynabench,
  author    = {Douwe Kiela and Max Bartolo and Yixin Nie and others},
  title     = {Dynabench: Rethinking Benchmarking in NLP},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  year      = {2021},
  pages     = {4110--4124},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2021.naacl-main.169}
}
@article{Kadavath2022KnowWhatKnow,
  author  = {Saurav Kadavath and others},
  title   = {Language Models (Mostly) Know What They Know},
  journal = {arXiv preprint arXiv:2207.05221},
  year    = {2022},
  doi     = {10.48550/arXiv.2207.05221}
}
@article{Wang2023SelfConsistency,
  author  = {Xuezhi Wang and Jason Wei and Dale Schuurmans and others},
  title   = {Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  journal = {arXiv preprint arXiv:2203.11171},
  year    = {2023},
  doi     = {10.48550/arXiv.2203.11171}
}
@article{Bommasani2021FoundationModels,
  author  = {Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and others},
  title   = {On the Opportunities and Risks of Foundation Models},
  journal = {arXiv preprint arXiv:2108.07258},
  year    = {2021},
  doi     = {10.48550/arXiv.2108.07258}
}
@article{Touvron2023LLaMA,
  author  = {Hugo Touvron and Thibaut Lavril and Gautier Izacard and others},
  title   = {LLaMA: Open and Efficient Foundation Language Models},
  journal = {arXiv preprint arXiv:2302.13971},
  year    = {2023},
  doi     = {10.48550/arXiv.2302.13971}
}
@article{Liang2022HELM,
  author  = {Percy Liang and Rishi Bommasani and Tony Lee and others},
  title   = {Holistic Evaluation of Language Models},
  journal = {arXiv preprint arXiv:2211.09110},
  year    = {2022},
  doi     = {10.48550/arXiv.2211.09110}
}
@article{Greshake2023IndirectPromptInjection,
  author  = {Karl Greshake and Matthias Kiessling and Milad Nasr and others},
  title   = {Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injections},
  journal = {arXiv preprint arXiv:2302.12173},
  year    = {2023},
  doi     = {10.48550/arXiv.2302.12173}
}
@inproceedings{Guo2017Calibration,
  author    = {Chuan Guo and Geoff Pleiss and Yu Sun and Kilian Q. Weinberger},
  title     = {On Calibration of Modern Neural Networks},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  year      = {2017},
  pages     = {1321--1330},
  publisher = {PMLR},
  url       = {https://proceedings.mlr.press/v70/guo17a.html}
}
@article{Kaplan2020ScalingLaws,
  author  = {Jared Kaplan and Sam McCandlish and Tom Henighan and others},
  title   = {Scaling Laws for Neural Language Models},
  journal = {arXiv preprint arXiv:2001.08361},
  year    = {2020},
  doi     = {10.48550/arXiv.2001.08361}
}
@inproceedings{Desai2020Calibration,
  author    = {Shrey Desai and Greg Durrett},
  title     = {Calibration of Pre-trained Transformers},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
  year      = {2020},
  pages     = {295--302},
  publisher = {Association for Computational Linguistics},
  url       = {https://arxiv.org/abs/2004.14788}
}
@article{Gehman2020RealToxicity,
  author  = {Samuel Gehman and Suchin Gururangan and Maarten Sap and Yejin Choi and Noah A. Smith},
  title   = {RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models},
  journal = {arXiv preprint arXiv:2009.11462},
  year    = {2020},
  doi     = {10.48550/arXiv.2009.11462}
}
@inproceedings{Xin2020DeeBERT,
  author    = {Ji Xin and Raphael Tang and Jae Myung Lee and others},
  title     = {DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2020},
  year      = {2020},
  pages     = {2246--2251},
  url       = {https://arxiv.org/abs/2004.12993}
}
@article{Liu2023LostInTheMiddle,
  author  = {Nelson F. Liu and Eric Ouyang and Tianyi Liu and Omer Levy},
  title   = {Lost in the Middle: How Language Models Use Long Context},
  journal = {arXiv preprint arXiv:2307.03172},
  year    = {2023},
  doi     = {10.48550/arXiv.2307.03172}
}
@article{Press2021ALiBi,
  author  = {Ofir Press and Noah A. Smith and Mike Lewis},
  title   = {Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
  journal = {arXiv preprint arXiv:2108.12409},
  year    = {2021},
  doi     = {10.48550/arXiv.2108.12409}
}
@article{Su2021RoFormer,
  author  = {Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},
  title   = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
  journal = {arXiv preprint arXiv:2104.09864},
  year    = {2021},
  doi     = {10.48550/arXiv.2104.09864}
}
@article{Lee2022Dedup,
  author  = {Katherine Lee and Daphne Ippolito and Douglas Eck and Chris Callison-Burch},
  title   = {Deduplicating Training Data Makes Language Models Better},
  journal = {arXiv preprint arXiv:2107.06499},
  year    = {2022},
  doi     = {10.48550/arXiv.2107.06499}
}
@article{Geifman2017Selective,
  author  = {Yair Geifman and Ran El-Yaniv},
  title   = {Selective Classification for Deep Neural Networks},
  journal = {arXiv preprint arXiv:1705.08500},
  year    = {2017},
  doi     = {10.48550/arXiv.1705.08500}
}