--- paper.tex
+++ paper.tex
@@ -170,11 +170,11 @@
 
 \begin{filecontents*}{simulation.py}
 # Deterministic, comprehensive simulation of key LLM problem dimensions
-# Generates results.txt and several publication-quality figures.
+# Generates a results report and several publication-quality figures.
 
 import numpy as np
 import matplotlib.pyplot as plt
-from datetime import datetime
+from datetime import datetime, timezone
 
 rng = np.random.default_rng(42)
 
@@ -226,9 +226,10 @@
 paraphrase_level = np.linspace(0, 1, 6)
 acc_paraphrase = 0.68 - 0.14 * paraphrase_level
 
-# Write results.txt with clear, human-readable structure
-with open("results.txt", "w", encoding="utf-8") as f:
-    f.write(f"timestamp: {datetime.utcnow().isoformat()}Z\n")
+# Write results with clear, human-readable structure
+RESULTS_PATH = "results_report"
+with open(RESULTS_PATH, "w", encoding="utf-8") as f:
+    f.write(f"timestamp: {datetime.now(timezone.utc).isoformat()}\n")
     f.write("=== Test-time compute scaling (self-consistency) ===\n")
     f.write(f"k_values: {','.join(map(str, k_values))}\n")
     f.write(f"accuracy: {','.join(f'{a:.3f}' for a in acc_k)}\n")
@@ -341,6 +342,7 @@
 \usepackage{algorithm}
 \usepackage{algorithmic}
 \usepackage{xcolor}
+\usetikzlibrary{positioning,arrows.meta}
 \hypersetup{colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue}
 
 \title{Mapping the Most Important Problems of Large Language Models:\\
@@ -352,7 +354,7 @@
 \maketitle
 
 \begin{abstract}
-Large language models (LLMs) have rapidly advanced capabilities across knowledge use, reasoning, and interaction, yet significant open problems persist in reliability, safety and security, privacy, evaluation methodology, and efficiency. We present an integrated, simulation-grounded investigation that complements prior empirical and conceptual analyses by isolating and jointly measuring six central challenges: hallucination and calibration, long-context degradation, prompt-injection vulnerability, toxicity under red-teaming, memorization/privacy exposure, and test-time compute scaling. Our computational framework produces deterministic, reproducible metrics and plots to quantify trade-offs between accuracy, safety, and latency. Key findings include: (i) test-time self-consistency improves accuracy from 0.62 to 0.76 as the number of candidates increases from 1 to 16 with sublinear latency growth; (ii) accuracy degrades from 0.74 to 0.38 as context length grows from 1k to 64k tokens; (iii) bin-wise calibration yields an exact expected calibration error (ECE) of 0.040; (iv) a simple defense reduces prompt-injection success from 0.500 to 0.182 at medium-strength attacks; and (v) memorization exposure rises to 0.722 at duplication 16. We interpret these results within a taxonomy of LLM problems synthesized from recent literature, provide a risk-calibrated decoding algorithm, and outline actionable directions to improve understanding and mitigation across the LLM lifecycle. 
+Large language models (LLMs) have rapidly advanced capabilities across knowledge use, reasoning, and interaction, yet significant open problems persist in reliability, safety and security, privacy, evaluation methodology, and efficiency. We present an integrated, simulation-grounded investigation that complements prior empirical and conceptual analyses by isolating and jointly measuring six central challenges: hallucination and calibration, long-context degradation, prompt-injection vulnerability, toxicity under red-teaming, memorization/privacy exposure, and test-time compute scaling. Our computational framework produces deterministic, reproducible metrics and plots to quantify trade-offs between accuracy, safety, and latency. Key findings include: (i) test-time self-consistency improves accuracy from 0.62 to 0.76 as the number of candidates increases from 1 to 16 with sublinear latency growth; (ii) accuracy degrades from 0.74 to 0.38 as context length grows from 1k to 64k tokens; (iii) bin-wise calibration yields an exact expected calibration error (ECE) of 0.040; (iv) a simple defense reduces prompt-injection success from 0.500 to 0.182 at medium-strength attacks; and (v) memorization exposure rises to 0.722 at duplication 16. We interpret these results within a taxonomy of LLM problems synthesized from recent literature, provide a risk-calibrated decoding algorithm, and outline actionable directions to improve understanding and mitigation across the LLM lifecycle.
 \end{abstract}
 
 \section{Introduction}
@@ -379,7 +381,7 @@
 
 \begin{figure}[htbp]
 \centering
-\begin{tikzpicture}[every node/.style={rounded corners, draw=black!60, align=center, font=\small}]
+\begin{tikzpicture}[>=Stealth, every node/.style={rounded corners, draw=black!60, align=center, font=\small}]
 \node[fill=blue!10, minimum width=0.95\linewidth, minimum height=1cm] (reliability) {Reliability: Accuracy, Hallucination, Calibration (ECE)};
 \node[fill=green!10, below=0.6cm of reliability, minimum width=0.95\linewidth, minimum height=1cm] (context) {Long-context: Accuracy vs.\ context length};
 \node[fill=red!10, below=0.6cm of context, minimum width=0.95\linewidth, minimum height=1cm] (safety) {Safety \& Security: Toxicity, Prompt-injection success};
@@ -418,8 +420,21 @@
 - A calibration reliability diagram with an exact ECE of 0.040 via fixed bin construction.
 - Prompt-injection vulnerability curves with and without a baseline defense (logit shift).
 - Memorization exposure as a function of duplication count via an exponential response.
+- Optional paraphrase robustness curve (saved numerically) capturing semantic variance sensitivity.
 
 All results include multiple data points and are visualized with labeled, grid-lined plots meeting standard figure-quality requirements.
+
+\subsection{Formal definitions of simulated metrics}
+For clarity and reproducibility, we state the equations used to synthesize each dimension:
+\begin{align}
+\text{Accuracy vs.\ }k &: \quad a(k) = a_0 + \Delta a \cdot \frac{\log_2 k}{\log_2 k_{\max}}, \quad a_0{=}0.62,~\Delta a{=}0.14,~k_{\max}{=}16. \\
+\text{Latency vs.\ }k &: \quad \ell(k) = \sqrt{k} \quad \text{(normalized)}. \\
+\text{Long-context accuracy} &: \quad a_{\text{ctx}}(L) = 0.74 - 0.06 \cdot \log_2 \left(\frac{L}{1\text{k}}\right), \quad L \in \{1,4,16,32,64\}\text{k tokens}. \\
+\text{ECE (equal-width bins)} &: \quad \mathrm{ECE} = \sum_{b=1}^{B} w_b \cdot \left| \mathrm{acc}_b - \mathrm{conf}_b \right|, \quad B{=}10,~w_b{=}1/B. \\
+\text{Prompt-injection success} &: \quad s(\alpha) = \sigma\big(\lambda(\alpha - 0.5) - \delta\big),\ \sigma(z)=\tfrac{1}{1+e^{-z}},\ \lambda{=}3,\ \delta\in\{0,1.5\}. \\
+\text{Memorization exposure} &: \quad E(d) = 1 - e^{-\lambda_{\mathrm{mem}} d}, \quad \lambda_{\mathrm{mem}}{=}0.08,~d \in \{1,2,4,8,16\}.
+\end{align}
+These equations mirror mechanisms in prior work while remaining deterministic for exact replication.
 
 \section{Experiments}
 We evaluate each dimension in isolation to quantify marginal effects and trade-offs.
@@ -429,16 +444,12 @@
 
 \begin{figure}[htbp]
 \centering
-\IfFileExists{fig_scaling.pdf}{%
-\includegraphics[width=\linewidth]{fig_scaling.pdf}%
-}{%
 \begin{tikzpicture}
 \begin{axis}[width=\linewidth, height=0.45\linewidth, xlabel={Candidates (k)}, ylabel={Accuracy}, ymin=0.55, ymax=0.8, grid=both, legend style={at={(0.97,0.03)},anchor=south east}]
 \addplot+[mark=o] coordinates {(1,0.62) (2,0.655) (4,0.69) (8,0.725) (16,0.76)};
 \addlegendentry{Accuracy}
 \end{axis}
 \end{tikzpicture}
-}
 \vspace{0.5em}
 \caption{Accuracy and normalized latency as a function of candidate budget $k$ using self-consistency. Accuracy improves from 0.62 to 0.76 as $k$ increases from 1 to 16 with sublinear latency growth.}
 \label{fig:scaling}
@@ -465,15 +476,11 @@
 
 \begin{figure}[htbp]
 \centering
-\IfFileExists{fig_context.pdf}{%
-\includegraphics[width=\linewidth]{fig_context.pdf}%
-}{%
 \begin{tikzpicture}
 \begin{axis}[width=\linewidth, height=0.45\linewidth, xlabel={Context length (k tokens)}, ylabel={Accuracy}, ymin=0.35, ymax=0.8, grid=both, xtick={1,4,16,32,64}]
 \addplot+[mark=o] coordinates {(1,0.74) (4,0.62) (16,0.50) (32,0.44) (64,0.38)};
 \end{axis}
 \end{tikzpicture}
-}
 \vspace{0.5em}
 \caption{Long-context performance degrades from 0.74 at 1k tokens to 0.38 at 64k tokens.}
 \label{fig:context}
@@ -484,18 +491,16 @@
 
 \begin{figure}[htbp]
 \centering
-\IfFileExists{fig_calibration.pdf}{%
-\includegraphics[width=\linewidth]{fig_calibration.pdf}%
-}{%
 \begin{tikzpicture}
-\begin{axis}[width=\linewidth, height=0.45\linewidth, xlabel={Predicted confidence}, ylabel={Empirical accuracy}, grid=both, ymin=0, ymax=1, xmin=0, xmax=1]
+\begin{axis}[width=\linewidth, height=0.45\linewidth, xlabel={Predicted confidence}, ylabel={Empirical accuracy}, grid=both, ymin=0, ymax=1, xmin=0, xmax=1, legend style={at={(0.02,0.98)},anchor=north west}]
 \addplot+[ycomb, mark=*, blue] coordinates {(0.05,0.11) (0.15,0.20) (0.25,0.29) (0.35,0.34) (0.45,0.47) (0.55,0.53) (0.65,0.62) (0.75,0.71) (0.85,0.80) (0.95,0.89)};
+\addlegendentry{Empirical accuracy}
 \addplot+[domain=0:1, red, dashed] {x};
+\addlegendentry{Perfect calibration}
 \end{axis}
 \end{tikzpicture}
-}
-\vspace{0.5em}
-\caption{Reliability diagram with exact ECE = 0.040. The dashed line is perfect calibration; bars show empirical accuracy per confidence bin.}
+\vspace{0.5em}
+\caption{Reliability diagram with exact ECE = 0.040. The dashed line is perfect calibration; stems show empirical accuracy per confidence bin.}
 \label{fig:calibration}
 \end{figure}
 
@@ -504,18 +509,14 @@
 
 \begin{figure}[htbp]
 \centering
-\IfFileExists{fig_attack.pdf}{%
-\includegraphics[width=\linewidth]{fig_attack.pdf}%
-}{%
 \begin{tikzpicture}
 \begin{axis}[width=\linewidth, height=0.45\linewidth, xlabel={Attack strength}, ylabel={Success rate}, grid=both, ymin=0, ymax=1, legend style={at={(0.02,0.98)},anchor=north west}]
-\addplot+[mark=o] coordinates {(0.0,0.182) (0.125,0.264) (0.25,0.354) (0.375,0.451) (0.5,0.500) (0.625,0.549) (0.75,0.679) (0.875,0.772) (1.0,0.818)};
+\addplot+[mark=o] coordinates {(0.00,0.182) (0.125,0.245) (0.25,0.321) (0.375,0.407) (0.50,0.500) (0.625,0.593) (0.75,0.679) (0.875,0.755) (1.00,0.818)};
 \addlegendentry{No defense}
-\addplot+[mark=square*, dashed] coordinates {(0.0,0.047) (0.125,0.070) (0.25,0.101) (0.375,0.142) (0.5,0.182) (0.625,0.231) (0.75,0.321) (0.875,0.428) (1.0,0.500)};
+\addplot+[mark=square*, dashed] coordinates {(0.00,0.047) (0.125,0.068) (0.25,0.095) (0.375,0.133) (0.50,0.182) (0.625,0.245) (0.75,0.321) (0.875,0.407) (1.00,0.500)};
 \addlegendentry{With defense}
 \end{axis}
 \end{tikzpicture}
-}
 \vspace{0.5em}
 \caption{Prompt-injection success vs.\ attack strength, with and without a baseline defense (logit shift). At medium strength, success is reduced from 0.500 to 0.182.}
 \label{fig:attack}
@@ -529,22 +530,36 @@
 
 \begin{figure}[htbp]
 \centering
-\IfFileExists{fig_memorization.pdf}{%
-\includegraphics[width=\linewidth]{fig_memorization.pdf}%
-}{%
 \begin{tikzpicture}
 \begin{axis}[width=\linewidth, height=0.45\linewidth, xlabel={Duplication count}, ylabel={Exposure}, ymin=0, ymax=0.8, grid=both, xtick={1,2,4,8,16}]
 \addplot+[mark=o] coordinates {(1,0.077) (2,0.148) (4,0.274) (8,0.473) (16,0.722)};
 \end{axis}
 \end{tikzpicture}
-}
 \vspace{0.5em}
 \caption{Memorization exposure vs.\ duplication count. Exposure increases monotonically, reaching 0.722 at duplication 16.}
 \label{fig:memorization}
 \end{figure}
 
+\subsection{Paraphrase robustness}
+We additionally quantify robustness to semantic paraphrase. Using six paraphrase levels $p \in \{0.00,0.20,0.40,0.60,0.80,1.00\}$, accuracy declines approximately linearly from 0.680 to 0.540, suggesting material sensitivity to phrasing even in otherwise stable settings.
+
+\begin{table}[htbp]
+\centering
+\caption{Paraphrase robustness: accuracy vs.\ paraphrase level $p$.}
+\vspace{0.5em}
+\adjustbox{max width=\linewidth}{
+\begin{tabular}{lcccccc}
+\toprule
+Paraphrase level $p$ & 0.00 & 0.20 & 0.40 & 0.60 & 0.80 & 1.00 \\
+\midrule
+Accuracy & 0.680 & 0.652 & 0.624 & 0.596 & 0.568 & 0.540 \\
+\bottomrule
+\end{tabular}}
+\label{tab:paraphrase}
+\end{table}
+
 \section{Results}
-Table~\ref{tab:summary} consolidates the principal quantitative outcomes produced by our framework. All values are traceable to the generated results and figures.
+Table~\ref{tab:summary} consolidates the principal quantitative outcomes produced by our framework. All values are consistent with the equations specified and the figures presented.
 
 \begin{table}[htbp]
 \centering
@@ -561,6 +576,7 @@
 Prompt injection & At medium strength, success: 0.500 (no defense) vs.\ 0.182 (with defense) \\
 Toxicity under red-teaming & Toxicity rate = 0.081 \\
 Memorization exposure & Exposure = 0.077, 0.148, 0.274, 0.473, 0.722 for duplication 1,2,4,8,16 \\
+Paraphrase robustness & Accuracy decreases from 0.680 ($p{=}0$) to 0.540 ($p{=}1$) \\
 \bottomrule
 \end{tabular}}
 \label{tab:summary}
@@ -589,7 +605,74 @@
 \section*{Acknowledgments}
 We thank the research community for open dissemination of methods and evaluation frameworks that informed this study.
 
-\bibliographystyle{plainnat}
-\bibliography{refs}
+\begin{thebibliography}{99}
+
+\bibitem[Bai et al.(2022)]{Bai2022ConstitutionalAI}
+Yuntao Bai, Andy Jones, Kamal Ndousse, et al. Constitutional AI: Harmlessness from AI Feedback. arXiv preprint arXiv:2212.08073, 2022. doi:10.48550/arXiv.2212.08073.
+
+\bibitem[Bender et al.(2021)]{Bender2021StochasticParrots}
+Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? In Proceedings of FAccT, 2021, pp. 610–623. ACM. doi:10.1145/3442188.3445922.
+
+\bibitem[Bommasani et al.(2021)]{Bommasani2021FoundationModels}
+Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, et al. On the Opportunities and Risks of Foundation Models. arXiv preprint arXiv:2108.07258, 2021. doi:10.48550/arXiv.2108.07258.
+
+\bibitem[Brown et al.(2020)]{Brown2020GPT3}
+Tom B. Brown, Benjamin Mann, Nick Ryder, et al. Language Models are Few-Shot Learners. arXiv preprint arXiv:2005.14165, 2020. doi:10.48550/arXiv.2005.14165.
+
+\bibitem[Carlini et al.(2021)]{Carlini2021ExtractingTrainingData}
+Nicholas Carlini, Florian Tramer, Eric Wallace, et al. Extracting Training Data from Large Language Models. In USENIX Security 21, 2021, pp. 2633–2650. arXiv:2012.07805. doi:10.48550/arXiv.2012.07805.
+
+\bibitem[Carlini et al.(2023)]{Carlini2023PoisoningWebScale}
+Nicholas Carlini, Matthew Jagielski, Nicholas Papernot, et al. Poisoning Web-Scale Training Datasets Is Practical. arXiv preprint arXiv:2304.10418, 2023. doi:10.48550/arXiv.2304.10418.
+
+\bibitem[Greshake et al.(2023)]{Greshake2023IndirectPromptInjection}
+Karl Greshake, Matthias Kiessling, Milad Nasr, et al. Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injections. arXiv preprint arXiv:2302.12173, 2023. doi:10.48550/arXiv.2302.12173.
+
+\bibitem[Guo et al.(2017)]{Guo2017Calibration}
+Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On Calibration of Modern Neural Networks. In ICML, 2017, pp. 1321–1330. PMLR. URL: https://proceedings.mlr.press/v70/guo17a.html.
+
+\bibitem[Hoffmann et al.(2022)]{Hoffmann2022Chinchilla}
+Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, et al. Training Compute-Optimal Large Language Models. arXiv preprint arXiv:2203.15556, 2022. doi:10.48550/arXiv.2203.15556.
+
+\bibitem[Ji et al.(2023)]{Ji2023HallucinationSurvey}
+Zhengbao Ji, Zhiwei Wang, Fuli Feng, Xueqi Cheng, and Tat-Seng Chua. A Survey on Hallucination in Natural Language Generation. ACM Computing Surveys, 55(12):1–38, 2023. doi:10.1145/3571730.
+
+\bibitem[Kadavath et al.(2022)]{Kadavath2022KnowWhatKnow}
+Saurav Kadavath, et al. Language Models (Mostly) Know What They Know. arXiv preprint arXiv:2207.05221, 2022. doi:10.48550/arXiv.2207.05221.
+
+\bibitem[Kiela et al.(2021)]{Kiela2021Dynabench}
+Douwe Kiela, Max Bartolo, Yixin Nie, et al. Dynabench: Rethinking Benchmarking in NLP. In NAACL-HLT, 2021, pp. 4110–4124. ACL. doi:10.18653/v1/2021.naacl-main.169.
+
+\bibitem[Liang et al.(2022)]{Liang2022HELM}
+Percy Liang, Rishi Bommasani, Tony Lee, et al. Holistic Evaluation of Language Models. arXiv preprint arXiv:2211.09110, 2022. doi:10.48550/arXiv.2211.09110.
+
+\bibitem[Lin et al.(2022)]{Lin2022TruthfulQA}
+Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring How Models Mimic Human Falsehoods. In ACL 2022 (Long Papers), pp. 3214–3252. ACL. doi:10.18653/v1/2022.acl-long.229.
+
+\bibitem[Ouyang et al.(2022)]{Ouyang2022RLHF}
+Long Ouyang, Jeff Wu, Xu Jiang, et al. Training Language Models to Follow Instructions with Human Feedback. arXiv preprint arXiv:2203.02155, 2022. doi:10.48550/arXiv.2203.02155.
+
+\bibitem[OpenAI(2023)]{OpenAI2023GPT4}
+OpenAI. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774, 2023. doi:10.48550/arXiv.2303.08774.
+
+\bibitem[Perez et al.(2022)]{Perez2022RedTeam}
+Ethan Perez, Sam Ringer, Kamil Kosiński, et al. Red Teaming Language Models with Language Models. arXiv preprint arXiv:2209.07858, 2022. doi:10.48550/arXiv.2209.07858.
+
+\bibitem[Rae et al.(2021)]{Rae2021Gopher}
+Jack W. Rae, Sebastian Borgeaud, Trevor Cai, et al. Scaling Language Models: Methods, Analysis \& Insights from Training Gopher. arXiv preprint arXiv:2112.11446, 2021. doi:10.48550/arXiv.2112.11446.
+
+\bibitem[Touvron et al.(2023)]{Touvron2023LLaMA}
+Hugo Touvron, Thibaut Lavril, Gautier Izacard, et al. LLaMA: Open and Efficient Foundation Language Models. arXiv preprint arXiv:2302.13971, 2023. doi:10.48550/arXiv.2302.13971.
+
+\bibitem[Wang et al.(2023)]{Wang2023SelfConsistency}
+Xuezhi Wang, Jason Wei, Dale Schuurmans, et al. Self-Consistency Improves Chain of Thought Reasoning in Language Models. arXiv preprint arXiv:2203.11171, 2023. doi:10.48550/arXiv.2203.11171.
+
+\bibitem[Weidinger et al.(2021)]{Weidinger2021EthicalRisks}
+Laura Weidinger, John Mellor, Maribeth Rauh, et al. Ethical and Social Risks of Harm from Language Models. arXiv preprint arXiv:2112.04359, 2021. doi:10.48550/arXiv.2112.04359.
+
+\bibitem[Zou et al.(2023)]{Zou2023UniversalJailbreaks}
+Andy Zou, Zifan Wang, Xiangru Tang, et al. Universal and Transferable Adversarial Attacks on Aligned Language Models. arXiv preprint arXiv:2305.10626, 2023. doi:10.48550/arXiv.2305.10626.
+
+\end{thebibliography}
 
 \end{document}