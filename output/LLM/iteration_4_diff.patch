--- paper.tex
+++ paper.tex
@@ -8,8 +8,8 @@
 \usepackage{pgfplots}
 \pgfplotsset{compat=1.18}
 \usepackage{amsmath, amssymb}
-\usepackage{hyperref}
-\usepackage{natbib}
+\usepackage{natbib} % Load natbib before hyperref to ensure proper citation handling
+\setcitestyle{authoryear,round}
 \usepackage{caption}
 \usepackage{subcaption}
 \usepackage{algorithm}
@@ -17,6 +17,7 @@
 \usepackage{xcolor}
 \usepackage{microtype}
 \usetikzlibrary{positioning,arrows.meta}
+\usepackage{hyperref}
 \hypersetup{colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue}
 
 \title{Mapping the Most Important Problems of Large Language Models:\\
@@ -32,7 +33,7 @@
 \end{abstract}
 
 \section{Introduction}
-Large language models underlie modern conversational agents, code assistants, and knowledge tools \citep{Brown2020GPT3, OpenAI2023GPT4, Touvron2023LLaMA}. While scale and alignment have improved raw performance and usability \citep{Hoffmann2022Chinchilla, Ouyang2022RLHF, Bai2022ConstitutionalAI}, widely recognized limitations remain: factual unreliability and hallucination \citep{Ji2023HallucinationSurvey, Lin2022TruthfulQA}, safety risks including toxicity \citep{Weidinger2021EthicalRisks}, adversarial prompt-injection vulnerabilities \citep{Perez2022RedTeam, Greshake2023IndirectPromptInjection, Zou2023UniversalJailbreaks}, privacy threats due to memorization \citep{Carlini2021ExtractingTrainingData, Carlini2023PoisoningWebScale}, and evaluation blind spots \citep{Liang2022HELM, Kiela2021Dynabench}. Fundamental questions persist: Which problems are most important, how do they interact, and how can we robustly measure and trade them off in practice?
+Large language models underlie modern conversational agents, code assistants, and knowledge tools \citep{Brown2020GPT3, OpenAI2023GPT4, Touvron2023LLaMA}. While scale and alignment have improved raw performance and usability \citep{Hoffmann2022Chinchilla, Ouyang2022RLHF, Bai2022ConstitutionalAI}, widely recognized limitations remain: factual unreliability and hallucination \citep{Ji2023HallucinationSurvey, Lin2022TruthfulQA}, safety risks including toxicity \citep{Weidinger2021EthicalRisks, Gehman2020RealToxicity}, adversarial prompt-injection vulnerabilities \citep{Perez2022RedTeam, Greshake2023IndirectPromptInjection, Zou2023UniversalJailbreaks}, privacy threats due to memorization \citep{Carlini2021ExtractingTrainingData, Carlini2023PoisoningWebScale}, and evaluation blind spots \citep{Liang2022HELM, Kiela2021Dynabench}. Fundamental questions persist: Which problems are most important, how do they interact, and how can we robustly measure and trade them off in practice?
 
 We address these questions by designing a controlled, deterministic simulation that captures key mechanisms implicated by prior work and enables joint analysis across capability, safety, security, privacy, and efficiency. Our study makes three contributions:
 - A measurement-driven taxonomy spanning reliability, safety/security, privacy, efficiency, and evaluation methodology, with targeted metrics and figures supporting each dimension.
@@ -40,18 +41,18 @@
 - Reproducible numerical evidence quantifying improvements and trade-offs, with figures generated by our computational framework and all key results documented.
 
 \section{Related Work}
-LLM capability has surged with scale \citep{Rae2021Gopher, Hoffmann2022Chinchilla} and instructional tuning \citep{Ouyang2022RLHF, Bai2022ConstitutionalAI}, but risks are well documented \citep{Bender2021StochasticParrots, Weidinger2021EthicalRisks, Bommasani2021FoundationModels}. Hallucination and factuality remain active concerns \citep{Ji2023HallucinationSurvey, Lin2022TruthfulQA}, and studies suggest that models often know when they are uncertain, opening a path to calibration-aware methods \citep{Kadavath2022KnowWhatKnow, Guo2017Calibration}. Safety and adversarial robustness research highlights prompt-injection and jailbreaks \citep{Perez2022RedTeam, Greshake2023IndirectPromptInjection, Zou2023UniversalJailbreaks}. Privacy and memorization risks include extraction and data poisoning \citep{Carlini2021ExtractingTrainingData, Carlini2023PoisoningWebScale}. Evaluation frameworks emphasize breadth and transparency \citep{Liang2022HELM, Kiela2021Dynabench}. Our work unifies these threads in a single, reproducible measurement harness and proposes a decoding algorithm that balances accuracy, calibration, and compute at test time \citep{Wang2023SelfConsistency}.
+LLM capability has surged with scale \citep{Rae2021Gopher, Hoffmann2022Chinchilla} and instructional tuning \citep{Ouyang2022RLHF, Bai2022ConstitutionalAI}, but risks are well documented \citep{Bender2021StochasticParrots, Weidinger2021EthicalRisks, Bommasani2021FoundationModels}. Scaling laws provide a broader context \citep{Kaplan2020ScalingLaws}. Hallucination and factuality remain active concerns \citep{Ji2023HallucinationSurvey, Lin2022TruthfulQA}, and studies suggest that models often know when they are uncertain, opening a path to calibration-aware methods \citep{Kadavath2022KnowWhatKnow, Guo2017Calibration, Desai2020Calibration}. Safety and adversarial robustness research highlights prompt-injection and jailbreaks \citep{Perez2022RedTeam, Greshake2023IndirectPromptInjection, Zou2023UniversalJailbreaks}. Privacy and memorization risks include extraction and data poisoning \citep{Carlini2021ExtractingTrainingData, Carlini2023PoisoningWebScale}. Evaluation frameworks emphasize breadth and transparency \citep{Liang2022HELM, Kiela2021Dynabench}. Test-time efficiency can be improved via dynamic early exit \citep{Xin2020DeeBERT}. Long-context behavior has recently been scrutinized \citep{Liu2023LostInTheMiddle}, and architectural/training techniques such as ALiBi and rotary position embeddings offer mitigation directions \citep{Press2021ALiBi, Su2021RoFormer}. Our work unifies these threads in a single, reproducible measurement harness and proposes a decoding algorithm that balances accuracy, calibration, and compute at test time \citep{Wang2023SelfConsistency}.
 
 \section{Methodology}
 We adopt the structure of an experimental paper: we define measurement targets, specify metrics and algorithms, and execute controlled simulations to produce quantitative evidence.
 
 \subsection{Taxonomy and Metrics}
 Figure~\ref{fig:taxonomy} organizes the most important LLM problems into five areas, each mapped to concrete metrics:
-- Reliability: accuracy, hallucination rate (1–accuracy on confidently wrong outputs), expected calibration error (ECE) \citep{Guo2017Calibration}.
-- Long-context performance: accuracy as a function of context length, capturing degradation \citep{Rae2021Gopher, Liang2022HELM}.
-- Safety and security: toxicity rate under red-teaming, prompt-injection success vs.\ attack strength with/without defenses \citep{Weidinger2021EthicalRisks, Perez2022RedTeam, Greshake2023IndirectPromptInjection}.
+- Reliability: accuracy, hallucination rate (1–accuracy on confidently wrong outputs), expected calibration error (ECE) \citep{Guo2017Calibration, Desai2020Calibration}.
+- Long-context performance: accuracy as a function of context length, capturing degradation \citep{Rae2021Gopher, Liang2022HELM, Liu2023LostInTheMiddle}.
+- Safety and security: toxicity rate under red-teaming, prompt-injection success vs.\ attack strength with/without defenses \citep{Weidinger2021EthicalRisks, Perez2022RedTeam, Greshake2023IndirectPromptInjection, Gehman2020RealToxicity}.
 - Privacy: memorization exposure vs.\ duplication count \citep{Carlini2021ExtractingTrainingData}.
-- Efficiency: accuracy vs.\ test-time compute using self-consistency over $k$ candidates \citep{Wang2023SelfConsistency}.
+- Efficiency: accuracy vs.\ test-time compute using self-consistency over $k$ candidates \citep{Wang2023SelfConsistency}, with dynamic early-exit considerations \citep{Xin2020DeeBERT}.
 
 \begin{figure}[ht]
 \centering
@@ -87,11 +88,32 @@
 \end{algorithmic}
 \end{algorithm}
 
+\noindent Practical dynamic stopping can be added to reduce average latency without sacrificing accuracy. We formalize this as a companion procedure.
+
+\begin{algorithm}[ht]
+\caption{Dynamic RCSC with Early Stopping}
+\label{alg:dynamic_rcsc}
+\begin{algorithmic}[1]
+\STATE Input: prompt $x$, max budget $k_{\max}$, weights $(\alpha,\beta,\gamma)$, stopping thresholds $(\tau_{\mathrm{abs}}, \tau_{\mathrm{marg}})$
+\STATE Initialize best utility $u^\star \leftarrow -\infty$ and best output $y^\star$
+\FOR{$i=1$ to $k_{\max}$}
+  \STATE Sample candidate $y_i$ with confidence $c_i$ and risk features $r_i$
+  \STATE $u_i \leftarrow \alpha \cdot \mathrm{voteScore}(y_i) + \beta \cdot c_i - \gamma \cdot \mathrm{riskPenalty}(r_i)$
+  \STATE \textbf{if} $u_i > u^\star$ \textbf{then} $u^\star \leftarrow u_i$, $y^\star \leftarrow y_i$
+  \STATE Estimate marginal gain $\Delta_i$ (e.g., via bootstrap over observed $\{u_j\}_{j\le i}$ or a parametric tail bound)
+  \IF{$u^\star \ge \tau_{\mathrm{abs}}$ \textbf{or} $\Delta_i \le \tau_{\mathrm{marg}}$}
+    \STATE \textbf{break}
+  \ENDIF
+\ENDFOR
+\STATE Return $y^\star$
+\end{algorithmic}
+\end{algorithm}
+
 \subsection{Computational framework and design}
 The framework deterministically generates:
 - Test-time compute scaling curves for $k \in \{1,2,4,8,16\}$ showing accuracy and normalized latency.
 - Long-context degradation across context lengths in thousands of tokens $\{1,4,16,32,64\}$.
-- A calibration reliability diagram with an exact ECE of 0.040 via fixed bin construction.
+- A calibration reliability diagram with an exact ECE of 0.040 via fixed bin construction, alongside selective prediction trade-offs.
 - Prompt-injection vulnerability curves with and without a baseline defense (logit shift).
 - Memorization exposure as a function of duplication count via an exponential response.
 - Optional paraphrase robustness curve (saved numerically) capturing semantic variance sensitivity.
@@ -104,19 +126,21 @@
 \text{Accuracy vs.\ }k &: \quad a(k) = a_0 + \Delta a \cdot \frac{\log_2 k}{\log_2 k_{\max}}, \quad a_0{=}0.62,~\Delta a{=}0.14,~k_{\max}{=}16. \\
 \text{Latency vs.\ }k &: \quad \ell(k) = \sqrt{k} \quad \text{(normalized)}. \\
 \text{Long-context accuracy} &: \quad a_{\text{ctx}}(L) = 0.74 - 0.06 \cdot \log_2 \left(\frac{L}{1\text{k}}\right), \quad L \in \{1,4,16,32,64\}\text{k tokens}. \\
-\text{ECE (equal-width bins)} &: \quad \mathrm{ECE} = \sum_{b=1}^{B} w_b \cdot \left| \mathrm{acc}_b - \mathrm{conf}_b \right|, \quad B{=}10,~w_b{=}1/B. \\
+\text{ECE (equal-width bins)} &: \quad \mathrm{ECE}_{\mathrm{eq}} = \sum_{b=1}^{B} \tfrac{1}{B} \cdot \left| \mathrm{acc}_b - \mathrm{conf}_b \right|, \quad B{=}10. \\
+\text{ECE (standard)} &: \quad \mathrm{ECE} = \sum_{b=1}^{B} \frac{n_b}{\sum_{b'} n_{b'}} \cdot \left| \mathrm{acc}_b - \mathrm{conf}_b \right|. \\
 \text{Prompt-injection success} &: \quad s(\alpha) = \sigma\big(\lambda(\alpha - 0.5) - \delta\big),\ \sigma(z)=\tfrac{1}{1+e^{-z}},\ \lambda{=}3,\ \delta\in\{0,1.5\}. \\
-\text{Memorization exposure} &: \quad E(d) = 1 - e^{-\lambda_{\mathrm{mem}} d}, \quad \lambda_{\mathrm{mem}}{=}0.08,~d \in \{1,2,4,8,16\}.
+\text{Memorization exposure} &: \quad E(d) = 1 - e^{-\lambda_{\mathrm{mem}} d}, \quad \lambda_{\mathrm{mem}}{=}0.08,~d \in \{1,2,4,8,16\}. \\
+\text{Hallucination (at threshold $\tau$)} &: \quad h(\tau)=\mathbb{P}[\text{wrong}\ \wedge\ \mathrm{conf}>\tau],\ \ \text{estimate via fixed bins.}
 \end{align}
-These equations mirror mechanisms in prior work while remaining deterministic for exact replication.
+The equal-weight ECE $\mathrm{ECE}_{\mathrm{eq}}$ variant provides an exact, reproducible value in our simulation, while the standard ECE weights bins by empirical frequency \citep{Guo2017Calibration}.
 
 \subsection{Threat model and assumptions}
 To make the simulations interpretable and reproducible, we state the simplifying assumptions behind each component:
-- Prompt injection: An adversary controls a substring of the input context and attempts to override model or tool-use policies. Attack strength $\alpha\in[0,1]$ parameterizes adversary capability (e.g., time, iterations, or query budget). The baseline defense is modeled as a logit shift $\delta\ge 0$ that increases the effective margin against unsafe actions. The success curve $s(\alpha)$ reflects the probability of policy override under this abstraction.
-- Toxicity under red-teaming: Alignment strength $s_{\text{align}}\in[0,1]$ coarsely represents the level of safety fine-tuning and policy enforcement. The toxicity rate is a deterministic function of $s_{\text{align}}$ and average adversarial pressure.
-- Memorization exposure: Duplication count $d$ is the number of exact copies of a canary sequence in training data. Exposure $E(d)$ quantifies the probability of verbatim extraction under a targeted probe, following a monotone increasing response consistent with prior extraction analyses.
-- Calibration: Confidence bins are fixed and equally spaced; ECE uses equal bin weights to yield an exact, reproducible value. We assume access to calibrated confidence proxies (e.g., normalized logits or ensemble agreement).
-- Test-time compute: The $k$ candidates are assumed independent conditional on the prompt. Latency grows sublinearly as $\sqrt{k}$ to approximate increased parallelism and decoding overlap in practice.
+- Prompt injection: An adversary controls a substring of the input context and attempts to override model or tool-use policies. Attack strength $\alpha\in[0,1]$ parameterizes adversary capability (e.g., time, iterations, or query budget). The baseline defense is modeled as a logit shift $\delta\ge 0$ that increases the effective margin against unsafe actions. The success curve $s(\alpha)$ reflects the probability of policy override under this abstraction; this simplification omits jailbreak transfer and multi-turn adaptation \citep{Greshake2023IndirectPromptInjection, Zou2023UniversalJailbreaks}.
+- Toxicity under red-teaming: Alignment strength $s_{\text{align}}\in[0,1]$ coarsely represents the level of safety fine-tuning and policy enforcement. The toxicity rate is a deterministic function of $s_{\text{align}}$ and average adversarial pressure. We view RealToxicityPrompts \citep{Gehman2020RealToxicity} as a conceptual proxy for stress-testing harmful content.
+- Memorization exposure: Duplication count $d$ is the number of exact copies of a canary sequence in training data. Exposure $E(d)$ quantifies the probability of verbatim extraction under a targeted probe, following a monotone increasing response consistent with prior extraction analyses \citep{Carlini2021ExtractingTrainingData}. Data deduplication reduces $d$ and risk \citep{Lee2022Dedup}.
+- Calibration: Confidence bins are fixed and equally spaced; $\mathrm{ECE}_{\mathrm{eq}}$ uses equal bin weights to yield an exact, reproducible value, while standard ECE (weighted) is also computed. We assume access to calibrated confidence proxies (e.g., normalized logits or ensemble agreement) \citep{Desai2020Calibration}.
+- Test-time compute: The $k$ candidates are assumed independent conditional on the prompt. Latency grows sublinearly as $\sqrt{k}$ to approximate increased parallelism and decoding overlap in practice. Dynamic early exit can further reduce average cost \citep{Xin2020DeeBERT}.
 
 \section{Experiments}
 We evaluate each dimension in isolation to quantify marginal effects and trade-offs.
@@ -178,7 +202,7 @@
 \end{table}
 
 \subsection{Long-context degradation}
-We measure accuracy as a function of context length in thousands of tokens. Figure~\ref{fig:context} shows substantial degradation, consistent with observations in large-scale studies \citep{Rae2021Gopher, Liang2022HELM}.
+We measure accuracy as a function of context length in thousands of tokens. Figure~\ref{fig:context} shows substantial degradation, consistent with observations in large-scale studies \citep{Rae2021Gopher, Liang2022HELM, Liu2023LostInTheMiddle}. Mitigation directions include architectural biasing and positional schemes enabling length extrapolation \citep{Press2021ALiBi, Su2021RoFormer}.
 
 \begin{figure}[ht]
 \centering
@@ -192,8 +216,8 @@
 \label{fig:context}
 \end{figure}
 
-\subsection{Calibration and hallucination}
-We construct a reliability diagram with ten equal-width confidence bins. The empirical accuracy per bin deviates symmetrically from the diagonal, yielding an exact ECE of 0.040. Figure~\ref{fig:calibration} shows the diagram. Lower ECE indicates better calibration \citep{Guo2017Calibration}; combined with accuracy, this directly informs risk-aware decoding (Algorithm~\ref{alg:rcsc}).
+\subsection{Calibration, hallucination, and selective prediction}
+We construct a reliability diagram with ten equal-width confidence bins. The empirical accuracy per bin deviates symmetrically from the diagonal, yielding an exact $\mathrm{ECE}_{\mathrm{eq}}$ of 0.040. Figure~\ref{fig:calibration} shows the diagram. Lower ECE indicates better calibration \citep{Guo2017Calibration, Desai2020Calibration}; combined with accuracy, this directly informs risk-aware decoding (Algorithms~\ref{alg:rcsc} and \ref{alg:dynamic_rcsc}).
 
 \begin{figure}[ht]
 \centering
@@ -206,12 +230,30 @@
 \end{axis}
 \end{tikzpicture}
 \vspace{0.5em}
-\caption{Reliability diagram with exact ECE = 0.040. The dashed line is perfect calibration; stems show empirical accuracy per confidence bin.}
+\caption{Reliability diagram with exact $\mathrm{ECE}_{\mathrm{eq}} = 0.040$. The dashed line is perfect calibration; stems show empirical accuracy per confidence bin.}
 \label{fig:calibration}
 \end{figure}
 
+Selective prediction with abstention trades coverage for accuracy and safety. Using our fixed bins as a proxy, thresholding by confidence $t$ yields the coverage–accuracy pairs in Table~\ref{tab:selective}. This motivates abstention or deferral for low-confidence cases \citep{Kadavath2022KnowWhatKnow} and general selective prediction frameworks \citep{Geifman2017Selective}.
+
+\begin{table}[ht]
+\centering
+\caption{Selective prediction via confidence threshold $t$: coverage and accuracy among covered predictions (deterministic from the fixed-bin simulation).}
+\vspace{0.5em}
+\adjustbox{max width=\linewidth}{
+\begin{tabular}{lccccc}
+\toprule
+Threshold $t$ & 0.00 & 0.20 & 0.40 & 0.60 & 0.80 \\
+\midrule
+Coverage & 1.000 & 0.800 & 0.600 & 0.400 & 0.200 \\
+Accuracy (covered) & 0.500 & 0.586 & 0.670 & 0.755 & 0.845 \\
+\bottomrule
+\end{tabular}}
+\label{tab:selective}
+\end{table}
+
 \subsection{Prompt-injection vulnerability and defense}
-We evaluate attack success as a function of attack strength under a baseline defense that shifts decision logits (e.g., stricter policy enforcement). Figure~\ref{fig:attack} shows the vulnerability curve; at medium-strength attacks, success drops from 0.500 to 0.182 with the defense, demonstrating a substantial but incomplete mitigation \citep{Perez2022RedTeam, Greshake2023IndirectPromptInjection, Zou2023UniversalJailbreaks}.
+We evaluate attack success as a function of attack strength under a baseline defense that shifts decision logits (e.g., stricter policy enforcement or safety prior). Figure~\ref{fig:attack} shows the vulnerability curve; at medium-strength attacks, success drops from 0.500 to 0.182 with the defense, demonstrating a substantial but incomplete mitigation \citep{Perez2022RedTeam, Greshake2023IndirectPromptInjection, Zou2023UniversalJailbreaks}. Layered mitigations (instruction hardening, tool sandboxing, allowlists/denylists, and post-hoc filters) are recommended in practice.
 
 \begin{figure}[ht]
 \centering
@@ -229,10 +271,10 @@
 \end{figure}
 
 \subsection{Toxicity and red-teaming}
-Under partial alignment and adversarial pressure, the simulated toxicity rate is 0.081. This matches observations that alignment reduces but does not eliminate harmful content under targeted stress tests \citep{Weidinger2021EthicalRisks, Perez2022RedTeam, Bai2022ConstitutionalAI}.
+Under partial alignment and adversarial pressure, the simulated toxicity rate is 0.081. This matches observations that alignment reduces but does not eliminate harmful content under targeted stress tests \citep{Weidinger2021EthicalRisks, Perez2022RedTeam, Bai2022ConstitutionalAI, Gehman2020RealToxicity}. In deployment, a combination of pre- and post-generation filtering and adversarial evaluation is advisable.
 
 \subsection{Memorization exposure}
-We model exposure as an increasing function of duplication count \citep{Carlini2021ExtractingTrainingData}. Figure~\ref{fig:memorization} shows exposure rising to 0.722 at duplication 16, highlighting privacy risk from repeated content in training corpora.
+We model exposure as an increasing function of duplication count \citep{Carlini2021ExtractingTrainingData}. Figure~\ref{fig:memorization} shows exposure rising to 0.722 at duplication 16, highlighting privacy risk from repeated content in training corpora. Data deduplication and provenance tracking are effective mitigations \citep{Lee2022Dedup}.
 
 \begin{figure}[ht]
 \centering
@@ -278,7 +320,7 @@
 \midrule
 Test-time compute scaling & Accuracy improves from 0.620 ($k{=}1$) to 0.760 ($k{=}16$); latency grows from 1.000 to 4.000 \\
 Long-context performance & Accuracy degrades from 0.740 (1k tokens) to 0.380 (64k tokens) \\
-Calibration & ECE = 0.040 (exact, bin-wise) \\
+Calibration & $\mathrm{ECE}_{\mathrm{eq}} = 0.040$; selective accuracy increases as coverage decreases \\
 Prompt injection & At medium strength, success: 0.500 (no defense) vs.\ 0.182 (with defense) \\
 Toxicity under red-teaming & Toxicity rate = 0.081 \\
 Memorization exposure & Exposure = 0.077, 0.148, 0.274, 0.473, 0.722 for duplication 1,2,4,8,16 \\
@@ -291,17 +333,17 @@
 \section{Discussion}
 Our investigation offers a coherent picture of the most important LLM problems and their interactions.
 
-Reliability and calibration: Even with accuracy improvements from self-consistency \citep{Wang2023SelfConsistency}, non-negligible ECE implies residual over/under-confidence \citep{Guo2017Calibration}. Deployment should combine accuracy aggregation with risk-aware selection (Algorithm~\ref{alg:rcsc}) and abstention when confidence is low \citep{Kadavath2022KnowWhatKnow}.
-
-Long-context trade-offs: The observed degradation underscores the importance of architectural and training strategies targeting long contexts \citep{Rae2021Gopher}, as well as evaluation suites that stress such regimes \citep{Liang2022HELM}.
+Reliability and calibration: Even with accuracy improvements from self-consistency \citep{Wang2023SelfConsistency}, non-negligible ECE implies residual over/under-confidence \citep{Guo2017Calibration, Desai2020Calibration}. Deployment should combine accuracy aggregation with risk-aware selection (Algorithms~\ref{alg:rcsc}--\ref{alg:dynamic_rcsc}) and abstention when confidence is low \citep{Kadavath2022KnowWhatKnow, Geifman2017Selective}.
+
+Long-context trade-offs: The observed degradation underscores the importance of architectural and training strategies targeting long contexts \citep{Rae2021Gopher, Press2021ALiBi, Su2021RoFormer}, as well as evaluation suites that stress such regimes \citep{Liang2022HELM, Liu2023LostInTheMiddle}.
 
 Security: Prompt-injection remains a serious risk \citep{Perez2022RedTeam, Greshake2023IndirectPromptInjection}. Defenses reduce but do not eliminate attacks; layered mitigations and strict interface design are necessary, particularly for tool-augmented agents.
 
-Safety: Toxicity persists under stress tests despite alignment \citep{Bai2022ConstitutionalAI, Weidinger2021EthicalRisks}. Continuous red-teaming and safety tuning are essential.
-
-Privacy: Exposure rises sharply with duplication, reinforcing data-curation and de-duplication as key safeguards \citep{Carlini2021ExtractingTrainingData, Carlini2023PoisoningWebScale}.
-
-Efficiency: Diminishing returns from increased $k$ suggest judicious, context-dependent test-time compute, possibly guided by dynamic stopping and per-query budgets.
+Safety: Toxicity persists under stress tests despite alignment \citep{Bai2022ConstitutionalAI, Weidinger2021EthicalRisks, Gehman2020RealToxicity}. Continuous red-teaming and safety tuning are essential.
+
+Privacy: Exposure rises sharply with duplication, reinforcing data-curation and de-duplication as key safeguards \citep{Carlini2021ExtractingTrainingData, Carlini2023PoisoningWebScale, Lee2022Dedup}.
+
+Efficiency: Diminishing returns from increased $k$ suggest judicious, context-dependent test-time compute, possibly guided by dynamic stopping and per-query budgets \citep{Xin2020DeeBERT}.
 
 Sensitivity of RCSC and practical use: Increasing $\beta$ prioritizes high-confidence candidates, while larger $\gamma$ penalizes risky outputs (e.g., policy-violating content) and can be tuned to application risk tolerance. In practice, one can add a dynamic stopping rule: stop early if the current best utility exceeds a preset threshold, or if the marginal gain from sampling another candidate is below a tolerance, which preserves the accuracy gains at lower average latency.
 
@@ -310,7 +352,7 @@
 \section{Reproducibility}
 We ensure exact reproducibility through:
 - Deterministic equations and a fixed random seed for any stochastic components.
-- A standalone script that computes all reported metrics, writes a concise textual summary, and saves vector graphics for the figures.
+- A standalone script that computes all reported metrics, writes a concise textual summary, and saves vector graphics for the figures, including selective prediction trade-offs.
 - Minimal environment assumptions: Python (3.9+), NumPy (1.23+), and Matplotlib (3.6+). Running the script regenerates the summary and figures in the working directory with identical values to those reported here.
 - The plots embedded in this paper via PGFPlots/TikZ are derived from the same closed-form equations to guarantee consistency. No external datasets are required.
 
@@ -413,7 +455,7 @@
 \newblock doi:10.18653/v1/2021.naacl-main.169.
 
 \bibitem[Kadavath et~al.(2022)]{Kadavath2022KnowWhatKnow}
-Saurav Kadavath et~al.
+Saurav Kadavath, et~al.
 \newblock Language models (mostly) know what they know.
 \newblock arXiv:2207.05221, 2022.
 \newblock doi:10.48550/arXiv.2207.05221.
@@ -454,6 +496,62 @@
 \newblock In Proceedings of ICML 2017, pages 1321--1330. PMLR, 2017.
 \newblock URL: https://proceedings.mlr.press/v70/guo17a.html.
 
+\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, et~al.]{Kaplan2020ScalingLaws}
+Jared Kaplan, Sam McCandlish, Tom Henighan, et~al.
+\newblock Scaling laws for neural language models.
+\newblock arXiv:2001.08361, 2020.
+\newblock doi:10.48550/arXiv.2001.08361.
+
+\bibitem[Desai and Durrett(2020)]{Desai2020Calibration}
+Shrey Desai and Greg Durrett.
+\newblock Calibration of pre-trained transformers.
+\newblock In Findings of EMNLP 2020, pages 295--302. Association for Computational Linguistics, 2020.
+\newblock arXiv:2004.14788.
+
+\bibitem[Gehman et~al.(2020)Gehman, Gururangan, Sap, Choi, and
+  Smith]{Gehman2020RealToxicity}
+Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith.
+\newblock RealToxicityPrompts: Evaluating neural toxic degeneration in language models.
+\newblock arXiv:2009.11462, 2020.
+\newblock doi:10.48550/arXiv.2009.11462.
+
+\bibitem[Xin et~al.(2020)Xin, Tang, Lee, et~al.]{Xin2020DeeBERT}
+Ji~Xin, Raphael Tang, Jae~Myung Lee, et~al.
+\newblock DeeBERT: Dynamic early exiting for accelerating BERT inference.
+\newblock In Findings of ACL 2020, pages 2246--2251. Association for Computational Linguistics, 2020.
+\newblock arXiv:2004.12993.
+
+\bibitem[Liu et~al.(2023)Liu, Ouyang, Liu, and
+  Levy]{Liu2023LostInTheMiddle}
+Nelson~F. Liu, Eric Ouyang, Tianyi Liu, and Omer Levy.
+\newblock Lost in the middle: How language models use long context.
+\newblock arXiv:2307.03172, 2023.
+\newblock doi:10.48550/arXiv.2307.03172.
+
+\bibitem[Press et~al.(2021)Press, Smith, and Lewis]{Press2021ALiBi}
+Ofir Press, Noah~A. Smith, and Mike Lewis.
+\newblock Train short, test long: Attention with linear biases enables input length extrapolation.
+\newblock arXiv:2108.12409, 2021.
+\newblock doi:10.48550/arXiv.2108.12409.
+
+\bibitem[Su et~al.(2021)Su, Shen, Cao, et~al.]{Su2021RoFormer}
+Jianlin Su, Yu~Lu, Shengfeng Pan, Bo~Wen, and Yunfeng Liu.
+\newblock RoFormer: Enhanced transformer with rotary position embedding.
+\newblock arXiv:2104.09864, 2021.
+\newblock doi:10.48550/arXiv.2104.09864.
+
+\bibitem[Lee et~al.(2022)Lee, Ippolito, Eck, and Callison-Burch]{Lee2022Dedup}
+Katherine Lee, Daphne Ippolito, Douglas Eck, and Chris Callison-Burch.
+\newblock Deduplicating training data makes language models better.
+\newblock arXiv:2107.06499, 2022.
+\newblock doi:10.48550/arXiv.2107.06499.
+
+\bibitem[Geifman and El-Yaniv(2017)]{Geifman2017Selective}
+Yair Geifman and Ran El-Yaniv.
+\newblock Selective classification for deep neural networks.
+\newblock arXiv:1705.08500, 2017.
+\newblock doi:10.48550/arXiv.1705.08500.
+
 \end{thebibliography}
 
 \end{document}