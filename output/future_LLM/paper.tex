\begin{filecontents*}{simulation.py}
#!/usr/bin/env python3
# Reproducible EB-SGD experiments and ablations.
# How to run: python3 simulation.py
# Outputs (current folder):
# - results.txt (logistic, per-epoch means/CIs across 10 seeds + diagnostics + timings)
# - results_summary.csv (final metrics and mean time per step)
# - results_mlp.txt (2-layer MLP nonconvex task, per-epoch means/CIs + diagnostics + timings)
# - ablation_alpha.csv (gain sweep)
# - ablation_W.csv (window sweep)
# - ablation_H.csv (target entropy sweep)
# - grid_lr.csv (small learning-rate grid for fairness)
# - best_grid.csv (best-of-grid summary used by the fairness figure)
# All numbers in the manuscript are read from these files.

import numpy as np, time, json, sys, platform, math

def sigmoid(z):
    "Numerically stable logistic function."
    return 1.0/(1.0+np.exp(-z))

def loss_and_grad_logreg(w, Xb, yb, lam=1e-4):
    "Binary logistic loss and gradient with L2 regularization for a minibatch."
    p = sigmoid(Xb.dot(w))
    eps = 1e-12
    loss = -np.mean(yb*np.log(p+eps)+(1-yb)*np.log(1-p+eps)) + 0.5*lam*np.sum(w*w)
    grad = Xb.T.dot(p-yb)/len(yb) + lam*w
    return loss, grad

def accuracy_logreg(w, X, y):
    "Binary accuracy for logistic regression."
    p = sigmoid(X.dot(w))
    yhat = (p>0.5).astype(np.float64)
    return float((yhat==y).mean())

def relu(x): return np.maximum(0.0, x)

def init_mlp(d, h, k, rng):
    "He initialization for a 2-layer ReLU MLP."
    return {"W1": rng.randn(d,h)*np.sqrt(2.0/d), "b1": np.zeros(h),
            "W2": rng.randn(h,k)*np.sqrt(2.0/h), "b2": np.zeros(k)}

def softmax(z):
    "Row-wise softmax."
    z = z - z.max(axis=1, keepdims=True)
    e = np.exp(z)
    return e/np.sum(e, axis=1, keepdims=True)

def mlp_forward(params, X):
    "Forward pass for the 2-layer MLP."
    h = relu(X.dot(params["W1"]) + params["b1"])
    logits = h.dot(params["W2"]) + params["b2"]
    return h, logits

def mlp_loss_and_grad(params, Xb, yb, lam=1e-4):
    "Cross-entropy loss and gradients for the MLP on a minibatch."
    h, logits = mlp_forward(params, Xb)
    P = softmax(logits)
    n = Xb.shape[0]
    y_one = np.zeros_like(P); y_one[np.arange(n), yb] = 1.0
    loss = -np.sum(y_one*np.log(P+1e-12))/n
    loss += 0.5*lam*(np.sum(params["W1"]**2)+np.sum(params["W2"]**2))
    dlogits = (P - y_one)/n
    dW2 = h.T.dot(dlogits) + lam*params["W2"]; db2 = dlogits.sum(axis=0)
    dh = dlogits.dot(params["W2"].T); dh[h<=0] = 0.0
    dW1 = Xb.T.dot(dh) + lam*params["W1"]; db1 = dh.sum(axis=0)
    return loss, {"W1": dW1, "b1": db1, "W2": dW2, "b2": db2}

def mlp_accuracy(params, X, y):
    "Classification accuracy for the MLP."
    _, logits = mlp_forward(params, X)
    yhat = np.argmax(logits, axis=1)
    return float((yhat==y).mean())

def unit(v):
    "Safe normalization to unit vector."
    n = np.linalg.norm(v)+1e-12
    return v/n

def vMF_entropy_from_window(grad_dir_window):
    """
    vMF-based directional entropy proxy from a window of unit vectors in R^d.
    Uses mean resultant length R and a monotone heuristic for kappa(R).
    Returns a squashed value in [0,1] for robustness.
    """
    if len(grad_dir_window)==0: return 0.5*np.log(2*np.pi)
    d = grad_dir_window.shape[1]
    R_vec = grad_dir_window.mean(axis=0); R = float(np.linalg.norm(R_vec))
    R = float(np.clip(R, 1e-8, 0.999999))
    kappa = (R*(d - R**2)) / (1 - R**2)  # monotone heuristic
    Ad = R  # proxy for A_d(kappa)
    logC = (d/2 - 1)*np.log(max(kappa,1e-12)) - (d/2)*np.log(2*np.pi) - kappa
    H = -kappa*Ad - logC
    return float(1.0/(1.0+np.exp(-(H/(d+1e-9)))))  # squashed to [0,1]

def run_logreg_once(seed, method, epochs=20, batch=64, base_eta=0.05, d=20, N_train=2000, N_test=1000,
                    eb_target=0.65, eb_alpha=2.0, W=50, align_target=0.70, align_beta=2.0):
    "Single run of the synthetic logistic regression task with the specified optimizer."
    rng = np.random.RandomState(seed)
    X = rng.randn(N_train + N_test, d)
    w_true = rng.randn(d)
    logits = X.dot(w_true) + 0.5*rng.randn(N_train + N_test)
    probs = sigmoid(logits); y = (probs > 0.5).astype(np.float64)
    X_train, X_test = X[:N_train], X[N_train:]; y_train, y_test = y[:N_train], y[N_train:]
    w = np.zeros(d); m, v = np.zeros(d), np.zeros(d)
    beta1, beta2, eps = 0.9, 0.999, 1e-8
    # SGDM state
    vel = np.zeros(d); mu = 0.9
    # RMSprop state
    s2 = np.zeros(d); rho = 0.99
    window = []; eta0 = base_eta
    results = []
    for ep in range(1, epochs+1):
        idx = np.arange(X_train.shape[0]); rng.shuffle(idx)
        Xbatches = np.array_split(X_train[idx], max(1, X_train.shape[0]//batch))
        ybatches = np.array_split(y_train[idx], max(1, y_train.shape[0]//batch))
        eta_eb_epoch, ent_eb_epoch, align_epoch = [], [], []
        t0 = time.perf_counter()
        for it, (Xb, yb) in enumerate(zip(Xbatches, ybatches), start=1):
            loss, g = loss_and_grad_logreg(w, Xb, yb)
            if method=='SGD':
                step = base_eta*g
            elif method=='SGDM':
                vel = mu*vel + g
                step = base_eta*vel
            elif method=='SGDCosine':
                t = (ep-1 + it/len(Xbatches)); T = epochs
                eta = 0.5*eta0*(1 + math.cos(math.pi * t / T))
                step = eta*g
            elif method=='RMSprop':
                s2 = rho*s2 + (1-rho)*(g*g)
                step = (base_eta*g)/(np.sqrt(s2)+eps)
            elif method=='Adam':
                m = beta1*m + (1-beta1)*g; v = beta2*v + (1-beta2)*(g*g)
                mhat = m/(1-beta1**(ep)); vhat = v/(1-beta2**(ep))
                step = (base_eta*mhat)/(np.sqrt(vhat)+eps)
            elif method=='AdamW':
                weight_decay = 1e-4
                m = beta1*m + (1-beta1)*g; v = beta2*v + (1-beta2)*(g*g)
                mhat = m/(1-beta1**(ep)); vhat = v/(1-beta2**(ep))
                step = (base_eta*mhat)/(np.sqrt(vhat)+eps)
                w *= (1 - base_eta*weight_decay)
            elif method=='Align_SGD':
                gn = np.linalg.norm(g)+1e-12; gh = g/gn
                window.append(gh);  window = window[-W:]
                R_vec = np.mean(window, axis=0); R = float(np.linalg.norm(R_vec))
                eta = base_eta*np.exp(align_beta*(R - align_target))
                eta = float(np.clip(eta, 1e-4, 0.2))
                step = eta*g; align_epoch.append(R)
            elif method=='EB_SGD':
                gn = np.linalg.norm(g)+1e-12; gh = g/gn
                window.append(gh); window = window[-W:]
                Hhat = vMF_entropy_from_window(np.array(window))
                eta = base_eta*np.exp(eb_alpha*(eb_target - Hhat))
                eta = float(np.clip(eta, 1e-4, 0.2))
                step = eta*g; eta_eb_epoch.append(eta); ent_eb_epoch.append(float(Hhat))
            else:
                raise ValueError("Unknown method")
            w -= step
        t1 = time.perf_counter()
        step_time = (t1 - t0)/max(1, len(Xbatches))
        tr_loss,_ = loss_and_grad_logreg(w, X_train, y_train)
        acc = accuracy_logreg(w, X_test, y_test)
        eta_mean = float(np.mean(eta_eb_epoch)) if eta_eb_epoch else float('nan')
        ent_mean = float(np.mean(ent_eb_epoch)) if ent_eb_epoch else float('nan')
        align_mean = float(np.mean(align_epoch)) if align_epoch else float('nan')
        results.append((ep, float(tr_loss), float(acc), eta_mean, ent_mean, align_mean, float(step_time)))
    return results

def run_mlp_once(seed, method, epochs=15, batch=128, base_eta=0.05, d=40, h=64, k=4, N_train=4000, N_test=1000,
                 eb_target=0.60, eb_alpha=1.5, W=50):
    "Single run of the synthetic 4-class MLP task with the specified optimizer."
    rng = np.random.RandomState(seed)
    X = rng.randn(N_train+N_test, d)
    centers = rng.randn(k, d)
    y = rng.randint(0, k, size=N_train+N_test)
    X += centers[y] + 0.3*rng.randn(N_train+N_test, d)
    X_train, X_test = X[:N_train], X[N_train:]; y_train, y_test = y[:N_train], y[N_train:]
    params = init_mlp(d, h, k, rng)
    m, v = {n: np.zeros_like(p) for n,p in params.items()}, {n: np.zeros_like(p) for n,p in params.items()}
    beta1, beta2, eps = 0.9, 0.999, 1e-8
    window = []; results = []
    for ep in range(1, epochs+1):
        idx = np.arange(X_train.shape[0]); np.random.shuffle(idx)
        Xbatches = np.array_split(X_train[idx], max(1, X_train.shape[0]//batch))
        ybatches = np.array_split(y_train[idx], max(1, y_train.shape[0]//batch))
        eta_eb_epoch, ent_eb_epoch = [], []
        t0 = time.perf_counter()
        for it,(Xb,yb) in enumerate(zip(Xbatches,ybatches), start=1):
            loss, g = mlp_loss_and_grad(params, Xb, yb)
            if method=='SGD':
                eta = base_eta
                for name in params: params[name] -= eta*g[name]
            elif method=='Adam':
                for name in params:
                    m[name] = beta1*m[name] + (1-beta1)*g[name]
                    v[name] = beta2*v[name] + (1-beta2)*(g[name]*g[name])
                    mhat = m[name]/(1-beta1**(ep)); vhat = v[name]/(1-beta2**(ep))
                    params[name] -= (base_eta*mhat)/(np.sqrt(vhat)+eps)
            elif method=='EB_SGD':
                flatg = np.concatenate([g["W1"].ravel(), g["b1"].ravel(), g["W2"].ravel(), g["b2"].ravel()])
                gh = unit(flatg)
                window.append(gh); window = window[-W:]
                Hhat = vMF_entropy_from_window(np.array(window))
                eta = base_eta*np.exp(eb_alpha*(eb_target - Hhat))
                eta = float(np.clip(eta, 1e-5, 0.1))
                for name in params: params[name] -= eta*g[name]
                eta_eb_epoch.append(eta); ent_eb_epoch.append(Hhat)
            else:
                raise ValueError("Unknown method")
        t1 = time.perf_counter()
        step_time = (t1 - t0)/max(1, len(Xbatches))
        tr_loss,_ = mlp_loss_and_grad(params, X_train, y_train)
        acc = mlp_accuracy(params, X_test, y_test)
        eta_mean = float(np.mean(eta_eb_epoch)) if eta_eb_epoch else float('nan')
        ent_mean = float(np.mean(ent_eb_epoch)) if ent_eb_epoch else float('nan')
        results.append((ep, float(tr_loss), float(acc), eta_mean, ent_mean, float(step_time)))
    return results

def aggregate_over_seeds_logreg(seeds, methods, epochs=20, d=20, N_train=2000, N_test=1000,
                                eb_target=0.65, eb_alpha=2.0, W=50, align_target=0.70, align_beta=2.0):
    "Aggregate metrics over seeds for the logistic task; compute means, CIs, and diagnostics."
    per_method = {m: [] for m in methods}
    base_etas = {'SGD':0.05, 'SGDM':0.05, 'SGDCosine':0.05, 'RMSprop':0.01, 'Adam':0.02, 'AdamW':0.02, 'Align_SGD':0.05, 'EB_SGD':0.03}
    for s in seeds:
        for m in methods:
            res = run_logreg_once(seed=s, method=m, epochs=epochs, base_eta=base_etas[m], d=d, N_train=N_train, N_test=N_test,
                                  eb_target=eb_target, eb_alpha=eb_alpha, W=W, align_target=align_target, align_beta=align_beta)
            per_method[m].append(res)
    agg = []
    for ep in range(1, epochs+1):
        row = {'epoch': ep}
        for m in methods:
            losses = [per_method[m][i][ep-1][1] for i in range(len(seeds))]
            accs   = [per_method[m][i][ep-1][2] for i in range(len(seeds))]
            tms    = [per_method[m][i][ep-1][6] for i in range(len(seeds))]
            row[f'loss_{m}_mean'] = float(np.mean(losses))
            row[f'loss_{m}_std']  = float(np.std(losses, ddof=1))
            row[f'acc_{m}_mean']  = float(np.mean(accs))
            row[f'acc_{m}_std']   = float(np.std(accs, ddof=1))
            row[f'time_{m}_ms']   = float(1000*np.mean(tms))
            n = len(seeds)
            row[f'loss_{m}_ci'] = float(1.96*row[f'loss_{m}_std']/np.sqrt(n))
            row[f'acc_{m}_ci']  = float(1.96*row[f'acc_{m}_std']/np.sqrt(n))
            if m=='EB_SGD':
                etas = [per_method[m][i][ep-1][3] for i in range(len(seeds))]
                ents = [per_method[m][i][ep-1][4] for i in range(len(seeds))]
                row['eb_eta_mean'] = float(np.mean(etas))
                row['eb_ent_mean'] = float(np.mean(ents))
            if m=='Align_SGD':
                aligns = [per_method[m][i][ep-1][5] for i in range(len(seeds))]
                row['align_metric_mean'] = float(np.mean(aligns))
        agg.append(row)
    summary = {}
    for m in methods:
        summary[m] = {
            'final_train_loss_mean': agg[-1][f'loss_{m}_mean'],
            'final_test_acc_mean': agg[-1][f'acc_{m}_mean'],
            'time_per_step_ms_mean': float(np.mean([r[f'time_{m}_ms'] for r in agg]))
        }
    return agg, summary

def write_csv(path, header, rows):
    "Write a CSV file given header (list) and rows (list of lists)."
    with open(path, "w") as f:
        f.write(",".join(header)+"\n")
        for r in rows: f.write(",".join(r)+"\n")

def small_lr_grid(seeds, method, lrs, epochs=20):
    "Evaluate a small learning-rate grid; return mean final accuracy over seeds for each lr."
    results = []
    for lr in lrs:
        finals=[]
        for s in seeds:
            res = run_logreg_once(seed=s, method=method, epochs=epochs, base_eta=lr)
            finals.append(res[-1][2])
        results.append((lr, float(np.mean(finals))))
    return results

def main():
    "Main entry point to generate all result files used by the manuscript."
    seeds = list(range(10))
    methods = ['SGD','SGDM','SGDCosine','RMSprop','Adam','AdamW','Align_SGD','EB_SGD']
    epochs = 20; d = 20; N_train, N_test = 2000, 1000

    agg, summary = aggregate_over_seeds_logreg(seeds, methods, epochs=epochs, d=d, N_train=N_train, N_test=N_test)

    ts = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
    meta = {
        "timestamp": ts,
        "how_to_run": "python3 simulation.py",
        "task":"Synthetic logistic regression",
        "N_train": int(N_train), "N_test": int(N_test), "d": int(d),
        "epochs": int(epochs), "seeds": seeds,
        "software": {"python": sys.version.split()[0], "numpy": np.__version__, "platform": platform.platform()},
        "params": {
            "SGD":{"eta":0.05},
            "SGDM":{"eta":0.05,"momentum":0.9},
            "SGDCosine":{"eta0":0.05, "schedule":"cosine over epochs"},
            "RMSprop":{"eta":0.01,"rho":0.99,"eps":1e-8},
            "Adam":{"eta":0.02,"beta1":0.9,"beta2":0.999,"eps":1e-8},
            "AdamW":{"eta":0.02,"beta1":0.9,"beta2":0.999,"eps":1e-8,"weight_decay":1e-4},
            "Align_SGD":{"eta_base":0.05,"target_align":0.70,"beta":2.0,"W":50},
            "EB_SGD":{"eta_base":0.03,"entropy_target":0.65,"alpha":2.0,"W":50}
        }
    }
    with open("results.txt","w") as f:
        f.write("# Meta: "+json.dumps(meta)+"\n")
        header_cols = ["epoch"]
        for m in methods:
            header_cols += [f"loss_{m}_mean", f"loss_{m}_ci", f"acc_{m}_mean", f"acc_{m}_ci"]
        header_cols += ["eb_eta_mean","eb_ent_mean","align_metric_mean"]
        for m in methods:
            header_cols += [f"time_{m}_ms"]
        f.write(",".join(header_cols)+"\n")
        for r in agg:
            vals = [str(r["epoch"])]
            for m in methods:
                vals += [f'{r[f"loss_{m}_mean"]:.4f}', f'{r[f"loss_{m}_ci"]:.4f}', f'{r[f"acc_{m}_mean"]:.4f}', f'{r[f"acc_{m}_ci"]:.4f}']
            vals += [f'{r.get("eb_eta_mean", float("nan")):.4f}', f'{r.get("eb_ent_mean", float("nan")):.4f}', f'{r.get("align_metric_mean", float("nan")):.4f}']
            for m in methods:
                vals += [f'{r[f"time_{m}_ms"]:.4f}']
            f.write(",".join(vals)+"\n")

    header = "method,final_train_loss_mean,final_test_acc_mean,time_per_step_ms_mean".split(",")
    rows = []
    for m in methods:
        rows.append([m, f'{summary[m]["final_train_loss_mean"]:.4f}', f'{summary[m]["final_test_acc_mean"]:.4f}', f'{summary[m]["time_per_step_ms_mean"]:.4f}'])
    write_csv("results_summary.csv", header, rows)

    # MLP nonconvex test (subset of methods)
    mlp_methods = ['SGD','Adam','EB_SGD']; mlp_epochs = 15
    base_etas = {'SGD':0.05,'Adam':0.005,'EB_SGD':0.01}
    per_method = {m: [] for m in mlp_methods}
    for m in mlp_methods:
        for s in seeds:
            res = run_mlp_once(seed=s, method=m, epochs=mlp_epochs, base_eta=base_etas[m])
            per_method[m].append(res)
    header = ["epoch"]
    for m in mlp_methods:
        header += [f"loss_{m}_mean", f"loss_{m}_ci", f"acc_{m}_mean", f"acc_{m}_ci"]
    header += ["eb_eta_mean","eb_ent_mean"]
    header += [f"time_{m}_ms" for m in mlp_methods]
    rows=[]
    for ep in range(1, mlp_epochs+1):
        row = [str(ep)]
        for m in mlp_methods:
            losses = [per_method[m][i][ep-1][1] for i in range(len(seeds))]
            accs   = [per_method[m][i][ep-1][2] for i in range(len(seeds))]
            tms    = [per_method[m][i][ep-1][5] for i in range(len(seeds))]
            mean_l, std_l = float(np.mean(losses)), float(np.std(losses, ddof=1))
            mean_a, std_a = float(np.mean(accs)), float(np.std(accs, ddof=1))
            n=len(seeds)
            ci_l = 1.96*std_l/np.sqrt(n) if n>1 else 0.0
            ci_a = 1.96*std_a/np.sqrt(n) if n>1 else 0.0
            row += [f"{mean_l:.4f}", f"{ci_l:.4f}", f"{mean_a:.4f}", f"{ci_a:.4f}"]
        etas = [per_method['EB_SGD'][i][ep-1][3] for i in range(len(seeds))]
        ents = [per_method['EB_SGD'][i][ep-1][4] for i in range(len(seeds))]
        row += [f"{float(np.mean(etas)):.4f}", f"{float(np.mean(ents)):.4f}"]
        for m in mlp_methods:
            tms = [per_method[m][i][ep-1][5] for i in range(len(seeds))]
            row += [f"{float(1000*np.mean(tms)):.4f}"]
        rows.append(row)
    write_csv("results_mlp.txt", header, rows)

    # Ablations (sweeps)
    alphas = [0.5, 1.0, 1.5, 2.0, 3.0]
    Wvals  = [20, 50, 80, 120, 160]
    rows_alpha=[]
    for a in alphas:
        agg_a, _ = aggregate_over_seeds_logreg(seeds, ['EB_SGD'], epochs=epochs, d=d, N_train=N_train, N_test=N_test,
                                               eb_target=0.65, eb_alpha=a, W=50)
        final_acc_synth = agg_a[-1]['acc_EB_SGD_mean']
        mlp_accs=[]
        for s in seeds:
            res = run_mlp_once(seed=s, method='EB_SGD', epochs=15, base_eta=0.01, eb_target=0.60, eb_alpha=a, W=50)
            mlp_accs.append(res[-1][2])
        final_acc_mlp = float(np.mean(mlp_accs))
        rows_alpha.append([f"{a}", f"{final_acc_synth:.4f}", f"{final_acc_mlp:.4f}"])
    write_csv("ablation_alpha.csv", ["alpha","final_acc_synth","final_acc_mlp"], rows_alpha)

    rows_W=[]
    for W in Wvals:
        agg_w, _ = aggregate_over_seeds_logreg(seeds, ['EB_SGD'], epochs=epochs, d=d, N_train=N_train, N_test=N_test,
                                               eb_target=0.65, eb_alpha=2.0, W=W)
        final_acc_synth = agg_w[-1]['acc_EB_SGD_mean']
        mlp_accs=[]
        for s in seeds:
            res = run_mlp_once(seed=s, method='EB_SGD', epochs=15, base_eta=0.01, eb_target=0.60, eb_alpha=1.5, W=W)
            mlp_accs.append(res[-1][2])
        final_acc_mlp = float(np.mean(mlp_accs))
        rows_W.append([f"{W}", f"{final_acc_synth:.4f}", f"{final_acc_mlp:.4f}"])
    write_csv("ablation_W.csv", ["W","final_acc_synth","final_acc_mlp"], rows_W)

    # Target entropy sweep
    Hstars = [0.50, 0.60, 0.65, 0.70, 0.80]
    rows_H=[]
    for Hs in Hstars:
        agg_h, _ = aggregate_over_seeds_logreg(seeds, ['EB_SGD'], epochs=epochs, d=d, N_train=N_train, N_test=N_test,
                                               eb_target=Hs, eb_alpha=2.0, W=50)
        final_acc_synth = agg_h[-1]['acc_EB_SGD_mean']
        rows_H.append([f"{Hs}", f"{final_acc_synth:.4f}"])
    write_csv("ablation_H.csv", ["Hstar","final_acc_synth"], rows_H)

    # Lightweight learning-rate grid (fairness)
    grid_methods = ['SGD','SGDCosine','Adam','AdamW','EB_SGD']
    lr_grid = {
        'SGD':[0.02, 0.05, 0.10],
        'SGDCosine':[0.02, 0.05, 0.10],
        'Adam':[0.005, 0.01, 0.02],
        'AdamW':[0.005, 0.01, 0.02],
        'EB_SGD':[0.01, 0.02, 0.03]
    }
    rows=[]
    grid_seeds = list(range(5))  # keep runtime moderate
    best = {}
    for m in grid_methods:
        results = small_lr_grid(grid_seeds, m, lr_grid[m], epochs=15)
        best_lr, best_acc = None, -1.0
        for lr, acc in results:
            rows.append([m, f"{lr}", f"{acc:.4f}"])
            if acc > best_acc:
                best_acc = acc; best_lr = lr
        best[m] = (best_lr, best_acc)
    write_csv("grid_lr.csv", ["method","lr","final_acc_synth"], rows)

    # Best-of-grid summary for fairness plot
    rows_best = []
    for m in grid_methods:
        lr, acc = best[m]
        rows_best.append([m, f"{lr}", f"{acc:.4f}"])
    write_csv("best_grid.csv", ["method","best_lr","best_final_acc"], rows_best)

    print("Wrote results.txt, results_summary.csv, results_mlp.txt, ablation_alpha.csv, ablation_W.csv, ablation_H.csv, grid_lr.csv, best_grid.csv")

if __name__ == "__main__":
    main()
\end{filecontents*}

\begin{filecontents*}{results.txt}
# Meta: {"timestamp": "2025-09-04 13:04:36", "how_to_run": "python3 simulation.py", "task": "Synthetic logistic regression", "N_train": 2000, "N_test": 1000, "d": 20, "epochs": 20, "seeds": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "software": {"python": "3.13.5", "numpy": "2.3.2", "platform": "Windows-11-10.0.26100-SP0"}, "params": {"SGD": {"eta": 0.05}, "SGDM": {"eta": 0.05, "momentum": 0.9}, "SGDCosine": {"eta0": 0.05, "schedule": "cosine over epochs"}, "RMSprop": {"eta": 0.01, "rho": 0.99, "eps": 1e-08}, "Adam": {"eta": 0.02, "beta1": 0.9, "beta2": 0.999, "eps": 1e-08}, "AdamW": {"eta": 0.02, "beta1": 0.9, "beta2": 0.999, "eps": 1e-08, "weight_decay": 0.0001}, "Align_SGD": {"eta_base": 0.05, "target_align": 0.7, "beta": 2.0, "W": 50}, "EB_SGD": {"eta_base": 0.03, "entropy_target": 0.65, "alpha": 2.0, "W": 50}}}
epoch,loss_SGD_mean,loss_SGD_ci,acc_SGD_mean,acc_SGD_ci,loss_SGDM_mean,loss_SGDM_ci,acc_SGDM_mean,acc_SGDM_ci,loss_SGDCosine_mean,loss_SGDCosine_ci,acc_SGDCosine_mean,acc_SGDCosine_ci,loss_RMSprop_mean,loss_RMSprop_ci,acc_RMSprop_mean,acc_RMSprop_ci,loss_Adam_mean,loss_Adam_ci,acc_Adam_mean,acc_Adam_ci,loss_AdamW_mean,loss_AdamW_ci,acc_AdamW_mean,acc_AdamW_ci,loss_Align_SGD_mean,loss_Align_SGD_ci,acc_Align_SGD_mean,acc_Align_SGD_ci,loss_EB_SGD_mean,loss_EB_SGD_ci,acc_EB_SGD_mean,acc_EB_SGD_ci,eb_eta_mean,eb_ent_mean,align_metric_mean,time_SGD_ms,time_SGDM_ms,time_SGDCosine_ms,time_RMSprop_ms,time_Adam_ms,time_AdamW_ms,time_Align_SGD_ms,time_EB_SGD_ms
1,0.5212,0.0030,0.9500,0.0053,0.2333,0.0034,0.9575,0.0058,0.5214,0.0030,0.9500,0.0053,0.2911,0.0087,0.9391,0.0111,0.2487,0.0102,0.9374,0.0116,0.2488,0.0102,0.9374,0.0116,0.4867,0.0033,0.9518,0.0054,0.5160,0.0031,0.9541,0.0049,0.0521,0.3802,0.8363,0.0193,0.0193,0.0188,0.0234,0.0240,0.0244,0.0387,0.0407
2,0.4322,0.0036,0.9538,0.0050,0.1771,0.0032,0.9622,0.0042,0.4333,0.0036,0.9538,0.0051,0.2381,0.0081,0.9515,0.0080,0.2039,0.0082,0.9558,0.0085,0.2039,0.0082,0.9558,0.0085,0.4021,0.0038,0.9559,0.0050,0.4323,0.0037,0.9554,0.0047,0.0477,0.4182,0.7830,0.0194,0.0197,0.0182,0.0217,0.0252,0.0260,0.0440,0.0453
3,0.3784,0.0036,0.9561,0.0048,0.1571,0.0032,0.9648,0.0044,0.3809,0.0036,0.9562,0.0048,0.2097,0.0074,0.9564,0.0072,0.1863,0.0073,0.9592,0.0071,0.1863,0.0073,0.9592,0.0071,0.3561,0.0039,0.9568,0.0046,0.3823,0.0038,0.9562,0.0048,0.0458,0.4389,0.7315,0.0174,0.0198,0.0176,0.0199,0.0238,0.0267,0.0419,0.0430
4,0.3421,0.0036,0.9572,0.0053,0.1451,0.0033,0.9645,0.0045,0.3462,0.0036,0.9571,0.0052,0.1909,0.0068,0.9581,0.0072,0.1755,0.0067,0.9607,0.0062,0.1755,0.0067,0.9607,0.0062,0.3267,0.0038,0.9577,0.0051,0.3485,0.0037,0.9577,0.0049,0.0443,0.4555,0.6872,0.0183,0.0193,0.0202,0.0208,0.0236,0.0248,0.0428,0.0491
5,0.3155,0.0035,0.9582,0.0052,0.1368,0.0034,0.9647,0.0046,0.3216,0.0035,0.9579,0.0052,0.1771,0.0062,0.9611,0.0059,0.1677,0.0063,0.9611,0.0056,0.1678,0.0063,0.9611,0.0056,0.3058,0.0037,0.9587,0.0049,0.3238,0.0037,0.9580,0.0051,0.0430,0.4696,0.6482,0.0198,0.0180,0.0248,0.0207,0.0235,0.0244,0.0399,0.0458
6,0.2952,0.0035,0.9590,0.0049,0.1305,0.0035,0.9647,0.0050,0.3033,0.0035,0.9586,0.0049,0.1663,0.0058,0.9611,0.0058,0.1618,0.0060,0.9624,0.0051,0.1618,0.0060,0.9624,0.0051,0.2900,0.0037,0.9595,0.0049,0.3048,0.0036,0.9588,0.0050,0.0421,0.4806,0.6172,0.0195,0.0197,0.0259,0.0203,0.0233,0.0247,0.0375,0.0448
7,0.2789,0.0034,0.9598,0.0047,0.1256,0.0036,0.9648,0.0045,0.2894,0.0034,0.9594,0.0049,0.1575,0.0055,0.9621,0.0054,0.1569,0.0058,0.9620,0.0057,0.1570,0.0058,0.9620,0.0057,0.2774,0.0036,0.9600,0.0048,0.2895,0.0035,0.9596,0.0047,0.0414,0.4892,0.5925,0.0257,0.0226,0.0220,0.0211,0.0237,0.0274,0.0402,0.0482
8,0.2656,0.0034,0.9605,0.0046,0.1215,0.0036,0.9645,0.0045,0.2786,0.0034,0.9599,0.0047,0.1501,0.0052,0.9631,0.0047,0.1528,0.0056,0.9627,0.0052,0.1528,0.0056,0.9627,0.0052,0.2671,0.0036,0.9605,0.0046,0.2769,0.0035,0.9600,0.0049,0.0408,0.4958,0.5736,0.0191,0.0242,0.0193,0.0212,0.0251,0.0281,0.0381,0.0450
9,0.2544,0.0033,0.9605,0.0044,0.1182,0.0037,0.9649,0.0044,0.2702,0.0034,0.9604,0.0046,0.1439,0.0050,0.9634,0.0050,0.1493,0.0054,0.9634,0.0053,0.1493,0.0054,0.9633,0.0052,0.2584,0.0036,0.9605,0.0044,0.2662,0.0034,0.9606,0.0046,0.0403,0.5024,0.5537,0.0183,0.0207,0.0174,0.0205,0.0257,0.0263,0.0373,0.0440
10,0.2448,0.0033,0.9608,0.0044,0.1154,0.0037,0.9651,0.0045,0.2635,0.0034,0.9605,0.0046,0.1385,0.0048,0.9639,0.0049,0.1462,0.0053,0.9634,0.0053,0.1462,0.0053,0.9635,0.0053,0.2509,0.0036,0.9607,0.0046,0.2571,0.0034,0.9607,0.0045,0.0398,0.5089,0.5330,0.0186,0.0187,0.0182,0.0198,0.0269,0.0243,0.0365,0.0539
11,0.2364,0.0033,0.9613,0.0047,0.1130,0.0038,0.9656,0.0050,0.2583,0.0033,0.9604,0.0044,0.1338,0.0046,0.9645,0.0048,0.1434,0.0052,0.9635,0.0053,0.1434,0.0052,0.9635,0.0053,0.2444,0.0036,0.9610,0.0046,0.2491,0.0034,0.9607,0.0046,0.0394,0.5135,0.5191,0.0177,0.0183,0.0178,0.0201,0.0264,0.0237,0.0395,0.0434
12,0.2291,0.0033,0.9612,0.0047,0.1108,0.0039,0.9649,0.0050,0.2542,0.0033,0.9604,0.0045,0.1297,0.0045,0.9643,0.0049,0.1409,0.0051,0.9632,0.0049,0.1409,0.0051,0.9632,0.0049,0.2387,0.0036,0.9612,0.0046,0.2420,0.0034,0.9612,0.0045,0.0391,0.5175,0.5071,0.0189,0.0194,0.0176,0.0211,0.0282,0.0234,0.0402,0.0480
13,0.2227,0.0032,0.9612,0.0046,0.1089,0.0039,0.9655,0.0051,0.2511,0.0033,0.9604,0.0046,0.1260,0.0044,0.9645,0.0051,0.1386,0.0050,0.9639,0.0054,0.1387,0.0050,0.9639,0.0054,0.2336,0.0035,0.9611,0.0047,0.2358,0.0034,0.9611,0.0047,0.0388,0.5218,0.4931,0.0179,0.0192,0.0190,0.0218,0.0245,0.0241,0.0389,0.0480
14,0.2169,0.0032,0.9614,0.0046,0.1073,0.0039,0.9655,0.0048,0.2488,0.0033,0.9605,0.0045,0.1228,0.0043,0.9648,0.0049,0.1365,0.0049,0.9643,0.0053,0.1366,0.0049,0.9643,0.0053,0.2290,0.0035,0.9611,0.0047,0.2301,0.0033,0.9612,0.0047,0.0385,0.5249,0.4839,0.0176,0.0222,0.0187,0.0225,0.0255,0.0255,0.0383,0.0443
15,0.2116,0.0032,0.9616,0.0048,0.1058,0.0040,0.9649,0.0048,0.2472,0.0033,0.9606,0.0046,0.1199,0.0043,0.9651,0.0050,0.1346,0.0049,0.9638,0.0048,0.1346,0.0049,0.9638,0.0048,0.2247,0.0035,0.9611,0.0047,0.2250,0.0033,0.9611,0.0047,0.0383,0.5276,0.4756,0.0179,0.0183,0.0219,0.0236,0.0245,0.0247,0.0425,0.0623
16,0.2069,0.0032,0.9618,0.0047,0.1044,0.0040,0.9659,0.0051,0.2461,0.0033,0.9608,0.0046,0.1173,0.0043,0.9648,0.0048,0.1328,0.0048,0.9643,0.0050,0.1328,0.0048,0.9643,0.0050,0.2209,0.0035,0.9616,0.0046,0.2203,0.0033,0.9616,0.0046,0.0381,0.5310,0.4643,0.0195,0.0192,0.0182,0.0244,0.0253,0.0242,0.0368,0.0553
17,0.2025,0.0032,0.9621,0.0046,0.1032,0.0041,0.9651,0.0051,0.2455,0.0033,0.9608,0.0046,0.1149,0.0043,0.9649,0.0045,0.1311,0.0047,0.9648,0.0052,0.1311,0.0047,0.9648,0.0052,0.2174,0.0035,0.9616,0.0046,0.2161,0.0033,0.9616,0.0046,0.0378,0.5341,0.4545,0.0198,0.0190,0.0185,0.0219,0.0254,0.0231,0.0375,0.0576
18,0.1985,0.0032,0.9626,0.0044,0.1021,0.0041,0.9656,0.0046,0.2451,0.0033,0.9608,0.0046,0.1128,0.0043,0.9650,0.0043,0.1295,0.0047,0.9647,0.0053,0.1296,0.0047,0.9647,0.0053,0.2142,0.0035,0.9615,0.0046,0.2121,0.0033,0.9616,0.0047,0.0376,0.5370,0.4447,0.0178,0.0178,0.0176,0.0213,0.0260,0.0240,0.0382,0.0595
19,0.1949,0.0032,0.9628,0.0041,0.1011,0.0041,0.9654,0.0047,0.2450,0.0033,0.9608,0.0046,0.1109,0.0043,0.9655,0.0048,0.1280,0.0046,0.9649,0.0049,0.1281,0.0046,0.9649,0.0049,0.2112,0.0035,0.9614,0.0048,0.2085,0.0033,0.9616,0.0048,0.0374,0.5400,0.4349,0.0178,0.0183,0.0260,0.0226,0.0250,0.0249,0.0426,0.0540
20,0.1915,0.0032,0.9629,0.0042,0.1002,0.0042,0.9651,0.0052,0.2450,0.0033,0.9608,0.0046,0.1091,0.0043,0.9653,0.0049,0.1266,0.0046,0.9642,0.0049,0.1266,0.0046,0.9642,0.0049,0.2084,0.0035,0.9617,0.0048,0.2051,0.0033,0.9618,0.0048,0.0373,0.5417,0.4298,0.0174,0.0178,0.0192,0.0200,0.0243,0.0259,0.0375,0.0451
\end{filecontents*}

\begin{filecontents*}{results_summary.csv}
method,final_train_loss_mean,final_test_acc_mean,time_per_step_ms_mean
SGD,0.1915,0.9629,0.0189
SGDM,0.1002,0.9651,0.0196
SGDCosine,0.2450,0.9608,0.0198
RMSprop,0.1091,0.9653,0.0214
Adam,0.1266,0.9642,0.0250
AdamW,0.1266,0.9642,0.0250
Align_SGD,0.2084,0.9617,0.0394
EB_SGD,0.2051,0.9618,0.0489
\end{filecontents*}

\begin{filecontents*}{results_mlp.txt}
epoch,loss_SGD_mean,loss_SGD_ci,acc_SGD_mean,acc_SGD_ci,loss_Adam_mean,loss_Adam_ci,acc_Adam_mean,acc_Adam_ci,loss_EB_SGD_mean,loss_EB_SGD_ci,acc_EB_SGD_mean,acc_EB_SGD_ci,eb_eta_mean,eb_ent_mean,time_SGD_ms,time_Adam_ms,time_EB_SGD_ms
1,0.0901,0.0086,0.9870,0.0022,0.0098,0.0006,0.9989,0.0007,0.1953,0.0207,0.9561,0.0092,0.0235,0.0312,0.1830,0.2155,0.2605
2,0.0485,0.0044,0.9943,0.0012,0.0090,0.0004,0.9993,0.0005,0.0983,0.0096,0.9850,0.0030,0.0231,0.0411,0.1718,0.2172,0.5796
3,0.0349,0.0030,0.9964,0.0012,0.0088,0.0003,0.9994,0.0004,0.0676,0.0064,0.9909,0.0019,0.0229,0.0475,0.1774,0.2024,0.5704
4,0.0281,0.0023,0.9967,0.0011,0.0086,0.0003,0.9994,0.0004,0.0526,0.0049,0.9939,0.0014,0.0227,0.0531,0.1909,0.2057,0.5914
5,0.0240,0.0019,0.9973,0.0009,0.0085,0.0003,0.9994,0.0004,0.0437,0.0039,0.9950,0.0015,0.0226,0.0573,0.1856,0.2183,0.5781
6,0.0213,0.0016,0.9977,0.0009,0.0084,0.0003,0.9994,0.0004,0.0377,0.0033,0.9959,0.0014,0.0225,0.0605,0.1769,0.2358,0.5887
7,0.0193,0.0014,0.9978,0.0010,0.0083,0.0002,0.9994,0.0004,0.0335,0.0029,0.9965,0.0011,0.0224,0.0634,0.1724,0.2130,0.5855
8,0.0178,0.0013,0.9980,0.0010,0.0082,0.0002,0.9994,0.0004,0.0303,0.0026,0.9965,0.0011,0.0223,0.0648,0.1644,0.2064,0.5889
9,0.0166,0.0012,0.9981,0.0010,0.0082,0.0002,0.9994,0.0004,0.0279,0.0023,0.9967,0.0011,0.0223,0.0662,0.1612,0.2524,0.5816
10,0.0157,0.0011,0.9983,0.0009,0.0081,0.0002,0.9994,0.0004,0.0259,0.0021,0.9971,0.0011,0.0222,0.0673,0.1657,0.2337,0.6074
11,0.0149,0.0010,0.9985,0.0008,0.0080,0.0002,0.9994,0.0004,0.0242,0.0020,0.9973,0.0009,0.0222,0.0687,0.1645,0.2452,0.5929
12,0.0143,0.0009,0.9987,0.0008,0.0080,0.0002,0.9994,0.0004,0.0229,0.0018,0.9976,0.0008,0.0221,0.0699,0.1746,0.2078,0.5769
13,0.0137,0.0009,0.9987,0.0008,0.0079,0.0002,0.9994,0.0004,0.0217,0.0017,0.9977,0.0009,0.0221,0.0706,0.1782,0.2240,0.5817
14,0.0132,0.0008,0.9987,0.0008,0.0079,0.0002,0.9994,0.0004,0.0207,0.0016,0.9977,0.0009,0.0221,0.0712,0.1750,0.2134,0.5811
15,0.0128,0.0008,0.9988,0.0008,0.0078,0.0002,0.9994,0.0004,0.0198,0.0015,0.9977,0.0009,0.0221,0.0719,0.1878,0.2028,0.5927
\end{filecontents*}

\begin{filecontents*}{ablation_alpha.csv}
alpha,final_acc_synth,final_acc_mlp
0.5,0.9614,0.9967
1.0,0.9613,0.9973
1.5,0.9615,0.9977
2.0,0.9618,0.9980
3.0,0.9628,0.9987
\end{filecontents*}

\begin{filecontents*}{ablation_W.csv}
W,final_acc_synth,final_acc_mlp
20,0.9618,0.9978
50,0.9618,0.9978
80,0.9618,0.9978
120,0.9618,0.9978
160,0.9619,0.9978
\end{filecontents*}

\begin{filecontents*}{ablation_H.csv}
Hstar,final_acc_synth
0.5,0.9611
0.6,0.9614
0.65,0.9618
0.7,0.9624
0.8,0.9631
\end{filecontents*}

\begin{filecontents*}{grid_lr.csv}
method,lr,final_acc_synth
SGD,0.02,0.9572
SGD,0.05,0.9608
SGD,0.1,0.9622
SGDCosine,0.02,0.9546
SGDCosine,0.05,0.9582
SGDCosine,0.1,0.9606
Adam,0.005,0.9482
Adam,0.01,0.9578
Adam,0.02,0.9622
AdamW,0.005,0.9482
AdamW,0.01,0.9578
AdamW,0.02,0.9622
EB_SGD,0.01,0.9566
EB_SGD,0.02,0.9586
EB_SGD,0.03,0.9596
\end{filecontents*}

\begin{filecontents*}{best_grid.csv}
method,best_lr,best_final_acc
SGD,0.1,0.9622
SGDCosine,0.1,0.9606
Adam,0.02,0.9622
AdamW,0.02,0.9622
EB_SGD,0.03,0.9596
\end{filecontents*}

\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{siunitx}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue}
\pgfplotsset{compat=1.18}
\pgfplotsset{every axis/.append style={grid=both,grid style={line width=.1pt, draw=gray!30},major grid style={line width=.2pt,draw=gray!50}}}

\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark}
\newtheorem{lemma}{Lemma}

\title{Entropy-Balanced Stochastic Gradient Descent: Regulating Directional Entropy to Couple Exploration and Convergence}
\author{Anonymous}
\date{}

\begin{document}
\maketitle

\begin{abstract}
We introduce Entropy-Balanced Stochastic Gradient Descent (EB-SGD), a feedback optimizer that regulates the entropy of gradient directions observed along the training trajectory. EB-SGD estimates directional entropy via a von Mises--Fisher proxy on a moving window of unit-normalized gradients and modulates the global step online to track a target entropy. This couples exploration (high directional entropy) with convergence (low directional entropy) without per-parameter adaptivity or curvature estimation. We present the algorithm, formalize an EB envelope for noisy quadratics with finite-window estimation error, and deliver a fully reproducible study with multi-seed confidence intervals, ablations (gain, window, target entropy), controller diagnostics, and a lightweight fairness grid. All tables and figures are derived from outputs embedded within this document.
\end{abstract}

\section{Introduction}
First-order stochastic methods power modern machine learning. Their implicit regularization and noise-driven exploration influence generalization \cite{Smith2018BayesianSGDICLR,Keskar2017LargeBatchICLR,Wilson2017MarginalValueNeurIPS}. Widely used adaptive optimizers normalize gradient magnitudes with moving moments \cite{Duchi2011AdagradJMLR,Kingma2015AdamICLR,Reddi2018AdamConvergenceICLR,Luo2019AdaBoundICLR,Liu2020RAdamICLR,Zhuang2020AdaBeliefNeurIPS,Loshchilov2019AdamWICLR}, yet their generalization can differ from SGD \cite{Wilson2017MarginalValueNeurIPS,Foret2021SAMICLR}.

We propose to control a complementary signal: the entropy of gradient directions. EB-SGD tracks a von Mises--Fisher (vMF) entropy proxy over a sliding window of unit gradient directions and adjusts a global step via an exponential controller toward a target entropy. When directions cluster (low entropy), EB-SGD increases the step to exploit; when they scatter (high entropy), it reduces the step to stabilize.

Contributions:
- A new optimization principle: regulate the information content (entropy) of gradient directions to couple exploration and convergence.
- A simple instantiation with a vMF-based entropy proxy and clipped exponential control.
- A stability envelope on noisy quadratics that incorporates finite-window estimation error.
- Reproducible experiments with 10 seeds, 95\% CIs, ablations, diagnostics, and a learning-rate fairness grid, all produced by an embedded implementation.

\section{Related work}
Adaptive methods modulate steps from gradient moments and/or normalization, including Adagrad \cite{Duchi2011AdagradJMLR}, Adam \cite{Kingma2015AdamICLR}, AMSGrad and convergence fixes \cite{Reddi2018AdamConvergenceICLR}, AdaBound \cite{Luo2019AdaBoundICLR}, rectified/variance-aware schemes \cite{Liu2020RAdamICLR}, and belief-based updates \cite{Zhuang2020AdaBeliefNeurIPS}. Decoupled weight decay is a standard regularization mechanism in AdamW \cite{Loshchilov2019AdamWICLR}. Learning-rate schedules such as cosine annealing and warm restarts \cite{Loshchilov2017SGDRICLR}, and lookahead \cite{Zhang2019LookaheadNeurIPS} alter optimization dynamics. Sharpness-aware minimization \cite{Foret2021SAMICLR} and Entropy-SGD \cite{Chaudhari2019EntropySGDJSTAT} shape the objective to prefer wider minima.

Connections to generalization and loss landscapes include the generalization gap with large batches \cite{Keskar2017LargeBatchICLR}, the marginal value of adaptive methods \cite{Wilson2017MarginalValueNeurIPS}, weight averaging and wider optima \cite{Izmailov2018SWAUAI}, Bayesian SWAG \cite{Maddox2019SWAGNeurIPS}, and NTK-based analyses \cite{Jacot2018NTKNeurIPS}. Relations between step length and sharp directions are explored by \cite{Jastrzebski2018SharpestSGD}. For specific architectures, adaptive methods can be advantageous (e.g., attention models) \cite{Zhang2019WhyAdamAttention}. Sign-based compression methods like signSGD \cite{Bernstein2018signSGDICML} emphasize direction but not its entropy. Hypergradient descent adapts steps by differentiating through updates \cite{Baydin2018HypergradientICLR}. EB-SGD differs by targeting directional entropy on the unit sphere \cite{MardiaJupp2000Directional,Banerjee2005vMFJMLR}, rather than magnitude moments or explicit objective smoothing.

\section{Method: EB-SGD}
We minimize empirical risk with stochastic gradients $g_t=\nabla \ell(w_t;B_t)$. Let $\hat{u}_t=g_t/\|g_t\|$ and maintain a sliding window $U_t=\{\hat{u}_{t-W+1},\dots,\hat{u}_t\}\subset\mathbb{S}^{d-1}$. Define the mean resultant length
$
R_t=\Big\|\frac{1}{|U_t|}\sum_{\hat{u}\in U_t}\hat{u}\Big\|.
$
Under a vMF model, concentration $\kappa$ increases with $R_t$ and entropy decreases. We construct a normalized entropy proxy $\widehat{H}_t=\phi(R_t,d)\in[0,1]$ from a vMF entropy approximation with logistic squashing (see implementation details).

Closed-loop update:
$
w_{t+1}=w_t-\eta_t g_t,\quad
\eta_t=\operatorname{clip}\left(\eta_0\exp\big(\alpha(H^\star-\widehat{H}_t)\big),~\eta_{\min},~\eta_{\max}\right).
$
Clipping ranges used in our experiments are [1e-4, 0.2] for logistic regression and [1e-5, 0.1] for the MLP.

\begin{adjustbox}{width=\linewidth}
\begin{algorithm}[H]
\DontPrintSemicolon
\SetAlgoLined
\KwIn{$w_0$, base step $\eta_0$, target entropy $H^\star\in(0,1)$, gain $\alpha>0$, window $W$, clips $\eta_{\min},\eta_{\max}$}
\For{$t=1,2,\dots$}{
  Sample minibatch $B_t$ and compute $g_t$\;
  Normalize $\hat{u}_t=g_t/\|g_t\|$ and update window $U_t$ (drop oldest if $|U_t|>W$)\;
  Compute $R_t$ and $\widehat{H}_t=\phi(R_t,d)$\;
  Set $\eta_t=\operatorname{clip}\!\left(\eta_0\exp\!\big(\alpha(H^\star-\widehat{H}_t)\big),\eta_{\min},\eta_{\max}\right)$\;
  Update $w_{t+1}=w_t-\eta_t g_t$\;
}
\caption{Entropy-Balanced SGD (EB-SGD)}
\end{algorithm}
\end{adjustbox}

Design: a cold-start window grows to size $W$; logistic squashing stabilizes the proxy; clipping avoids extremes. A lightweight EMA on $\widehat{H}_t$ is optional.

\section{Theory: stability and robustness}
Consider $f(w)=\tfrac12 w^\top A w$ with $A\succ0$ and stochastic gradients $g_t=Aw_t+\xi_t$, $\mathbb{E}[\xi_t]=0$, $\mathbb{E}[\xi_t\xi_t^\top]=\Sigma$. For small noise-to-signal, gradient directions concentrate around $Aw_t$; the mean resultant $R$ increases with $\|Aw_t\|$ and decreases with noise \cite{MardiaJupp2000Directional}.

Assumptions and proxy properties:
- A1 (Directional concentration): For fixed $w$, gradient directions over a short window admit a vMF approximation with concentration $\kappa(w)$ increasing in $\|Aw\|$.
- A2 (Proxy monotonicity): The proxy $\widehat{H}=\psi(H_{\text{vMF}}(\kappa(R)))$ is smooth and strictly decreasing in $R$; implementation uses a monotone $\kappa(R)$ and logistic $\psi$.
- A3 (Finite-window noise): With window $W$, $\sqrt{W}(\hat{R}-R)\Rightarrow\mathcal{N}(0,\sigma_R^2)$ and thus $|\widehat{H}-\bar{H}(w)|\le \varepsilon$ with high probability, where $\varepsilon=\Theta(1/\sqrt{W})$ (delta method).

\begin{lemma}[Monotonicity and sensitivity]
Under A1--A2, $d\widehat{H}/dR<0$. Under A3, $\operatorname{Var}(\widehat{H})=\Theta(1/W)$.
\end{lemma}

\begin{theorem}[Mean-square stability under EB envelope with estimation error]
Let $\lambda_{\max}=\lambda_{\max}(A)$. Suppose A1--A3 hold and $|\widehat{H}_t-\bar{H}(w_t)|\le \varepsilon$ for all $t$ w.p. $\ge 1-\delta$, where $\varepsilon=\Theta(1/\sqrt{W})$. If
\[
\eta_{\max} < \frac{2}{\lambda_{\max}},\qquad
0< \eta_0 \exp\!\big(\alpha(H^\star - \underline{H} + \varepsilon)\big) < \frac{2}{\lambda_{\max}},
\]
for a lower bound $\underline{H}\le \bar{H}(w_t)$ along the trajectory, then, on the high-probability event, the EB-SGD recursion
$
w_{t+1}=(I-\eta_t A)w_t - \eta_t \xi_t
$
with $\eta_t=\operatorname{clip}\left(\eta_0 e^{\alpha(H^\star-\widehat{H}_t)},\eta_{\min},\eta_{\max}\right)$ is mean-square stable:
\[
\sup_t \ \mathbb{E}\|w_t\|^2 \ \le\ C_1 \|w_0\|^2 + C_2 \operatorname{tr}(\Sigma).
\]
As $\|Aw_t\|\to 0$, $R\downarrow$ and $\bar{H}(w_t)\uparrow$, decreasing $\mathbb{E}[\eta_t|w_t]$ and the steady-state variance term.
\end{theorem}

\noindent Proof sketch. The clipped step satisfies $\eta_t\in(0,2/\lambda_{\max})$ by assumption, hence $I-\eta_t A$ is a contraction in the $A$-norm. With $\eta_t$ adapted from $\widehat{H}_t$, the error $|\widehat{H}_t-\bar{H}(w_t)|\le\varepsilon$ induces a multiplicative distortion of at most $e^{\alpha\varepsilon}$ in $\eta_t$ inside the clip, which preserves the contraction. A standard Lyapunov argument on $V_t=\|w_t\|^2$ yields a linear drift with additive noise $\eta_t^2\mathbb{E}\|\xi_t\|^2$, uniformly bounded by the clipping envelope, giving the stated bound with constants depending on $(\lambda_{\max},\eta_{\max},\alpha,H^\star,\varepsilon)$.

\begin{remark}[Lipschitzness and non-isotropic noise]
The logistic squashing $\psi(z)=1/(1+e^{-z})$ is 1/4-Lipschitz, which bounds the sensitivity of $\widehat{H}$ to perturbations in the unsquashed entropy. Provided the mapping $R\mapsto \kappa(R)$ is monotone smooth (as in standard approximations to invert $A_d(\kappa)$ \cite{MardiaJupp2000Directional}), the composite $R\mapsto \widehat{H}$ is Lipschitz on $[0,1)$ with a constant depending on $d$. For non-isotropic noise $\Sigma$, directional concentration is governed by the local signal-to-noise ratio along $Aw_t$; the vMF approximation acts as a conservative isotropic surrogate, and the clipping envelope ensures stability even if the proxy slightly underestimates entropy in anisotropic regimes.
\end{remark}

\section{Experiments}
We report means and 95\% CIs over $n{=}10$ seeds. All plots and tables are rendered directly from structured outputs embedded within this document. Tasks:
- Synthetic logistic regression: $d{=}20$, $N_{\text{train}}{=}2000$, $N_{\text{test}}{=}1000$, 20 epochs.
- Nonconvex 2-layer ReLU MLP: input $d{=}40$, hidden 64, $k{=}4$ classes (Gaussian mixture), $N_{\text{train}}{=}4000$, $N_{\text{test}}{=}1000$, 15 epochs.

Baselines: SGD (fixed step), SGDM, SGDCosine \cite{Loshchilov2017SGDRICLR}, RMSprop \cite{Tieleman2012RMSprop}, Adam \cite{Kingma2015AdamICLR}, AdamW \cite{Loshchilov2019AdamWICLR}, a gradient-alignment controller (Align baseline), and EB-SGD.

Note on CIs: we use normal-based intervals $1.96\times \text{SE}$, which slightly underestimates width for $n{=}10$; using a $t$-multiplier with 9 degrees of freedom ($\approx 2.262$) would widen intervals by about 15\% without altering qualitative conclusions.

\subsection{Logistic regression learning curves}
\begin{figure}[h]
\centering
\begin{adjustbox}{width=\linewidth}
\begin{tikzpicture}
\begin{axis}[
  width=\linewidth, height=0.62\linewidth,
  xlabel={Epoch}, ylabel={Training loss (mean ± 95\% CI)},
  legend style={at={(0.5,1.02)},anchor=south,legend columns=3},
  xmin=1,xmax=20
]
\addplot+[thick,blue,mark=*,error bars/.cd,y dir=both,y explicit]
  table [x=epoch,y=loss_SGD_mean,y error=loss_SGD_ci,col sep=comma,comment chars={\#}] {results.txt};
\addlegendentry{SGD}
\addplot+[thick,red,mark=square*,error bars/.cd,y dir=both,y explicit]
  table [x=epoch,y=loss_Adam_mean,y error=loss_Adam_ci,col sep=comma,comment chars={\#}] {results.txt};
\addlegendentry{Adam}
\addplot+[thick,green!50!black,mark=triangle*,error bars/.cd,y dir=both,y explicit]
  table [x=epoch,y=loss_EB_SGD_mean,y error=loss_EB_SGD_ci,col sep=comma,comment chars={\#}] {results.txt};
\addlegendentry{EB-SGD}
\end{axis}
\end{tikzpicture}
\end{adjustbox}
\caption{Synthetic logistic regression: training loss with 95\% CIs over 10 seeds.}
\label{fig:loss}
\end{figure}

\begin{figure}[h]
\centering
\begin{adjustbox}{width=\linewidth}
\begin{tikzpicture}
\begin{axis}[
  width=\linewidth, height=0.62\linewidth,
  xlabel={Epoch}, ylabel={Test accuracy (mean ± 95\% CI)},
  legend style={at={(0.5,1.02)},anchor=south,legend columns=3},
  xmin=1,xmax=20
]
\addplot+[thick,blue,mark=*,error bars/.cd,y dir=both,y explicit]
  table [x=epoch,y=acc_SGD_mean,y error=acc_SGD_ci,col sep=comma,comment chars={\#}] {results.txt};
\addlegendentry{SGD}
\addplot+[thick,red,mark=square*,error bars/.cd,y dir=both,y explicit]
  table [x=epoch,y=acc_Adam_mean,y error=acc_Adam_ci,col sep=comma,comment chars={\#}] {results.txt};
\addlegendentry{Adam}
\addplot+[thick,green!50!black,mark=triangle*,error bars/.cd,y dir=both,y explicit]
  table [x=epoch,y=acc_EB_SGD_mean,y error=acc_EB_SGD_ci,col sep=comma,comment chars={\#}] {results.txt};
\addlegendentry{EB-SGD}
\end{axis}
\end{tikzpicture}
\end{adjustbox}
\caption{Synthetic logistic regression: test accuracy with 95\% CIs.}
\label{fig:acc}
\end{figure}

\subsection{Controller diagnostics and alignment comparison}
\begin{figure}[h]
\centering
\begin{adjustbox}{width=\linewidth}
\begin{tikzpicture}
\begin{axis}[
  width=\linewidth, height=0.62\linewidth,
  xlabel={Epoch}, ymin=0.02, ymax=0.06,
  ylabel={EB-SGD step size (mean)}, axis y line*=left,
  legend style={at={(0.5,1.02)},anchor=south,legend columns=3}
]
\addplot+[thick,green!60!black,mark=triangle*]
  table [x=epoch,y=eb_eta_mean,col sep=comma,comment chars={\#}] {results.txt};
\addlegendentry{$\eta_t$ (EB)}
\end{axis}
\begin{axis}[
  width=\linewidth, height=0.62\linewidth,
  xlabel={Epoch}, ymin=0.35, ymax=0.90,
  ylabel={Entropy / Alignment}, axis y line*=right, axis x line=none,
  yticklabel style={text opacity=1}, y label style={text opacity=1}
]
\addplot+[thick,gray!70!black,mark=*]
  table [x=epoch,y=eb_ent_mean,col sep=comma,comment chars={\#}] {results.txt};
\addlegendentry{Entropy proxy}
\addplot+[thick,blue!50!black,mark=square*]
  table [x=epoch,y=align_metric_mean,col sep=comma,comment chars={\#}] {results.txt};
\addlegendentry{Alignment (baseline)}
\end{axis}
\end{tikzpicture}
\end{adjustbox}
\caption{Controller behavior on logistic: EB-SGD step size responds to the entropy proxy; alignment baseline signal is shown for comparison.}
\label{fig:diag}
\end{figure}

\begin{table}[h]
\centering
\begin{adjustbox}{width=\linewidth}
\pgfplotstabletypeset[
  col sep=comma,
  string type,
  columns/method/.style={column name=Method, string type, verb string type},
  columns/final_train_loss_mean/.style={column name=Final training loss, fixed, precision=4},
  columns/final_test_acc_mean/.style={column name=Final test accuracy, fixed, precision=4},
  columns/time_per_step_ms_mean/.style={column name=Mean time per step (ms), fixed, precision=4},
  every head row/.style={before row=\toprule, after row=\midrule},
  every last row/.style={after row=\bottomrule}
]{results_summary.csv}
\end{adjustbox}
\caption{Summary for the logistic task. EB-SGD reaches 0.9618 final test accuracy versus 0.9642 for Adam/AdamW and 0.9653 for RMSprop, while costing 0.0489 ms per step on average versus 0.0189 ms for SGD ($2.59\times$, $\approx$159\% overhead) and 0.0250 ms for AdamW ($1.96\times$, $\approx$96\% overhead).}
\label{tab:summary}
\end{table}

\subsection{Nonconvex MLP results}
\begin{figure}[h]
\centering
\begin{adjustbox}{width=\linewidth}
\begin{tikzpicture}
\begin{axis}[
  width=\linewidth, height=0.62\linewidth,
  xlabel={Epoch}, ylabel={Test accuracy (mean ± 95\% CI)},
  legend style={at={(0.5,1.02)},anchor=south,legend columns=3},
  xmin=1,xmax=15
]
\addplot+[thick,blue,mark=* ,error bars/.cd,y dir=both,y explicit]
  table [x=epoch,y=acc_SGD_mean,y error=acc_SGD_ci,col sep=comma] {results_mlp.txt};
\addlegendentry{SGD}
\addplot+[thick,red,mark=square*,error bars/.cd,y dir=both,y explicit]
  table [x=epoch,y=acc_Adam_mean,y error=acc_Adam_ci,col sep=comma] {results_mlp.txt};
\addlegendentry{Adam}
\addplot+[thick,green!50!black,mark=triangle*,error bars/.cd,y dir=both,y explicit]
  table [x=epoch,y=acc_EB_SGD_mean,y error=acc_EB_SGD_ci,col sep=comma] {results_mlp.txt};
\addlegendentry{EB-SGD}
\end{axis}
\end{tikzpicture}
\end{adjustbox}
\caption{Nonconvex MLP (synthetic 4-class): EB-SGD converges competitively but does not surpass Adam in final accuracy on this easy synthetic task.}
\label{fig:mlp}
\end{figure}

\subsection{Ablations: gain, window size, and target entropy}
\begin{figure}[h]
\centering
\begin{adjustbox}{width=\linewidth}
\begin{tikzpicture}
\begin{axis}[
  width=\linewidth, height=0.62\linewidth,
  ybar, bar width=10pt,
  xlabel={Gain $\alpha$}, ylabel={Final acc. (logistic)},
  symbolic x coords={0.5,1.0,1.5,2.0,3.0},
  legend style={at={(0.5,1.02)},anchor=south,legend columns=1}
]
\addplot+[fill=green!50!white] table [x=alpha,y=final_acc_synth,col sep=comma] {ablation_alpha.csv};
\addlegendentry{EB-SGD}
\end{axis}
\end{tikzpicture}
\end{adjustbox}
\caption{Ablation on EB-SGD gain $\alpha$: moderate-to-high gains are stable on this task with slight variations in final accuracy.}
\label{fig:abl_alpha}
\end{figure}

\begin{figure}[h]
\centering
\begin{adjustbox}{width=\linewidth}
\begin{tikzpicture}
\begin{axis}[
  width=\linewidth, height=0.62\linewidth,
  ybar, bar width=10pt,
  xlabel={Window size $W$}, ylabel={Final acc. (logistic)},
  symbolic x coords={20,50,80,120,160},
  legend style={at={(0.5,1.02)},anchor=south,legend columns=1}
]
\addplot+[fill=blue!50!white] table [x=W,y=final_acc_synth,col sep=comma] {ablation_W.csv};
\addlegendentry{EB-SGD}
\end{axis}
\end{tikzpicture}
\end{adjustbox}
\caption{Ablation on $W$: final accuracy is insensitive to $W$ on this problem.}
\label{fig:abl_W}
\end{figure}

\begin{figure}[h]
\centering
\begin{adjustbox}{width=\linewidth}
\begin{tikzpicture}
\begin{axis}[
  width=\linewidth, height=0.62\linewidth,
  ybar, bar width=10pt,
  xlabel={Target entropy $H^\star$}, ylabel={Final acc. (logistic)},
  symbolic x coords={0.5,0.6,0.65,0.7,0.8},
  legend style={at={(0.5,1.02)},anchor=south}
]
\addplot+[fill=orange!60!white] table [x=Hstar,y=final_acc_synth,col sep=comma] {ablation_H.csv};
\addlegendentry{EB-SGD}
\end{axis}
\end{tikzpicture}
\end{adjustbox}
\caption{Ablation on target entropy $H^\star$: slightly higher targets can help on this synthetic task.}
\label{fig:abl_H}
\end{figure}

\subsection{Fairness: lightweight learning-rate grid}
We evaluate a small grid (three learning rates per method, five seeds) and report each method's best-of-grid final accuracy on the logistic task.
\begin{figure}[h]
\centering
\begin{adjustbox}{width=\linewidth}
\begin{tikzpicture}
\begin{axis}[
  width=\linewidth, height=0.62\linewidth,
  ybar, bar width=12pt,
  xlabel={Method}, ylabel={Best-of-grid final acc.},
  symbolic x coords={SGD,SGDCosine,Adam,AdamW,EB_SGD},
  xtick=data,
  nodes near coords, nodes near coords align={vertical}
]
\addplot+[fill=gray!40] table [x=method,y=best_final_acc,col sep=comma] {best_grid.csv};
\end{axis}
\end{tikzpicture}
\end{adjustbox}
\caption{Best-of-grid (5 seeds, 3 LRs/method). EB-SGD is competitive but slightly trails the best-tuned baselines on this synthetic task.}
\label{fig:grid}
\end{figure}

\section{Complexity, limitations, and discussion}
Per update, EB-SGD adds (i) one gradient normalization, (ii) a window update, and (iii) vector-norm/exponential operations to compute the vMF proxy. With a naive mean over a length-$W$ FIFO, this costs $O(Wd)$ extra flops; maintaining a running sum reduces it to $O(d)$. The current implementation uses the naive update; hence the measured overheads should be viewed as an upper bound. Empirically (Table~\ref{tab:summary}), EB-SGD averages 0.0489 ms per step versus 0.0189 ms for SGD ($2.59\times$) and 0.0250 ms for AdamW ($1.96\times$) on our platform. The implementation records software versions and OS in a meta log for reproducibility.

Limitations: the vMF proxy is approximate; variance depends on $W$; the target $H^\star$ is a tunable hyperparameter. EB-SGD is competitive on our synthetic tasks but does not surpass Adam/AdamW in final accuracy. Future work: (i) calibrate the $R\mapsto\kappa$ mapping and quantify proxy bias as a function of $d$ and $W$; (ii) integrate momentum and weight decay with entropy control; (iii) scale to standard benchmarks (e.g., MNIST logistic, CIFAR-10 CNNs) and study regimes with more nonstationarity; (iv) adapt $H^\star$ online via hypergradients \cite{Baydin2018HypergradientICLR}.

\paragraph{Reproducibility.}
An executable implementation is embedded in this document. It deterministically writes structured outputs for all metrics, ablations, diagnostics, and timings. All plots and tables are rendered directly from those embedded outputs.

\begin{thebibliography}{99}

\bibitem{Kingma2015AdamICLR}
Diederik P. Kingma and Jimmy Ba.
\newblock Adam: A Method for Stochastic Optimization.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2015.
\newblock \url{https://iclr.cc/Conferences/2015}.

\bibitem{Reddi2018AdamConvergenceICLR}
Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the Convergence of Adam and Beyond.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2018.
\newblock \url{https://openreview.net/forum?id=ryQu7f-RZ}.

\bibitem{Loshchilov2019AdamWICLR}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled Weight Decay Regularization.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2019.
\newblock \url{https://openreview.net/forum?id=Bkg6RiCqY7}.

\bibitem{Luo2019AdaBoundICLR}
Liyuan Luo, Yuanhao Xiong, Yan Liu, and Xu Sun.
\newblock Adaptive Gradient Methods with Dynamic Bound of Learning Rate.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2019.
\newblock \url{https://openreview.net/forum?id=Bkg3g2R9FX}.

\bibitem{Liu2020RAdamICLR}
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han.
\newblock On the Variance of the Adaptive Learning Rate and Beyond.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2020.
\newblock \url{https://openreview.net/forum?id=rkgz2aEKDr}.

\bibitem{Zhuang2020AdaBeliefNeurIPS}
Juntang Zhuang, Tommy Tang, Sekhar C. Tatikonda, Nicha Dvornek, Yifan Ding, Xenophon Papademetris, and James S. Duncan.
\newblock AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2020.
\newblock \url{https://proceedings.neurips.cc/paper_files/paper/2020/hash/5c3b116ce47beeb6b4179ea5f8f2c47d-Abstract.html}.

\bibitem{Foret2021SAMICLR}
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur.
\newblock Sharpness-Aware Minimization for Efficiently Improving Generalization.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2021.
\newblock \url{https://openreview.net/forum?id=6Tm1mposlrM}.

\bibitem{Izmailov2018SWAUAI}
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson.
\newblock Averaging Weights Leads to Wider Optima and Better Generalization.
\newblock In \emph{Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI)}, volume 80, pages 876--885. PMLR, 2018.
\newblock \url{http://proceedings.mlr.press/v80/izmailov18a.html}.

\bibitem{Maddox2019SWAGNeurIPS}
Wesley J. Maddox, Timur Garipov, Pavel Izmailov, Dmitry Vetrov, and Andrew Gordon Wilson.
\newblock A Simple Baseline for Bayesian Uncertainty in Deep Learning.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2019.
\newblock \url{https://proceedings.neurips.cc/paper_files/paper/2019/hash/118921efba23fc329e6560b27861f0c2-Abstract.html}.

\bibitem{Jacot2018NTKNeurIPS}
Arthur Jacot, Franck Gabriel, and Clément Hongler.
\newblock Neural Tangent Kernel: Convergence and Generalization in Neural Networks.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2018.
\newblock \url{https://proceedings.neurips.cc/paper_files/paper/2018/hash/b2cd9f2b50a5a6697b327a9c98d4b2a0-Abstract.html}.

\bibitem{Keskar2017LargeBatchICLR}
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang.
\newblock On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2017.
\newblock \url{https://openreview.net/forum?id=H1oyRlYgg}.

\bibitem{Wilson2017MarginalValueNeurIPS}
Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht.
\newblock The Marginal Value of Adaptive Gradient Methods in Machine Learning.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2017.
\newblock \url{https://proceedings.neurips.cc/paper_files/paper/2017/hash/9e82757e9a1c12cb710ad680db11f6f1-Abstract.html}.

\bibitem{Smith2018BayesianSGDICLR}
Samuel L. Smith and Quoc V. Le.
\newblock A Bayesian Perspective on Generalization and Stochastic Gradient Descent.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2018.
\newblock \url{https://openreview.net/forum?id=BJij4yg0Z}.

\bibitem{Chaudhari2019EntropySGDJSTAT}
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina.
\newblock Entropy-SGD: Biasing Gradient Descent Into Wide Valleys.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment}, 2019(12):124018, 2019.
\newblock DOI: 10.1088/1742-5468/ab3983.

\bibitem{Zhang2019LookaheadNeurIPS}
Michael R. Zhang, James Lucas, Jimmy Ba, and Geoffrey E. Hinton.
\newblock Lookahead Optimizer: k steps forward, 1 step back.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2019.
\newblock \url{https://proceedings.neurips.cc/paper_files/paper/2019/hash/90fd4f88b6184c74a99be060f7a994f7-Abstract.html}.

\bibitem{Loshchilov2017SGDRICLR}
Ilya Loshchilov and Frank Hutter.
\newblock SGDR: Stochastic Gradient Descent with Warm Restarts.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2017.
\newblock \url{https://openreview.net/forum?id=Skq89Scxx}.

\bibitem{Bernstein2018signSGDICML}
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Anima Anandkumar.
\newblock signSGD: Compressed Optimisation for Non-Convex Problems.
\newblock In \emph{Proceedings of the 35th International Conference on Machine Learning (ICML)}, volume 80, pages 560--569. PMLR, 2018.
\newblock \url{http://proceedings.mlr.press/v80/bernstein18a.html}.

\bibitem{Baydin2018HypergradientICLR}
Atilim Günes Baydin, Robert Cornish, David Martinez-Rubio, Mark Schmidt, and Frank Wood.
\newblock Online Learning Rate Adaptation with Hypergradient Descent.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2018.
\newblock \url{https://openreview.net/forum?id=BkrsAzWAb}.

\bibitem{Jastrzebski2018SharpestSGD}
Stanislaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos Storkey.
\newblock On the Relation Between the Sharpest Directions of DNN Loss and the SGD Step Length.
\newblock \emph{arXiv preprint arXiv:1711.04623}, 2018.
\newblock \url{https://arxiv.org/abs/1711.04623}.

\bibitem{MardiaJupp2000Directional}
Kanti V. Mardia and Peter E. Jupp.
\newblock \emph{Directional Statistics}.
\newblock John Wiley \& Sons, Chichester, 2000.

\bibitem{Banerjee2005vMFJMLR}
Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh, and Suvrit Sra.
\newblock Clustering on the Unit Hypersphere using von Mises-Fisher Distributions.
\newblock \emph{Journal of Machine Learning Research}, 6:1345--1382, 2005.
\newblock \url{http://jmlr.org/papers/v6/banerjee05a.html}.

\bibitem{Duchi2011AdagradJMLR}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.
\newblock \emph{Journal of Machine Learning Research}, 12:2121--2159, 2011.
\newblock \url{http://jmlr.org/papers/v12/duchi11a.html}.

\bibitem{Tieleman2012RMSprop}
Tijmen Tieleman and Geoffrey Hinton.
\newblock Lecture 6.5 - RMSProp: Divide the gradient by a running average of its recent magnitude.
\newblock COURSERA: Neural Networks for Machine Learning, 2012.
\newblock \url{https://www.cs.toronto.edu/~hinton/coursera/lecture6/lec6.pdf}.

\bibitem{Zhang2019WhyAdamAttention}
Jerry Zhang, Nisheeth K. Vishnoi, Samy Bengio, and Yoram Singer.
\newblock Why Are Adaptive Methods Good for Attention Models?
\newblock \emph{arXiv preprint arXiv:1912.03194}, 2019.
\newblock \url{https://arxiv.org/abs/1912.03194}.

\end{thebibliography}

\end{document}
