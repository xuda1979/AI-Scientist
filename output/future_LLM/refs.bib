@article{Kingma2015AdamICLR,
  author = {Diederik P. Kingma and Jimmy Ba},
  title = {Adam: A Method for Stochastic Optimization},
  journal = {International Conference on Learning Representations (ICLR)},
  year = {2015},
  url = {https://iclr.cc/Conferences/2015}
}

@article{Reddi2018AdamConvergenceICLR,
  author = {Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},
  title = {On the Convergence of Adam and Beyond},
  journal = {International Conference on Learning Representations (ICLR)},
  year = {2018},
  url = {https://openreview.net/forum?id=ryQu7f-RZ}
}

@article{Loshchilov2019AdamWICLR,
  author = {Ilya Loshchilov and Frank Hutter},
  title = {Decoupled Weight Decay Regularization},
  journal = {International Conference on Learning Representations (ICLR)},
  year = {2019},
  url = {https://openreview.net/forum?id=Bkg6RiCqY7}
}

@article{Luo2019AdaBoundICLR,
  author = {Liyuan Luo and Yuanhao Xiong and Yan Liu and Xu Sun},
  title = {Adaptive Gradient Methods with Dynamic Bound of Learning Rate},
  journal = {International Conference on Learning Representations (ICLR)},
  year = {2019},
  url = {https://openreview.net/forum?id=Bkg3g2R9FX}
}

@article{Liu2020RAdamICLR,
  author = {Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},
  title = {On the Variance of the Adaptive Learning Rate and Beyond},
  journal = {International Conference on Learning Representations (ICLR)},
  year = {2020},
  url = {https://openreview.net/forum?id=rkgz2aEKDr}
}

@article{Zhuang2020AdaBeliefNeurIPS,
  author = {Juntang Zhuang and Tommy Tang and Sekhar C. Tatikonda and Nicha Dvornek and Yifan Ding and Xenophon Papademetris and James S. Duncan},
  title = {AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  year = {2020},
  url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/5c3b116ce47beeb6b4179ea5f8f2c47d-Abstract.html}
}

@article{Foret2021SAMICLR,
  author = {Pierre Foret and Ariel Kleiner and Hossein Mobahi and Behnam Neyshabur},
  title = {Sharpness-Aware Minimization for Efficiently Improving Generalization},
  journal = {International Conference on Learning Representations (ICLR)},
  year = {2021},
  url = {https://openreview.net/forum?id=6Tm1mposlrM}
}

@article{Izmailov2018SWAUAI,
  author = {Pavel Izmailov and Dmitrii Podoprikhin and Timur Garipov and Dmitry Vetrov and Andrew Gordon Wilson},
  title = {Averaging Weights Leads to Wider Optima and Better Generalization},
  journal = {Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI)},
  volume = {80},
  pages = {876--885},
  year = {2018},
  url = {http://proceedings.mlr.press/v80/izmailov18a.html}
}

@article{Maddox2019SWAGNeurIPS,
  author = {Wesley J. Maddox and Timur Garipov and Pavel Izmailov and Dmitry Vetrov and Andrew Gordon Wilson},
  title = {A Simple Baseline for Bayesian Uncertainty in Deep Learning},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  year = {2019},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/118921efba23fc329e6560b27861f0c2-Abstract.html}
}

@article{Jacot2018NTKNeurIPS,
  author = {Arthur Jacot and Franck Gabriel and Clément Hongler},
  title = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  year = {2018},
  url = {https://proceedings.neurips.cc/paper_files/paper/2018/hash/b2cd9f2b50a5a6697b327a9c98d4b2a0-Abstract.html}
}

@article{Keskar2017LargeBatchICLR,
  author = {Nitish Shirish Keskar and Dheevatsa Mudigere and Jorge Nocedal and Mikhail Smelyanskiy and Ping Tak Peter Tang},
  title = {On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima},
  journal = {International Conference on Learning Representations (ICLR)},
  year = {2017},
  url = {https://openreview.net/forum?id=H1oyRlYgg}
}

@article{Wilson2017MarginalValueNeurIPS,
  author = {Ashia C. Wilson and Rebecca Roelofs and Mitchell Stern and Nathan Srebro and Benjamin Recht},
  title = {The Marginal Value of Adaptive Gradient Methods in Machine Learning},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  year = {2017},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/9e82757e9a1c12cb710ad680db11f6f1-Abstract.html}
}

@article{Smith2018BayesianSGDICLR,
  author = {Samuel L. Smith and Quoc V. Le},
  title = {A Bayesian Perspective on Generalization and Stochastic Gradient Descent},
  journal = {International Conference on Learning Representations (ICLR)},
  year = {2018},
  url = {https://openreview.net/forum?id=BJij4yg0Z}
}

@article{Chaudhari2019EntropySGDJSTAT,
  author = {Pratik Chaudhari and Anna Choromanska and Stefano Soatto and Yann LeCun and Carlo Baldassi and Christian Borgs and Jennifer Chayes and Levent Sagun and Riccardo Zecchina},
  title = {Entropy-SGD: Biasing Gradient Descent Into Wide Valleys},
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2019},
  number = {12},
  pages = {124018},
  year = {2019},
  doi = {10.1088/1742-5468/ab3983}
}

@article{Zhang2019LookaheadNeurIPS,
  author = {Michael R. Zhang and James Lucas and Jimmy Ba and Geoffrey E. Hinton},
  title = {Lookahead Optimizer: k steps forward, 1 step back},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  year = {2019},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/90fd4f88b6184c74a99be060f7a994f7-Abstract.html}
}

@article{Loshchilov2017SGDRICLR,
  author = {Ilya Loshchilov and Frank Hutter},
  title = {SGDR: Stochastic Gradient Descent with Warm Restarts},
  journal = {International Conference on Learning Representations (ICLR)},
  year = {2017},
  url = {https://openreview.net/forum?id=Skq89Scxx}
}

@article{Bernstein2018signSGDICML,
  author = {Jeremy Bernstein and Yu-Xiang Wang and Kamyar Azizzadenesheli and Anima Anandkumar},
  title = {signSGD: Compressed Optimisation for Non-Convex Problems},
  journal = {Proceedings of the 35th International Conference on Machine Learning (ICML)},
  volume = {80},
  pages = {560--569},
  year = {2018},
  url = {http://proceedings.mlr.press/v80/bernstein18a.html}
}

@article{Baydin2018HypergradientICLR,
  author = {Atilim Günes Baydin and Robert Cornish and David Martinez-Rubio and Mark Schmidt and Frank Wood},
  title = {Online Learning Rate Adaptation with Hypergradient Descent},
  journal = {International Conference on Learning Representations (ICLR)},
  year = {2018},
  url = {https://openreview.net/forum?id=BkrsAzWAb}
}

@article{Jastrzebski2018SharpestSGD,
  author = {Stanislaw Jastrzebski and Zachary Kenton and Devansh Arpit and Nicolas Ballas and Asja Fischer and Yoshua Bengio and Amos Storkey},
  title = {On the Relation Between the Sharpest Directions of DNN Loss and the SGD Step Length},
  journal = {arXiv preprint arXiv:1711.04623},
  year = {2018},
  url = {https://arxiv.org/abs/1711.04623}
}

@book{MardiaJupp2000Directional,
  author = {Kanti V. Mardia and Peter E. Jupp},
  title = {Directional Statistics},
  publisher = {John Wiley \& Sons},
  address = {Chichester},
  year = {2000}
}

@article{Banerjee2005vMFJMLR,
  author = {Arindam Banerjee and Inderjit S. Dhillon and Joydeep Ghosh and Suvrit Sra},
  title = {Clustering on the Unit Hypersphere using von Mises-Fisher Distributions},
  journal = {Journal of Machine Learning Research},
  volume = {6},
  pages = {1345--1382},
  year = {2005},
  url = {http://jmlr.org/papers/v6/banerjee05a.html}
}

@article{Duchi2011AdagradJMLR,
  author = {John Duchi and Elad Hazan and Yoram Singer},
  title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  journal = {Journal of Machine Learning Research},
  volume = {12},
  pages = {2121--2159},
  year = {2011},
  url = {http://jmlr.org/papers/v12/duchi11a.html}
}

@misc{Tieleman2012RMSprop,
  author = {Tijmen Tieleman and Geoffrey Hinton},
  title = {Lecture 6.5 - RMSProp: Divide the gradient by a running average of its recent magnitude},
  howpublished = {COURSERA: Neural Networks for Machine Learning},
  year = {2012},
  url = {https://www.cs.toronto.edu/~hinton/coursera/lecture6/lec6.pdf}
}

@article{Zhang2019WhyAdamAttention,
  author = {Jerry Zhang and Nisheeth K. Vishnoi and Samy Bengio and Yoram Singer},
  title = {Why Are Adaptive Methods Good for Attention Models?},
  journal = {arXiv preprint arXiv:1912.03194},
  year = {2019},
  url = {https://arxiv.org/abs/1912.03194}
}
