@article{Kingma2015Adam,
  author = {Diederik P. Kingma and Jimmy Ba},
  title = {Adam: A Method for Stochastic Optimization},
  journal = {arXiv},
  year = {2015},
  doi = {10.48550/arXiv.1412.6980},
  url = {https://arxiv.org/abs/1412.6980}
}
@article{Reddi2019AdamConvergence,
  author = {Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},
  title = {On the Convergence of Adam and Beyond},
  journal = {arXiv},
  year = {2019},
  doi = {10.48550/arXiv.1904.09237},
  url = {https://arxiv.org/abs/1904.09237}
}
@article{Luo2019AdaBound,
  author = {Liyuan Luo and Yuanhao Xiong and Yan Liu and Xu Sun},
  title = {Adaptive Gradient Methods with Dynamic Bound of Learning Rate},
  journal = {arXiv},
  year = {2019},
  doi = {10.48550/arXiv.1902.09843},
  url = {https://arxiv.org/abs/1902.09843}
}
@article{Loshchilov2017AdamW,
  author = {Ilya Loshchilov and Frank Hutter},
  title = {Decoupled Weight Decay Regularization},
  journal = {arXiv},
  year = {2017},
  doi = {10.48550/arXiv.1711.05101},
  url = {https://arxiv.org/abs/1711.05101}
}
@article{Wilson2017MarginalValue,
  author = {Ashia C. Wilson and Rebecca Roelofs and Mitchell Stern and Nathan Srebro and Benjamin Recht},
  title = {The Marginal Value of Adaptive Gradient Methods in Machine Learning},
  journal = {arXiv},
  year = {2017},
  doi = {10.48550/arXiv.1705.08292},
  url = {https://arxiv.org/abs/1705.08292}
}
@article{Smith2018BayesianSGD,
  author = {Samuel L. Smith and Quoc V. Le},
  title = {A Bayesian Perspective on Generalization and Stochastic Gradient Descent},
  journal = {arXiv},
  year = {2018},
  doi = {10.48550/arXiv.1710.06451},
  url = {https://arxiv.org/abs/1710.06451}
}
@article{Jastrzebski2018NoiseScale,
  author = {Stanislaw Jastrz{\k{e}}bski and Zachary Kenton and Devansh Arpit and Nicolas Ballas and Asja Fischer and Yoshua Bengio and Amos Storkey},
  title = {On the Relation Between the Sharpest Directions of DNN Loss and the SGD Step Length},
  journal = {arXiv},
  year = {2018},
  doi = {10.48550/arXiv.1711.04623},
  url = {https://arxiv.org/abs/1711.04623}
}
@article{Foret2021SAM,
  author = {Pierre Foret and Ariel Kleiner and Hossein Mobahi and Behnam Neyshabur},
  title = {Sharpness-Aware Minimization for Efficiently Improving Generalization},
  journal = {arXiv},
  year = {2021},
  doi = {10.48550/arXiv.2010.01412},
  url = {https://arxiv.org/abs/2010.01412}
}
@article{Izmailov2018SWA,
  author = {Pavel Izmailov and Dmitrii Podoprikhin and Timur Garipov and Dmitry Vetrov and Andrew Gordon Wilson},
  title = {Averaging Weights Leads to Wider Optima and Better Generalization},
  journal = {arXiv},
  year = {2018},
  doi = {10.48550/arXiv.1803.05407},
  url = {https://arxiv.org/abs/1803.05407}
}
@article{Maddox2019SWAG,
  author = {Wesley J. Maddox and Timur Garipov and Pavel Izmailov and Dmitry Vetrov and Andrew Gordon Wilson},
  title = {A Simple Baseline for Bayesian Uncertainty in Deep Learning},
  journal = {arXiv},
  year = {2019},
  doi = {10.48550/arXiv.1902.02476},
  url = {https://arxiv.org/abs/1902.02476}
}
@article{Cohen2021EdgeStability,
  author = {Jeremy Cohen and Simran Kaur and Yuanzhi Li and J. Zico Kolter and Ameet Talwalkar},
  title = {The Role of Sharpness on the Trainability of Neural Networks},
  journal = {arXiv},
  year = {2021},
  doi = {10.48550/arXiv.2103.00065},
  url = {https://arxiv.org/abs/2103.00065}
}
@article{Chaudhari2016EntropySGD,
  author = {Pratik Chaudhari and Anna Choromanska and Stefano Soatto and Yann LeCun and Carlo Baldassi and Christian Borgs and Jennifer Chayes and Levent Sagun and Riccardo Zecchina},
  title = {Entropy-SGD: Biasing Gradient Descent Into Wide Valleys},
  journal = {arXiv},
  year = {2016},
  doi = {10.48550/arXiv.1611.01838},
  url = {https://arxiv.org/abs/1611.01838}
}
@article{Keskar2016LargeBatch,
  author = {Nitish Shirish Keskar and Dheevatsa Mudigere and Jorge Nocedal and Mikhail Smelyanskiy and Ping Tak Peter Tang},
  title = {On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima},
  journal = {arXiv},
  year = {2016},
  doi = {10.48550/arXiv.1609.04836},
  url = {https://arxiv.org/abs/1609.04836}
}
@article{Zhang2019WhyAdamAttention,
  author = {Jerry Zhang and Nisheeth K. Vishnoi and Samy Bengio and Yoram Singer},
  title = {Why Are Adaptive Methods Good for Attention Models?},
  journal = {arXiv},
  year = {2019},
  doi = {10.48550/arXiv.1912.03194},
  url = {https://arxiv.org/abs/1912.03194}
}
@article{Zhang2019Lookahead,
  author = {Michael R. Zhang and James Lucas and Jimmy Ba and Geoffrey E. Hinton},
  title = {Lookahead Optimizer: k steps forward, 1 step back},
  journal = {arXiv},
  year = {2019},
  doi = {10.48550/arXiv.1907.08610},
  url = {https://arxiv.org/abs/1907.08610}
}
@article{Liu2019RAdam,
  author = {Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},
  title = {On the Variance of the Adaptive Learning Rate and Beyond},
  journal = {arXiv},
  year = {2019},
  doi = {10.48550/arXiv.1908.03265},
  url = {https://arxiv.org/abs/1908.03265}
}
@article{Zhuang2020AdaBelief,
  author = {Juntang Zhuang and Tommy Tang and Sekhar C. Tatikonda and Nicha Dvornek and Yifan Ding and Xenophon Papademetris and James S. Duncan},
  title = {AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients},
  journal = {arXiv},
  year = {2020},
  doi = {10.48550/arXiv.2010.07468},
  url = {https://arxiv.org/abs/2010.07468}
}
@article{Jacot2018NTK,
  author = {Arthur Jacot and Franck Gabriel and Cl{\'e}ment Hongler},
  title = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
  journal = {arXiv},
  year = {2018},
  doi = {10.48550/arXiv.1806.07572},
  url = {https://arxiv.org/abs/1806.07572}
}
